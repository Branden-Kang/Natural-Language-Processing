{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTHFgm5SpJ7bFld/oILcRw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://codemaker2016.medium.com/build-your-own-chatgpt-using-google-gemini-api-1b079f6a8415)"
      ],
      "metadata": {
        "id": "H3cyPBy3MaxL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bHDBQhSNLcEt"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai langchain-google-genai streamlit pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring API Key"
      ],
      "metadata": {
        "id": "B0cWGNmfNDBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"Your API Key\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "Jyygo-YCNAH9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating text responses"
      ],
      "metadata": {
        "id": "_3NV3u-nNFzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyAjsDpD-XXXXXXXXXXXXXXX\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "response = model.generate_content(\"List 5 planets each with an interesting fact\")\n",
        "print(response.text)\n",
        "\n",
        "response = model.generate_content(\"what are top 5 frequently used emojis?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "aQ4QLhcQNEwF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"What is Quantum Computing?\",\n",
        "                                  generation_config = genai.types.GenerationConfig(\n",
        "                                  candidate_count = 1,\n",
        "                                  stop_sequences = ['.'],\n",
        "                                  max_output_tokens = 40,\n",
        "                                  top_p = 0.6,\n",
        "                                  top_k = 5,\n",
        "                                  temperature = 0.8)\n",
        "                                )\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "aj_brEhXNHRs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyAjsDpD-XXXXXXXXXXXXXXX\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "import PIL\n",
        "\n",
        "image = PIL.Image.open('assets/sample_image.jpg')\n",
        "vision_model = genai.GenerativeModel('gemini-pro-vision')\n",
        "response = vision_model.generate_content([\"Explain the picture?\",image])\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "1DRDta_BNKjv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interacting with chat version of Gemini LLM"
      ],
      "metadata": {
        "id": "lEwa3ACYNR79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyAjsDpD-XXXXXXXXXXXXXXX\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "chat_model = genai.GenerativeModel('gemini-pro')\n",
        "chat = chat_model .start_chat(history=[])\n",
        "\n",
        "response = chat.send_message(\"Which is one of the best place to visit in India during summer?\")\n",
        "print(response.text)\n",
        "response = chat.send_message(\"Tell me more about that place in 50 words\")\n",
        "print(response.text)\n",
        "print(chat.history)"
      ],
      "metadata": {
        "id": "axc1NAOONNYs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating Langchain with Gemini"
      ],
      "metadata": {
        "id": "3z2UAb4XNVbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "response = llm.invoke(\"Explain Quantum Computing in 50 words?\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "2ERsbkbtNTSY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_responses = llm.batch(\n",
        "    [\n",
        "        \"Who is the Prime Minister of India?\",\n",
        "        \"What is the capital of India?\",\n",
        "    ]\n",
        ")\n",
        "for response in batch_responses:\n",
        "    print(response.content)"
      ],
      "metadata": {
        "id": "LPyoAVAMNZ2U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
        "\n",
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"Describe the image\",\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": \"https://picsum.photos/id/237/200/300\"\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm.invoke([message])\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "9MVr-eb1Nbpm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
        "\n",
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"Find the differences between the given images\",\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": \"https://picsum.photos/id/237/200/300\"\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": \"https://picsum.photos/id/219/5000/3333\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm.invoke([message])\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "PoUOEaP_Nefv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a ChatGPT Clone with Gemini API"
      ],
      "metadata": {
        "id": "KBFQmOZmNjj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "st.title(\"Gemini Bot\")\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyAjsDpD-XXXXXXXXXXXXX\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "# Select the model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = [\n",
        "        {\n",
        "            \"role\":\"assistant\",\n",
        "            \"content\":\"Ask me Anything\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Process and store Query and Response\n",
        "def llm_function(query):\n",
        "    response = model.generate_content(query)\n",
        "\n",
        "    # Displaying the Assistant Message\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response.text)\n",
        "\n",
        "    # Storing the User Message\n",
        "    st.session_state.messages.append(\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": query\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Storing the User Message\n",
        "    st.session_state.messages.append(\n",
        "        {\n",
        "            \"role\":\"assistant\",\n",
        "            \"content\": response.text\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Accept user input\n",
        "query = st.chat_input(\"What's up?\")\n",
        "\n",
        "# Calling the Function when Input is Provided\n",
        "if query:\n",
        "    # Displaying the User Message\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(query)\n",
        "\n",
        "    llm_function(query)"
      ],
      "metadata": {
        "id": "GPywuB6HNiXV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "streamlit run gemini-bot.py\n",
        "```"
      ],
      "metadata": {
        "id": "_qniShBrNoE1"
      }
    }
  ]
}