{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifying Tweets for Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7YfTOHvOGB0nqwaxIOSno"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Urt8XXkcFD",
        "colab_type": "text"
      },
      "source": [
        "# Classifying Tweets for Sentiment Analysis\n",
        "Reference: https://medium.com/vickdata/detecting-hate-speech-in-tweets-natural-language-processing-in-python-for-beginners-4e591952223"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzsooe_knoCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wBs-wmDbzqg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9d2296fa-71eb-46ec-e20e-b213d5c0dc7c"
      },
      "source": [
        "train = pd.read_csv('https://datahack-prod.s3.amazonaws.com/train_file/train_E6oV3lV.csv')\n",
        "print(\"Training Set:\"% train.columns, train.shape, len(train))\n",
        "test = pd.read_csv('https://datahack-prod.s3.amazonaws.com/test_file/test_tweets_anuFYb8.csv')\n",
        "print(\"Test Set:\"% test.columns, test.shape, len(test))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set: (31962, 3) 31962\n",
            "Test Set: (17197, 2) 17197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKRv_czsg4Gp",
        "colab_type": "text"
      },
      "source": [
        "## Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTWCJLmJg4zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def  clean_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
        "    return df\n",
        "test_clean = clean_text(test, \"tweet\")\n",
        "train_clean = clean_text(train, \"tweet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh2_AC3HhNy7",
        "colab_type": "text"
      },
      "source": [
        "## Handling imbalanced classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqFnOIU7hO-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4e39883f-938d-4884-8f6f-bcb0989da178"
      },
      "source": [
        "# Upsampling\n",
        "from sklearn.utils import resample\n",
        "train_majority = train_clean[train_clean.label==0]\n",
        "train_minority = train_clean[train_clean.label==1]\n",
        "train_minority_upsampled = resample(train_minority, \n",
        "                                 replace=True,    \n",
        "                                 n_samples=len(train_majority),   \n",
        "                                 random_state=123)\n",
        "train_upsampled = pd.concat([train_minority_upsampled, train_majority])\n",
        "train_upsampled['label'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    29720\n",
              "0    29720\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFdhu_yIhuB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downsampling\n",
        "# train_majority = train_clean[train_clean.label==0]\n",
        "# train_minority = train_clean[train_clean.label==1]\n",
        " \n",
        "# train_majority_downsampled = resample(train_majority, \n",
        "#                                  replace=True,  \n",
        "#                                  n_samples=len(train_minority),   \n",
        "#                                  random_state=123)\n",
        "# train_downsampled = pd.concat([train_majority_downsampled, train_minority])\n",
        "# train_downsampled['label'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgRvguEohNq2",
        "colab_type": "text"
      },
      "source": [
        "I tried to use both methods and I can get a better result from downsampling so I go for it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WplQMsOjWxi",
        "colab_type": "text"
      },
      "source": [
        "## CountVectoriser\n",
        "A BoW model splits the words in a piece of text into tokens disregarding grammar and word order. The model also counts the frequency in which a word occurs in the text, and assigns a weight proportional to this frequency. The output is a matrix of term frequencies where each row represents the text and each column a word in the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lG01_cGj_3U",
        "colab_type": "text"
      },
      "source": [
        "## TfidfTransformer\n",
        "CountVectoriser accomplishes the first two, splitting the words into tokens and counting the frequency. We can use another scikit-learn function called TfidfTransformer to apply the frequency weighting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz-laIfSkHQW",
        "colab_type": "text"
      },
      "source": [
        "## Let's training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJHo_PzujYnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "pipeline_sgd = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf',  TfidfTransformer()),\n",
        "    ('nb', SGDClassifier()),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0jKxa0KkOqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_upsampled['tweet'],train_upsampled['label'],random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n-BPgjIkQvM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be1c20b7-2afb-49fe-f2a6-93c707b39392"
      },
      "source": [
        "model = pipeline_sgd.fit(X_train, y_train)\n",
        "y_predict = model.predict(X_test)\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, y_predict)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9694666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}