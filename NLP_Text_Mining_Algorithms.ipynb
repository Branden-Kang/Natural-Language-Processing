{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP: Text Mining Algorithms.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNamPcKuRAii06lRkQ+HQtj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sGrZKYVmxgM",
        "colab_type": "text"
      },
      "source": [
        "[Reference](https://medium.com/fintechexplained/nlp-text-mining-algorithms-4546c6ca30a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3G2qnj4tL92",
        "colab_type": "text"
      },
      "source": [
        "- N-Grams\n",
        "- Bag of Words (BoW)\n",
        "- Term Frequency-Inverse Document Frequency (TF-IDF)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnOYe0IZmvlX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "37fb2a7f-acb1-467d-dac7-6be413ea98d7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "text = 'FinTechExplained is a publication'\n",
        "\n",
        "grams1 = ngrams(nltk.word_tokenize(text), 1)\n",
        "grams2 = ngrams(nltk.word_tokenize(text), 2)\n",
        "grams3 = ngrams(nltk.word_tokenize(text), 3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e662bybdtQ7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "041252e6-7be6-4c99-e309-1ef3c9692598"
      },
      "source": [
        "print(list(grams1))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('FinTechExplained',), ('is',), ('a',), ('publication',)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ZPc1Owtsf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2c9e7fe-ec30-4a31-a8ad-9077ceb6cc20"
      },
      "source": [
        "print(list(grams2))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('FinTechExplained', 'is'), ('is', 'a'), ('a', 'publication')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_5UdrkhtwGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "168eea2f-ad88-4ac3-c96d-1f1d35c617e3"
      },
      "source": [
        "print(list(grams3))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('FinTechExplained', 'is', 'a'), ('is', 'a', 'publication')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWXLX2T7ufKu",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "data = {'twitter':get_tweets(),\n",
        "        'facebook':get_fb_statuses()}\n",
        "        \n",
        "vectoriser = CountVectorizer()\n",
        "vec = vectoriser.fit_transform(data['twitter'].append(data['facebook']))\n",
        "\n",
        "df = pd.DataFrame(vec.toarray().transpose(), index = vectoriser.get_feature_names())\n",
        "\n",
        "df.columns = ['twitter', 'facebook']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-TrGJvjuv9-",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizerdata = {'twitter':get_tweets(),\n",
        "        'facebook':get_fb_statuses()}vectoriser = TfidfVectorizer()\n",
        "vec = vectoriser.fit_transform(data['twitter'].append(data['facebook']))df = pd.DataFrame(vec.toarray().transpose(), index = vectoriser.get_feature_names())df.columns = ['twitter', 'facebook']\n",
        "```"
      ]
    }
  ]
}