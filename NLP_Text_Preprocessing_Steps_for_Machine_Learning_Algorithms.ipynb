{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmqzIIVKigFoX0vWpXZQ7A"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://nautilus06.medium.com/nlp-text-preprocessing-steps-for-machine-learning-algorithms-bc015ccf0173)"
      ],
      "metadata": {
        "id": "QSxJbZuJhc4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Tokenization"
      ],
      "metadata": {
        "id": "1AEbeq9khfLL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aijHA_thZ0Z",
        "outputId": "ecdb140b-daee-4e02-bdf0-e1a7f7e87995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'text', 'for', 'tokenization', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "text = \"This is a sample text for tokenization.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Stopword Removal"
      ],
      "metadata": {
        "id": "tTVw4vpahuT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "tokens = [word for word in tokens if not word in stop_words]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvTkSRpWhgr3",
        "outputId": "5a5dc9c3-1543-4295-87d2-267a22ec1de0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'sample', 'text', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Stemming"
      ],
      "metadata": {
        "id": "eTEEps4Bhvhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqp6C4RthnmU",
        "outputId": "c795c142-8b0e-475b-a7ed-605cb1b04295"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'sampl', 'text', 'token', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Lemmatization"
      ],
      "metadata": {
        "id": "4n6Vk5s6hx8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOsUQ3qJhw_C",
        "outputId": "8475971a-8b48-4ac3-d876-c85f721abda9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'sample', 'text', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Part-of-speech (POS) tagging"
      ],
      "metadata": {
        "id": "jcNb5qUEh8y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "# Tokenize the sentence into words\n",
        "words = nltk.word_tokenize(sentence)\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "# Print the POS tags\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl31nZ7Th4kA",
        "outputId": "6535de16-e4ee-4a9c-9e21-f8ab39398bbf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "pNFEDE4uiFy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Sample text for NER\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "# Process the text with the language model\n",
        "doc = nlp(text)\n",
        "# Extract named entities from the text\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blMsSImwh9Mw",
        "outputId": "15390915-6259-44ab-f67c-f942758006d7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Spell Checking and Correction"
      ],
      "metadata": {
        "id": "KZzAZgjAiLMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyspellchecker\n",
        "\n",
        "# from spellchecker import SpellChecker\n",
        "# # initialize spell checker\n",
        "# spell = SpellChecker()\n",
        "# # example sentence with spelling errors\n",
        "# sentence = \"Ths sentnce hs spellng erors that nd to b corcted.\"\n",
        "# # tokenize sentence\n",
        "# tokens = sentence.split()\n",
        "# # iterate over tokens and correct spelling errors\n",
        "# for i in range(len(tokens)):\n",
        "#     # check if token is misspelled\n",
        "#     if not spell.correction(tokens[i]) == tokens[i]:\n",
        "#         # replace misspelled token with corrected spelling\n",
        "#         tokens[i] = spell.correction(tokens[i])\n",
        "# # join corrected tokens back into sentence\n",
        "# corrected_sentence = ' '.join(tokens)\n",
        "# print(corrected_sentence)"
      ],
      "metadata": {
        "id": "BXW1-L_1iIMi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Removing HTML tags, punctuation, and special characters"
      ],
      "metadata": {
        "id": "wI6Kt81AiSUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def remove_html_tags(text):\n",
        " clean_text = re.sub('<.*?>', '', text)\n",
        " return clean_text\n",
        "\n",
        "def remove_punctuation(text):\n",
        " clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
        " return clean_text\n",
        "\n",
        "def remove_special_characters(text):\n",
        " clean_text = re.sub('[^a-zA-Z0â€“9\\s]', '', text)\n",
        " return clean_text\n",
        "\n",
        "text = \"<p>Hello, world!</p>\"\n",
        "clean_text = remove_html_tags(text)\n",
        "clean_text = remove_punctuation(clean_text)\n",
        "clean_text = remove_special_characters(clean_text)\n",
        "print(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNmu8WzoiNtL",
        "outputId": "9ffce06f-69c2-4dcf-ed55-d8ba0bf51252"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Converting to Lowercase"
      ],
      "metadata": {
        "id": "i7vrC8QXikeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sample TEXT for preprocessing\"\n",
        "text = text.lower()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFmb75QVii1I",
        "outputId": "7c3481df-3411-421e-9453-6eeedeb38bc3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a sample text for preprocessing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Text Vectorization"
      ],
      "metadata": {
        "id": "O4E5P25yirgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Example text corpus\n",
        "corpus = [\"This is the first document.\", \n",
        " \"This document is the second document.\", \n",
        " \"And this is the third one.\", \n",
        " \"Is this the first document?\"]\n",
        "\n",
        "# Vectorize text using BoW representation\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"BoW representation:\")\n",
        "print(X_bow.toarray())\n",
        "print(\"Vocabulary:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Vectorize text using TF-IDF representation\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"TF-IDF representation:\")\n",
        "print(X_tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbbmURcZiqpF",
        "outputId": "c01ee32c-202a-4a6b-e50b-7595afc3449b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW representation:\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n",
            "Vocabulary:\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "TF-IDF representation:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    }
  ]
}