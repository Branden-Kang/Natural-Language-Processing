{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz330Kun1CJiyzGaxpEQDk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://admantium.medium.com/nlp-text-vectorization-methods-from-scratch-ce3c5822c813)"
      ],
      "metadata": {
        "id": "zGE745Czf7y0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_6kZBx64fjfp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from copy import deepcopy\n",
        "from collections import Counter\n",
        "from gensim import downloader\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from time import time\n",
        "\n",
        "class SciKitTransformer(BaseEstimator, TransformerMixin):\n",
        "  def fit(self, X=None, y=None):\n",
        "    return self\n",
        "  def transform(self, X=None):\n",
        "    return self"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WikipediaCorpus(PlaintextCorpusReader):\n",
        "    def __init__(self, root_path):\n",
        "        PlaintextCorpusReader.__init__(self, root_path, r'.*[0-9].txt')\n",
        "\n",
        "    def filter(self, word):\n",
        "        #only keep letters, numbers, and sentence delimiter\n",
        "        word = re.sub('[\\(\\)\\.,;:+\\--\"]', '', word)\n",
        "        #remove multiple whitespace\n",
        "        word = re.sub(r'\\s+', '', word)\n",
        "        if not word in stopwords.words(\"english\"):\n",
        "            return word.lower()\n",
        "        return ''\n",
        "\n",
        "    def vocab(self):\n",
        "        return sorted(set([self.filter(word) for word in corpus.words()]))\n",
        "\n",
        "    def max_words(self):\n",
        "        max = 0\n",
        "        for doc in self.fileids():\n",
        "            l = len(self.words(doc))\n",
        "            max = l if l > max else max\n",
        "        return max\n",
        "\n",
        "    def describe(self, fileids=None, categories=None):\n",
        "        started = time()\n",
        "        return {\n",
        "            'files': len(self.fileids()),\n",
        "            'paras': len(self.paras()),\n",
        "            'sents': len(self.sents()),\n",
        "            'words': len(self.words()),\n",
        "            'vocab': len(self.vocab()),\n",
        "            'max_words': self.max_words(),\n",
        "            'time': time()-started\n",
        "        }"
      ],
      "metadata": {
        "id": "n-pYrGHmgACF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = WikipediaCorpus('ai_sentences')\n",
        "\n",
        "print(corpus.fileids())\n",
        "print(corpus.describe())\n",
        "print(corpus.vocab())\n",
        "\n",
        "class OneHotEncoder(SciKitTransformer):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab_dict = dict.fromkeys(vocab, 0.0)\n",
        "\n",
        "    def one_hot_vector(self, tokens):\n",
        "        vec_dict = deepcopy(self.vocab_dict)\n",
        "        for token in tokens:\n",
        "            if token in self.vocab_dict:\n",
        "                vec_dict[token] = 1.0\n",
        "        vec = [v for v in vec_dict.values()]\n",
        "        return np.array(vec)\n",
        "\n",
        "encoder = OneHotEncoder(corpus.vocab())\n",
        "\n",
        "sent1 = [word for word in word_tokenize(corpus.raw('sent1.txt'))]\n",
        "vec1 = encoder.one_hot_vector(sent1)\n",
        "\n",
        "print(vec1)\n",
        "print(vec1.shape)\n",
        "\n",
        "sent2 = [word for word in word_tokenize(corpus.raw('sent2.txt'))]\n",
        "vec2 = encoder.one_hot_vector(sent2)\n",
        "\n",
        "print(vec2)\n",
        "print(vec2.shape)"
      ],
      "metadata": {
        "id": "JVbZwEbGgCcK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class CountEncoder(SciKitTransformer):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = dict.fromkeys(vocab, 0.0)\n",
        "\n",
        "    def count_vector(self, tokens):\n",
        "        vec_dict = deepcopy(self.vocab)\n",
        "        token_vec = Counter(tokens)\n",
        "        doc_length = len(tokens)\n",
        "        for token, count in token_vec.items():\n",
        "            if token in self.vocab:\n",
        "                vec_dict[token] = count/doc_length\n",
        "        vec = [v for v in vec_dict.values()]\n",
        "        return np.array(vec)\n",
        "\n",
        "encoder = CountEncoder(corpus.vocab())\n",
        "\n",
        "sent1 = [word for word in word_tokenize(corpus.raw('sent1.txt'))]\n",
        "vec1 = encoder.count_vector(sent1)\n",
        "\n",
        "print(vec1)\n",
        "print(vec1.shape)\n",
        "\n",
        "sent2 = [word for word in word_tokenize(corpus.raw('sent2.txt'))]\n",
        "vec2 = encoder.count_vector(sent2)\n",
        "\n",
        "print(vec2)\n",
        "print(vec2.shape)"
      ],
      "metadata": {
        "id": "0afAd0NEgGey"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfIdfEncoder(SciKitTransformer):\n",
        "    def __init__(self, doc_arr, vocab):\n",
        "        self.doc_arr = doc_arr\n",
        "        self.vocab = vocab\n",
        "        self.word_frequency = self._word_frequency()\n",
        "\n",
        "    def _word_frequency(self):\n",
        "        word_frequency = dict.fromkeys(self.vocab, 0.0)\n",
        "        for doc_name in self.doc_arr:\n",
        "            doc_words = Counter([word for word in self.doc_arr[doc_name]])\n",
        "            for word, _ in doc_words.items():\n",
        "                if word in self.vocab:\n",
        "                    word_frequency[word] += 1.0\n",
        "        return word_frequency\n",
        "\n",
        "    def TfIdf_vector(self, doc_name):\n",
        "        if not doc_name in self.doc_arr:\n",
        "            print(f'Document \"{doc_name}\" not found.')\n",
        "            return\n",
        "        number_of_docs = len(self.doc_arr)\n",
        "        doc_len = len(self.doc_arr[doc_name])\n",
        "        doc_words = Counter([word for word in self.doc_arr[doc_name]])\n",
        "        TfIdf_vec = dict.fromkeys(self.vocab, 0.0)\n",
        "        for word, word_count in doc_words.items():\n",
        "            if word in self.vocab:\n",
        "                tf = word_count/doc_len\n",
        "                idf = np.log(number_of_docs/self.word_frequency[word])\n",
        "                idf = 1 if idf == 0 else idf\n",
        "                TfIdf_vec[word] = tf * idf\n",
        "        vec = [v for v in TfIdf_vec.values()]\n",
        "        return np.array(vec)\n",
        "\n",
        "doc_list = [doc for doc in corpus.fileids()]\n",
        "words_list = [corpus.words(doc) for doc in [doc for doc in corpus.fileids()]]\n",
        "doc_arr = dict(zip(doc_list, words_list))\n",
        "\n",
        "encoder = TfIdfEncoder(doc_arr, corpus.vocab())\n",
        "vec1 = encoder.TfIdf_vector('sent1.txt')\n",
        "\n",
        "print(vec1)\n",
        "print(vec1.shape)\n",
        "vec2 = encoder.TfIdf_vector('sent2.txt')\n",
        "\n",
        "print(vec2)\n",
        "print(vec2.shape)"
      ],
      "metadata": {
        "id": "rCSCwjtBgav6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv = downloader.load('word2vec-google-news-300')\n",
        "\n",
        "class Word2VecEncoder(SciKitTransformer):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.vector_lookup = downloader.load('word2vec-google-news-300')\n",
        "\n",
        "    def word_vector(self, tokens):\n",
        "        vec = np.array([])\n",
        "        for token in tokens:\n",
        "            if token in self.vocab:\n",
        "                if token in self.vector_lookup:\n",
        "                    print(f'Add {token}')\n",
        "                    vec = np.append(self.vector_lookup[token], vec)\n",
        "        return vec\n",
        "\n",
        "encoder = Word2VecEncoder(corpus.vocab())\n",
        "\n",
        "sent1 = [word for word in word_tokenize(corpus.raw('sent1.txt'))]\n",
        "vec1 = encoder.word_vector(sent1)\n",
        "print(vec1)\n",
        "print(vec1.shape)\n",
        "\n",
        "sent2 = [word for word in word_tokenize(corpus.raw('sent2.txt'))]\n",
        "vec2 = encoder.word_vector(sent2)\n",
        "\n",
        "print(vec2)\n",
        "print(vec2.shape)"
      ],
      "metadata": {
        "id": "SoeJmX4EghI6"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}