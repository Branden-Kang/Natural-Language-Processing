{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOXBrqAX3vkWv0CfJ48Ygg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://lopezyse.medium.com/nlp-text-vectorization-e472a3a9983a)"
      ],
      "metadata": {
        "id": "Mwd8P0MhjK0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "bDTugwuSkYSV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XechCGXTTmRy",
        "outputId": "97e38589-3950-4dfa-f5aa-f5ef5f6bf63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'robots': 6, 'will': 7, 'augment': 2, 'humans': 4, 'ai': 1, 'affect': 0, 'human': 3, 'improve': 5}\n"
          ]
        }
      ],
      "source": [
        "from sklearn. feature_extraction. text import CountVectorizer\n",
        "texts = [\"robots will augment humans\",\n",
        " \"AI will affect human will\",\n",
        " \"humans will improve robots\"]\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(texts)\n",
        "print(vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = vectorizer.transform(texts)\n",
        "print(vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83R4D0tMj_rk",
        "outputId": "30aaecc0-0830-4fb7-8a1b-c200d5510b99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-znNDZhkCvu",
        "outputId": "bdf18b0f-50c8-443f-ac26-1f88875928b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 0 1 0 1 1]\n",
            " [1 1 0 1 0 0 0 2]\n",
            " [0 0 0 0 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term frequencyâ€“inverse document frequency (TF-IDF)"
      ],
      "metadata": {
        "id": "M5LJ7wcRka1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "texts = [\"robots will augment humans\",\n",
        " \"AI will affect human will\",\n",
        " \"humans will improve robots\"]\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
        "tfidf_vectorizer.fit_transform(texts)\n",
        "print(tfidf_vectorizer.vocabulary_)\n",
        "tfidf_vector = tfidf_vectorizer.transform(texts)\n",
        "print(tfidf_vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWoEyOC2kDrQ",
        "outputId": "391eb618-0f5a-4d20-e342-885c228759ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'robots': 6, 'will': 7, 'augment': 2, 'humans': 4, 'ai': 1, 'affect': 0, 'human': 3, 'improve': 5}\n",
            "[[0.         0.         0.63174505 0.         0.4804584  0.\n",
            "  0.4804584  0.37311881]\n",
            " [0.4769856  0.4769856  0.         0.4769856  0.         0.\n",
            "  0.         0.56343076]\n",
            " [0.         0.         0.         0.         0.4804584  0.63174505\n",
            "  0.4804584  0.37311881]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "texts = [\"robots will augment humans\",\n",
        " \"AI will affect human will\",\n",
        " \"humans will improve robots\"]\n",
        "for i, text in enumerate(texts):\n",
        " tokenized= []\n",
        " for word in text.split(' '):\n",
        "  word = word.split('.')[0]\n",
        "  word = word.lower()\n",
        "  tokenized.append(word)\n",
        " texts[i] = tokenized\n",
        "model = word2vec.Word2Vec(texts, min_count = 1)"
      ],
      "metadata": {
        "id": "BQFQEXaqkkrp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=['robots'], topn=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlPH6ivcktRw",
        "outputId": "19c783ec-acb4-4f82-9a52-0186a154c182"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('human', 0.17018885910511017)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gensim\n",
        "# model2 = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "# model2.most_similar(positive=['robots'], topn=5)"
      ],
      "metadata": {
        "id": "zE319NJYlGfB"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}