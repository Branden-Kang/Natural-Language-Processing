{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-Processing Text in Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNRGVjofVokV8dS/Vh5pYYs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jmpcior1lWy",
        "colab_type": "text"
      },
      "source": [
        "[Reference](https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8kqE9Wp3ZSc",
        "colab_type": "text"
      },
      "source": [
        "# Convert text to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uotuk9hc1gCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "dfd4e634-ca5e-44c0-eea0-5b98a9ad6aaf"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def to_lower(text):\n",
        "    \"\"\"\n",
        "    Converting text to lower case as in, converting \"Hello\" to  \"hello\" or \"HELLO\" to \"hello\".\n",
        "    \"\"\"\n",
        "    return ' '.join([w.lower() for w in nltk.word_tokenize(text)])\n",
        "text = \"\"\"Harry Potter is the most miserable, lonely boy you can imagine. He's shunned by his relatives, the Dursley's, that have raised him since he was an infant. He's forced to live in the cupboard under the stairs, forced to wear his cousin Dudley's hand-me-down clothes, and forced to go to his neighbour's house when the rest of the family is doing something fun. Yes, he's just about as miserable as you can get.\"\"\"\n",
        "print (to_lower(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "harry potter is the most miserable , lonely boy you can imagine . he 's shunned by his relatives , the dursley 's , that have raised him since he was an infant . he 's forced to live in the cupboard under the stairs , forced to wear his cousin dudley 's hand-me-down clothes , and forced to go to his neighbour 's house when the rest of the family is doing something fun . yes , he 's just about as miserable as you can get .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3kerbyU3YkC",
        "colab_type": "text"
      },
      "source": [
        "# Remove Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acu90_eE3XTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "116de417-c3e7-4323-ef82-543dedd16d2f"
      },
      "source": [
        "import re\n",
        "text = \"\"\"<head><body>hello world!</body></head>\"\"\"\n",
        "cleaned_text = re.sub('<[^<]+?>','', text)\n",
        "print (cleaned_text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello world!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb6SxyiC3fSZ",
        "colab_type": "text"
      },
      "source": [
        "# Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISIwe4jB3gmS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5c44974-da49-46ca-d3f3-61350f1ab45c"
      },
      "source": [
        "text = \"There was 200 people standing right next to me at 2pm.\"\n",
        "output = ''.join(c for c in text if not c.isdigit())\n",
        "print(output)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There was  people standing right next to me at pm.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXuFad9K3k7-",
        "colab_type": "text"
      },
      "source": [
        "# Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knaF4Ka83lzj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e742d5e-3641-491b-8b86-082f639707fb"
      },
      "source": [
        "from string import punctuation\n",
        "def strip_punctuation(s):\n",
        "    return ''.join(c for c in s if c not in punctuation)\n",
        "text = \"Hello! how are you doing?\"\n",
        "print (strip_punctuation(text))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello how are you doing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzTrGBxa3rRn",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJVIAoda3s4I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7eb970cd-d520-4220-eca8-35167ed34233"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "#is based on The Porter Stemming Algorithm\n",
        "stopword = stopwords.words('english')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = 'the functions of this fan is great'\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "print (lemmatized_word)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['the', 'function', 'of', 'this', 'fan', 'is', 'great']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY_-6TOl36iE",
        "colab_type": "text"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bRu_9ZO389C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a2d664fd-4038-43d5-fab3-524386ebfb26"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "#is based on The Porter Stemming Algorithm\n",
        "stopword = stopwords.words('english')\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "text = 'This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit'\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
        "print (stemmed_word)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'is', 'a', 'demo', 'text', 'for', 'nlp', 'use', 'nltk', '.', 'full', 'form', 'of', 'nltk', 'is', 'natur', 'languag', 'toolkit']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xCQCMuU4D4Y",
        "colab_type": "text"
      },
      "source": [
        "# word tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOBZptUV4E47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0f71a7d1-1d18-41fb-8265-0f67d4db7a0b"
      },
      "source": [
        "text = 'This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit'\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "print (word_tokens)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'Demo', 'Text', 'for', 'NLP', 'using', 'NLTK', '.', 'Full', 'form', 'of', 'NLTK', 'is', 'Natural', 'Language', 'Toolkit']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYF1eIFG4IVw",
        "colab_type": "text"
      },
      "source": [
        "# sentence tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-qlgj7a4Jge",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "58cebeb8-a043-4c7c-9eb8-8a48a8e0079b"
      },
      "source": [
        "text = 'This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit'\n",
        "sent_token = nltk.sent_tokenize(text)\n",
        "print (sent_token)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This is a Demo Text for NLP using NLTK.', 'Full form of NLTK is Natural Language Toolkit']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSh3i-Sj4M8M",
        "colab_type": "text"
      },
      "source": [
        "# Stop words removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J2yHUlW4N3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2a7e5f9d-033d-4dc4-b776-f3dcff143fd6"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stopword = stopwords.words('english')\n",
        "text = 'This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit'\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "removing_stopwords = [word for word in word_tokens if word not in stopword]\n",
        "print (removing_stopwords)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'Demo', 'Text', 'NLP', 'using', 'NLTK', '.', 'Full', 'form', 'NLTK', 'Natural', 'Language', 'Toolkit']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn1Ta4ER4WTq",
        "colab_type": "text"
      },
      "source": [
        "# Contraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRYZbUgG4YF6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "e627fb3c-b8e2-4918-b21e-0709c583f5e9"
      },
      "source": [
        "# coding: utf-8\n",
        "import re\n",
        "import nltk\n",
        "!pip install contractions\n",
        "from contractions import contractions_dict\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
        "                                      flags=re.IGNORECASE | re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contractions_dict.get(match) \\\n",
        "            if contractions_dict.get(match) \\\n",
        "            else contractions_dict.get(match.lower())\n",
        "        expanded_contraction = expanded_contraction\n",
        "        return expanded_contraction\n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "def main():\n",
        "    text = \"\"\"I ain't going there. You'll have to go alone.\"\"\"\n",
        "    \n",
        "    text=expand_contractions(text,contractions_dict)\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "    \n",
        "    print (tokenized_sentences)\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/00/92/a05b76a692ac08d470ae5c23873cf1c9a041532f1ee065e74b374f218306/contractions-0.0.25-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 3.9MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 17.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81701 sha256=0caa26ca97e2a70fb27288a213ea446678309bfd9fdf9da8151fff7f0b5dc7ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.25 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "[['I', 'are', 'not', 'going', 'there', '.'], ['you', 'will', 'have', 'to', 'go', 'alone', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-1MHCgd4pYy",
        "colab_type": "text"
      },
      "source": [
        "# Spell Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc5o8nD61p8r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "da5b464a-cfae-4f12-c2e2-420199535d20"
      },
      "source": [
        "!pip install autocorrect\n",
        "from autocorrect import spell\n",
        "text = \"This is a wrld of hope\"\n",
        "spells = [spell(w) for w in (nltk.word_tokenize(text))]\n",
        "print (spells)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/5b/6510d8370201fc96cbb773232c2362079389ed3285b0b1c6a297ef6eadc0/autocorrect-2.0.0.tar.gz (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.0.0-cp36-none-any.whl size=1811640 sha256=bf4b52fc6a7945ab1858e28200f528bbc90ef105b067764f902abb06cfe93ce9\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/06/bc/e66f28d72bed29591eadc79cebb2e7964ad0282804ab233da3\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.0.0\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "['This', 'is', 'a', 'world', 'of', 'hope']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}