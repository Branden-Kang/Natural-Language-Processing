{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Presidential Debate Sentiment Analysis with LSTM, OneVsRest, LinearSVC.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjtm675LpcVoXh98Z0xkCt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz5DOUbdVU-F",
        "colab_type": "text"
      },
      "source": [
        "[Reference](https://medium.com/hackernoon/presidential-debate-sentiment-analysis-with-lstm-onevsrest-linearsvc-nlp-step-by-step-guide-b9683e2c8ed9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiLA5iNXUpRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "54b2c1f7-d7fa-4362-8699-9bb23201df48"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import RidgeClassifier"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb7U36bnUqut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/Branden-Kang/Natural-Language-Processing/master/Data/Sentiment.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhXtSuGkU1FJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "a9faad11-e455-4081-aaf5-7b69512074d0"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>candidate</th>\n",
              "      <th>candidate_confidence</th>\n",
              "      <th>relevant_yn</th>\n",
              "      <th>relevant_yn_confidence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sentiment_confidence</th>\n",
              "      <th>subject_matter</th>\n",
              "      <th>subject_matter_confidence</th>\n",
              "      <th>candidate_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>relevant_yn_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>sentiment_gold</th>\n",
              "      <th>subject_matter_gold</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>No candidate mentioned</td>\n",
              "      <td>1.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.6578</td>\n",
              "      <td>None of the above</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I_Am_Kenzi</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-08-07 09:54:46 -0700</td>\n",
              "      <td>629697200650592256</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Quito</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Scott Walker</td>\n",
              "      <td>1.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>None of the above</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PeacefulQuest</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-08-07 09:54:46 -0700</td>\n",
              "      <td>629697199560069120</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>No candidate mentioned</td>\n",
              "      <td>1.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.6629</td>\n",
              "      <td>None of the above</td>\n",
              "      <td>0.6629</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PussssyCroook</td>\n",
              "      <td>NaN</td>\n",
              "      <td>27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-08-07 09:54:46 -0700</td>\n",
              "      <td>629697199312482304</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>No candidate mentioned</td>\n",
              "      <td>1.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>None of the above</td>\n",
              "      <td>0.7039</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MattFromTexas31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>138</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-08-07 09:54:45 -0700</td>\n",
              "      <td>629697197118861312</td>\n",
              "      <td>Texas</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>1.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0.7045</td>\n",
              "      <td>None of the above</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sharonDay5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>156</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-08-07 09:54:45 -0700</td>\n",
              "      <td>629697196967903232</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Arizona</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id               candidate  ...  tweet_location               user_timezone\n",
              "0   1  No candidate mentioned  ...             NaN                       Quito\n",
              "1   2            Scott Walker  ...             NaN                         NaN\n",
              "2   3  No candidate mentioned  ...             NaN                         NaN\n",
              "3   4  No candidate mentioned  ...           Texas  Central Time (US & Canada)\n",
              "4   5            Donald Trump  ...             NaN                     Arizona\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwNLXj5DU1vB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "525aca6a-3358-4b19-8505-673e1a48ca83"
      },
      "source": [
        "df = df[['text','sentiment']]\n",
        "df.sample(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10914</th>\n",
              "      <td>THIS ISNT A DEBATE!!!! This is a Q and A #GOPD...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11092</th>\n",
              "      <td>“@msgoddessrises: Damn I'm out of Peach vodka!...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13800</th>\n",
              "      <td>Okay, @JebBush gets a point for that! #commonC...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8686</th>\n",
              "      <td>RT @BettyFckinWhite: So many great jokes on Tw...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13241</th>\n",
              "      <td>RT @mozgovaya: 10 men on stage discussing one ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text sentiment\n",
              "10914  THIS ISNT A DEBATE!!!! This is a Q and A #GOPD...  Negative\n",
              "11092  “@msgoddessrises: Damn I'm out of Peach vodka!...  Negative\n",
              "13800  Okay, @JebBush gets a point for that! #commonC...  Positive\n",
              "8686   RT @BettyFckinWhite: So many great jokes on Tw...  Negative\n",
              "13241  RT @mozgovaya: 10 men on stage discussing one ...  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELbI6H6VU6je",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f27c991c-6ad6-40a6-ebf1-2ab9dee1ac03"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13871, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBmqaxKvU_s0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6364bd40-60a6-42b8-a9bd-dd5c68854f98"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Negative    8493\n",
              "Neutral     3142\n",
              "Positive    2236\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV75OdyCVBoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b24cd9fa-1fb3-4bae-e7ab-a33576e45b00"
      },
      "source": [
        "df.iloc[57,0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'RT @factcheckdotorg: .@JebBush said he cut FL taxes by $19B. But that includes cuts in estate taxes mandated by federal law. #GOPDebate. ht…'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_iWTi-MVC7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.33, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTQCayY9VGmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "175f6250-ee9d-4b1c-a80c-31428ad1b3d0"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10428    RT @SalMasekela: These self righteous hypocrit...\n",
              "13671    RT @brock_a_r: I wonder which candidate is goi...\n",
              "89       A few of my favorite #Twitter responses to the...\n",
              "5024     @FoxNews Megyn Kelly's #GOPDebate performance ...\n",
              "8572     I didn't watch the #GOPDebates tonight, so I w...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXGARARKVH8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9c81e959-6d19-457f-fa5b-149d19b255cb"
      },
      "source": [
        "y_train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10428    Negative\n",
              "13671    Negative\n",
              "89        Neutral\n",
              "5024     Negative\n",
              "8572     Negative\n",
              "Name: sentiment, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05QIXXxQVOSm",
        "colab_type": "text"
      },
      "source": [
        "# Text Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1q4daQjVMln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;#]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z +_]')\n",
        "STOPWORDS = stopwords.words('english')\n",
        "STOPWORDS.extend(['rt', 'http']) # extend stopwords; rt means re-tweet\n",
        "STOPWORDS = set(STOPWORDS)\n",
        "\n",
        "def text_prepare(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join([word for word in text.split() if word not in STOPWORDS]) # delete stopwords from text\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZsax73OVTs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = [text_prepare(x) for x in X_train]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nK8_ql2VeB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4717a0f2-d717-44e6-8c1c-583598192748"
      },
      "source": [
        "X_train[:3]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['salmasekela self righteous hypocrites trying god shameless gopdebates',\n",
              " 'brock_a_r wonder candidate going first accidentally call ben carson help gopdebates',\n",
              " 'favorite twitter responses gopdebate last night tco 2iqcdrcdlm tco 7tdma3vlm8']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDDX8pLGVfhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val = [text_prepare(x) for x in X_val]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1qztqbNVi5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = [text_prepare(x) for x in X_test]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz3PD9N1VlRq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "66d89298-6e27-4e20-d0df-735db37da460"
      },
      "source": [
        "# Dictionary of all words from train corpus with their counts.\n",
        "words_counts = {}\n",
        "\n",
        "from collections import Counter\n",
        "words_counts = Counter([word for line in X_train for word in line.split(' ')])\n",
        "\n",
        "# Sorting \n",
        "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "# Top 10\n",
        "most_common_words[:10]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gopdebate', 4811),\n",
              " ('gopdebates', 2827),\n",
              " ('tco', 1941),\n",
              " ('rwsurfergirl', 1072),\n",
              " ('trump', 953),\n",
              " ('fox', 714),\n",
              " ('amp', 578),\n",
              " ('debate', 569),\n",
              " ('realdonaldtrump', 560),\n",
              " ('news', 504)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKpUSlvEVpW4",
        "colab_type": "text"
      },
      "source": [
        "# Word Embedding with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPY4n4fHVnqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tfidf_features(X_train, X_val, X_test):\n",
        "    \"\"\"\n",
        "        X_train, X_val, X_test - input text       \n",
        "        return TF-IDF vectorizer for each dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    # filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the tweets)\n",
        "    # ngram!!! -->  ngram_range=(1,2)\n",
        "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5, token_pattern='(\\S+)')\n",
        "    \n",
        "    # Fit and transform the vectorizer on the train set\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    \n",
        "    # Transform the test and val sets \n",
        "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "    \n",
        "    return X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vectorizer.vocabulary_\n",
        "    \n",
        "    \n",
        "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cjjZu5TVwwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "37d98d98-a8b5-434f-e436-501c52d74930"
      },
      "source": [
        "X_train_tfidf.toarray()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.40615562],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzHggbd4V0-9",
        "colab_type": "text"
      },
      "source": [
        "# 1st Model: Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crkc0U5bVzxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "3125e185-9aa4-435d-a5eb-fa52e99fcfbc"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "# %%time\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Return accuracy\n",
        "scores = cross_val_score(logreg, X_train_tfidf, y_train, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "\n",
        "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross-validation mean accuracy 67.93%, std 0.32.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9esUz90WBAW",
        "colab_type": "text"
      },
      "source": [
        "# 2nd Model: LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0ZCAeP4V3tI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2773e9bd-0c21-40e9-8864-4f0d5673634d"
      },
      "source": [
        "%%time\n",
        "svc = LinearSVC(dual=False)\n",
        "svc.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "scores = cross_val_score(svc, X_test_tfidf, y_test, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation mean accuracy 63.81%, std 0.36.\n",
            "CPU times: user 277 ms, sys: 4.03 ms, total: 281 ms\n",
            "Wall time: 464 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JEK5YO8WFc4",
        "colab_type": "text"
      },
      "source": [
        "# 3rd Model: OneVsRest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3ECd9U2WDoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_classifier(X_train, y_train):\n",
        "    \"\"\"\n",
        "      X_train, y_train — training text and sentiment\n",
        "      \n",
        "      return: trained classifier\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create and fit LogisticRegression wraped into OneVsRestClassifier.\n",
        "    \n",
        "    model = OneVsRestClassifier(LogisticRegression(penalty='l2', C=1.0))\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "    \n",
        " \n",
        "classifier_tfidf = train_classifier(X_train_tfidf, y_train)\n",
        "\n",
        "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
        "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO4B9cDJWIPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1bed7e2a-3325-4283-9853-7e59ac4af06e"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score \n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "def evaluation_scores(y_val, predicted):\n",
        "    \n",
        "    print (\"Accracy={}\".format(accuracy_score(y_val, predicted)))\n",
        "    print (\"F1_macro={}\".format(f1_score(y_val, predicted, average='macro')))\n",
        "    print (\"F1_micro={}\".format(f1_score(y_val, predicted, average='micro')))\n",
        "    print (\"F1_wted={}\".format(f1_score(y_val, predicted, average='weighted')))\n",
        "    \n",
        "print('Tfidf')\n",
        "evaluation_scores(y_val, y_val_predicted_labels_tfidf)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tfidf\n",
            "Accracy=0.6906939214631522\n",
            "F1_macro=0.5550930129562491\n",
            "F1_micro=0.6906939214631522\n",
            "F1_wted=0.6518652578108339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28m4ajvWWODc",
        "colab_type": "text"
      },
      "source": [
        "# 4th Model: LSTM with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QtriQuGWKDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzM12PSIWQAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = [text_prepare(x) for x in df['text']]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWqFTS8HWUig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f913e057-a338-4a0b-8071-a7c7b4ff664c"
      },
      "source": [
        "X[:3]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nancyleegrahn everyone feel climate change question last night exactly gopdebate',\n",
              " 'scottwalker didnt catch full gopdebate last night scotts best lines 90 seconds walker16 tco zsff',\n",
              " 'tjmshow mention tamir rice gopdebate held cleveland wow']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KvR_NjNWVcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBxWEg30WgNz",
        "colab_type": "text"
      },
      "source": [
        "## Create LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf3qLABaWXjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "958c0352-f7e8-450c-c6db-eb68246189e3"
      },
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 24, 128)           256000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 24, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 591       \n",
            "=================================================================\n",
            "Total params: 511,391\n",
            "Trainable params: 511,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3frSBbVWfXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = pd.get_dummies(df['sentiment']).values"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWcIxsZRWp1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e67e6eaa-7a05-4778-d796-02f4405266f7"
      },
      "source": [
        "Y[:3]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0],\n",
              "       [0, 0, 1],\n",
              "       [0, 1, 0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaLGggCoWqpd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "9193b425-cf5f-4993-e654-fc1b412f1f3c"
      },
      "source": [
        "# create train and test datasets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "print(\"Trianing \", X_train.shape,Y_train.shape)\n",
        "print(\"Testing \",X_test.shape,Y_test.shape)\n",
        "\n",
        "batch_size = 32\n",
        "model.fit(X_train, Y_train, epochs = 20, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trianing  (9293, 24) (9293, 3)\n",
            "Testing  (4578, 24) (4578, 3)\n",
            "Epoch 1/20\n",
            "291/291 - 37s - loss: 0.8316 - accuracy: 0.6423\n",
            "Epoch 2/20\n",
            "291/291 - 37s - loss: 0.6890 - accuracy: 0.7036\n",
            "Epoch 3/20\n",
            "291/291 - 37s - loss: 0.6250 - accuracy: 0.7337\n",
            "Epoch 4/20\n",
            "291/291 - 37s - loss: 0.5877 - accuracy: 0.7529\n",
            "Epoch 5/20\n",
            "291/291 - 37s - loss: 0.5568 - accuracy: 0.7652\n",
            "Epoch 6/20\n",
            "291/291 - 38s - loss: 0.5248 - accuracy: 0.7797\n",
            "Epoch 7/20\n",
            "291/291 - 37s - loss: 0.4934 - accuracy: 0.7950\n",
            "Epoch 8/20\n",
            "291/291 - 37s - loss: 0.4696 - accuracy: 0.8045\n",
            "Epoch 9/20\n",
            "291/291 - 37s - loss: 0.4457 - accuracy: 0.8162\n",
            "Epoch 10/20\n",
            "291/291 - 37s - loss: 0.4260 - accuracy: 0.8214\n",
            "Epoch 11/20\n",
            "291/291 - 37s - loss: 0.4044 - accuracy: 0.8315\n",
            "Epoch 12/20\n",
            "291/291 - 37s - loss: 0.3854 - accuracy: 0.8408\n",
            "Epoch 13/20\n",
            "291/291 - 37s - loss: 0.3777 - accuracy: 0.8445\n",
            "Epoch 14/20\n",
            "291/291 - 37s - loss: 0.3549 - accuracy: 0.8524\n",
            "Epoch 15/20\n",
            "291/291 - 37s - loss: 0.3520 - accuracy: 0.8568\n",
            "Epoch 16/20\n",
            "291/291 - 37s - loss: 0.3362 - accuracy: 0.8611\n",
            "Epoch 17/20\n",
            "291/291 - 37s - loss: 0.3235 - accuracy: 0.8637\n",
            "Epoch 18/20\n",
            "291/291 - 37s - loss: 0.3164 - accuracy: 0.8664\n",
            "Epoch 19/20\n",
            "291/291 - 37s - loss: 0.3095 - accuracy: 0.8694\n",
            "Epoch 20/20\n",
            "291/291 - 37s - loss: 0.2948 - accuracy: 0.8775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb40f9e4908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}