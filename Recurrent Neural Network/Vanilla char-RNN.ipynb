{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of assignment2_sol.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEeT3dyRzQO6",
        "colab_type": "text"
      },
      "source": [
        "## Vanilla char-RNN\n",
        "Implement, train, and evaluate a simple RNN where the input are sequences of characters from sentences. This RNN can then be used to generate sentences char-by-char.\n",
        "\n",
        "NO Pytorch here but raw numpy codes. The idea is to have you go deeper into the model.\n",
        "\n",
        "Explore the explosion and vanishing gradient issues in this example and have a deeper understanding of RNN before moving to fancier recurrent networks such as LSTM (bidirectinal possibly with attention) in the following sections.\n",
        "\n",
        "The equation numbers are referring to the deep learning book (online version).\n",
        "Chap10 https://www.deeplearningbook.org/contents/rnn.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwPa_Sjjw8ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# !pwd\n",
        "# os.chdir('gdrive/My Drive/research/Datasets/Amazon_UCSD')\n",
        "# !pwd\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHyL9_uHwua5",
        "colab_type": "code",
        "outputId": "09a7ab18-93d4-4752-8162-d0039b463f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# First import necessary packages and define global variables\n",
        "\"\"\"\n",
        "    Codes borrowed from Karpathy:\n",
        "    https://gist.github.com/karpathy/d4dee566867f8291f086\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "char_to_ix = {}\n",
        "ix_to_char = {}\n",
        "review_in_ix = None # training corpus\n",
        "valid_review_in_ix = None # validation corpus\n",
        "test_review_in_ix = None # test corpus\n",
        "texts = None\n",
        "avg_len = None\n",
        "\n",
        "\n",
        "# model and training\n",
        "hidden_size = 128\n",
        "learning_rate = 1e-4\n",
        "input_size = None   # the number of unique characters in the alphabet\n",
        "output_size = None  # same as input_size\n",
        "\n",
        "\n",
        "# model parameters: the naming follows the \"Deep Learning\" book RNN notation.\n",
        "U = None    # mapping from x to h\n",
        "W = None    # mapping from previous h to current h\n",
        "b = None    # bias in mapping from x to h\n",
        "V = None    # mapping from h to o\n",
        "c = None    # bias in mapping from h to o\n",
        "\n",
        "# Then define utility functions for saving, loading, and initializing models.\n",
        "\n",
        "def save_model(epoch_id, iteration_id, state_dict):\n",
        "\n",
        "    with open(f'assignment2/char_rnn_model_{epoch_id}_{iteration_id}.pkl', 'wb') as out_f:\n",
        "        pickle.dump(state_dict, out_f)\n",
        "\n",
        "\n",
        "def load_model(epoch_id, iteration_id):\n",
        "    global U, V, W, b, c\n",
        "    global mU, mV, mW, mb, mc\n",
        "    global epoches, n\n",
        "    global smooth_loss\n",
        "\n",
        "    print(f'Loading pre-trained model from epoch {epoch_id}, iteration {iteration_id}')\n",
        "    with open(f'assignment2/char_rnn_model_{epoch_id}_{iteration_id}.pkl', 'rb') as in_f:\n",
        "        params = pickle.load(in_f)\n",
        "    \n",
        "        U = params['U']\n",
        "        V = params['V']\n",
        "        W = params['W']\n",
        "        b = params['b']\n",
        "        c = params['c']\n",
        "\n",
        "        mU = params['mU']\n",
        "        mV = params['mV']\n",
        "        mW = params['mW']\n",
        "        mb = params['mb']\n",
        "        mc = params['mc']\n",
        "\n",
        "        epoches = epoch_id\n",
        "        n = iteration_id\n",
        "\n",
        "        smooth_loss = params['smooth_loss']\n",
        "\n",
        "\n",
        "def init_model():\n",
        "    \"\"\"\n",
        "    Randomly initialize all model parameters\n",
        "    \"\"\"\n",
        "    global U, V, W, b, c\n",
        "    global mU, mV, mW, mb, mc\n",
        "    global epoches, n\n",
        "    global smooth_loss\n",
        "\n",
        "    W = np.random.randn(hidden_size, hidden_size) * 0.01  # mapping from h to h\n",
        "    U = np.random.randn(hidden_size, input_size) * 0.01  # mapping from x to h\n",
        "    V = np.random.randn(output_size, hidden_size) * 0.01  # mapping from h to o\n",
        "    b = np.zeros((hidden_size, 1))  # bias term in the hidden unit\n",
        "    c = np.zeros((output_size, 1))  # bias term in the output unit\n",
        "\n",
        "    # sentence counter and batch counter\n",
        "    epoches, n = 0, 0\n",
        "    mU, mV, mW = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
        "    mb, mc = np.zeros_like(b), np.zeros_like(c)\n",
        "    smooth_loss = -np.log(1.0/output_size) # * avg_len\n",
        "\n",
        "# Functions for handling text data\n",
        "\n",
        "def read_reviews(path):\n",
        "    \"\"\"\n",
        "    Read a csv file of Amazon reviews and put the reviews in texts.\n",
        "    path: pointing to the csv file.\n",
        "    \"\"\"\n",
        "    print(path)\n",
        "    df = pd.read_csv(path)\n",
        "    texts = []\n",
        "    for r in range(df.shape[0]):\n",
        "        try:\n",
        "            texts.append(df.iloc[r, 0])\n",
        "        except Exception as inst:\n",
        "            break\n",
        "    print('{} reviews loaded.'.format(df.shape[0]))\n",
        "    return texts\n",
        "\n",
        "\n",
        "def reviews_to_ix(texts):\n",
        "    \"\"\"\n",
        "    Build indices of the characters (the alphabet)\n",
        "    and turn the sentences into sequences of char indices.\n",
        "    \"\"\"\n",
        "\n",
        "    global char_to_ix, ix_to_char, review_in_ix, avg_len\n",
        "    global input_size, output_size\n",
        "\n",
        "    chars = set()\n",
        "    for review in texts:\n",
        "        try:\n",
        "            chars.update(set(review))\n",
        "        except:\n",
        "            pass\n",
        "    char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "    char_to_ix['eos'] = len(char_to_ix)\n",
        "    ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "    ix_to_char[char_to_ix['eos']] = 'eos'\n",
        "\n",
        "    # turn reviews into sequence of character indices.\n",
        "    review_in_ix = []\n",
        "    for review in texts:\n",
        "        try:\n",
        "            review_in_ix.append([char_to_ix[c] for c in review])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    input_size = len(char_to_ix)\n",
        "    output_size = input_size    # the output space is the same as the input space\n",
        "\n",
        "    print(f'{len(review_in_ix)} reviews; '\n",
        "          f'{len(char_to_ix)} unique chars (input_size); '\n",
        "          f'{sum([len(s) for s in review_in_ix])} chars in total')\n",
        "    avg_len = np.mean([len(s) for s in review_in_ix])\n",
        "\n",
        "\n",
        "def more_review_to_ix(more_texts):\n",
        "    \"\"\"\n",
        "    Assume that the training reviews have been indexed and the char_to_ix is built,\n",
        "    now index additional reviews (for validation and testing).\n",
        "    \"\"\"\n",
        "    global char_to_ix\n",
        "\n",
        "    # turn reviews into sequence of character indices.\n",
        "    more_review_in_ix = []\n",
        "    for review in more_texts:\n",
        "        try:\n",
        "            more_review_in_ix.append([char_to_ix[c] for c in review])\n",
        "        except:\n",
        "            pass\n",
        "    return more_review_in_ix\n",
        "\n",
        "# Define the RNN loss function.\n",
        "\n",
        "def lossFunc(inputs, targets, hprev = None):\n",
        "    \"\"\"\n",
        "    Define the loss function and implement the BPTT algorithm.\n",
        "    Note that this is not the built-in PyTorch loss.\n",
        "\n",
        "    inputs: an array of integers, representing char indices in a review.\n",
        "    targets: an array of integers, which is the shift of inputs to the right by one time step.\n",
        "    hprev: initial hidden states. If None, start from all zeros.\n",
        "    \n",
        "    return: the loss, the gradients on model parameters, and the last hidden state\n",
        "    \"\"\"\n",
        "    # RNN parameters\n",
        "    global U, V, W, b, c\n",
        "\n",
        "    assert len(inputs) == len(targets), f'inputs and targets have different lengths,\\\n",
        "        {len(inputs)} != {len(targets)}'\n",
        "    \n",
        "    # use dictionaries to ease indexing of the time steps\n",
        "    # key = time-steps, value = the vectors at those steps.\n",
        "    xs, hs, os, ps = {}, {}, {}, {}\n",
        "    hs[-1] = np.copy(hprev) # the initial hidden units before the 0-th step\n",
        "    loss = 0\n",
        "\n",
        "    # forward pass: go from the 0-th to the second last entries\n",
        "    for t in range(len(inputs)):\n",
        "        # create an one-hot vector for the char at position t\n",
        "        xs[t] = np.zeros((input_size, 1))\n",
        "        xs[t][inputs[t]] = 1\n",
        "        # compute the hidden units (Eq. 10.9 of the DL book)\n",
        "        hs[t] = np.tanh(b + np.dot(W, hs[t - 1]) + np.dot(U, xs[t]))\n",
        "        # the unnormalized log output probabilities (Eq. 10.10 of the DL book)\n",
        "        os[t] = np.dot(V, hs[t]) + c\n",
        "        # the output probabilities of the next character (Eq. 10.11 of the DL book)\n",
        "        ps[t] = np.exp(os[t]) / np.sum(np.exp(os[t]))\n",
        "\n",
        "        # cross entropy loss when predicting the next word index\n",
        "        # this is the so-called \"Perplexity\" in the SLP book.\n",
        "        loss += -np.log(ps[t][targets[t], 0])\n",
        "\n",
        "    # backward pass: implementing BPTT (Back-Propagation through Time), DL Section 10.2.2\n",
        "    dU, dV, dW = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
        "    db, dc = np.zeros_like(b), np.zeros_like(c)\n",
        "    dhnext = np.zeros_like(hs[0])\n",
        "\n",
        "    # starting from the last word.\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # at the current t, find dL/do = P(y) - y. Eq. 10.18 in the DL book.\n",
        "        do = np.copy(ps[t])\n",
        "        do[targets[t]] = -1\n",
        "\n",
        "        # Eq. 10.24 in the DL book for a single t.\n",
        "        dV += np.dot(do, hs[t].T)\n",
        "        # Eq. 10.22 in the DL book for a single t.\n",
        "        dc += do\n",
        "\n",
        "        # Eq. (20) in the DL book\n",
        "        dh = np.dot(V.T, do) + dhnext\n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh    # backprop through the tanh calculating hs[t]\n",
        "\n",
        "        # Eq. (10.25) and (10.27)\n",
        "        dU += np.dot(dhraw, xs[t].T)\n",
        "        dW += np.dot(dhraw, hs[t - 1].T)\n",
        "\n",
        "        # Eq. (10.23)\n",
        "        db += dhraw\n",
        "\n",
        "        # prepare dh in the horizontal direction for the next iteration\n",
        "        dhnext = np.dot(W.T, dhraw)\n",
        "\n",
        "    # gradient clipping\n",
        "    for dparam in [dU, dV, dW, db, dc]:\n",
        "        np.clip(dparam, -1, 1, out=dparam)\n",
        "\n",
        "    return loss, dU, dV, dW, db, dc, hs[len(inputs) - 1]\n",
        "\n",
        "\n",
        "def sample(h, seed_idx, n):\n",
        "    \"\"\"\n",
        "    sample a sentence of length n from the current model.\n",
        "    Use teacher forcing so that a predicted char will be fed into the\n",
        "    RNN as input at the next step.\n",
        "    :param h: memmory state\n",
        "    :param seed_idx: seed letter for the first time step\n",
        "    :param n: length of the sentence\n",
        "    \"\"\"\n",
        "    global U, V, W, b, c\n",
        "\n",
        "    x = np.zeros((input_size, 1))\n",
        "    x[seed_idx] = 1\n",
        "    ixes = []\n",
        "\n",
        "    for t in range(n):\n",
        "        # update h\n",
        "        h = np.tanh(np.dot(U, x) + np.dot(W, h) + b)\n",
        "        # unnormalized log probability\n",
        "        o = np.dot(V, h) + c\n",
        "        # output probability\n",
        "        p = np.exp(o) / np.sum(np.exp(o))\n",
        "        # sample one word\n",
        "        ix = np.random.choice(range(output_size), p=p.ravel())\n",
        "        # prepare the one-hot vector for the next location\n",
        "        # this is teacher forcing\n",
        "        x = np.zeros((input_size, 1))\n",
        "        x[ix] = 1\n",
        "        ixes.append(ix)\n",
        "\n",
        "    return ixes\n",
        "\n",
        "\n",
        "def run_model(corpus_in_ix):\n",
        "    global U, V, W, b, c\n",
        "    total_loss = 0\n",
        "    total_targets = 0\n",
        "\n",
        "    for n in range(len(corpus_in_ix)):\n",
        "        inputs = corpus_in_ix[n]\n",
        "        targets = inputs + [char_to_ix['eos']]\n",
        "        targets = targets[1:]\n",
        "        hprev = np.zeros((hidden_size, 1))\n",
        "        loss, _, _, _, _, _, _ = lossFunc(inputs, targets, hprev)\n",
        "        total_loss += loss\n",
        "        total_targets += len(targets)\n",
        "    return total_loss / total_targets\n",
        "\n",
        "\n",
        "def train():\n",
        "    \"\"\"\n",
        "    SGD with Adam training.\n",
        "    Each iteration of the SGD takes one review and then foward-backprop to update the parameters, using Adam.\n",
        "\n",
        "    The model parameters, the Adam variables, the epoches, n, and smooth_loss\n",
        "    should have been initialized somewhere outside train().\n",
        "    \"\"\"\n",
        "    global U, V, W, b, c\n",
        "    global mU, mV, mW, mb, mc\n",
        "    global epoches, n\n",
        "    global smooth_loss\n",
        "    global review_in_ix, valid_review_in_ix, test_review_in_ix\n",
        "\n",
        "    while True:\n",
        "        # prepare inputs and the targets.\n",
        "        if n == len(review_in_ix) or n == 0:\n",
        "            epoches += 1\n",
        "            n = 0\n",
        "\n",
        "        # They are of the same lengths, and targets is inputs shift to the right by one step with a special eos symbol\n",
        "        inputs = review_in_ix[n]\n",
        "        # print(inputs)\n",
        "        targets = inputs + [char_to_ix['eos']]\n",
        "        targets = targets[1:]\n",
        "\n",
        "        # forward pass\n",
        "        hprev = np.zeros((hidden_size, 1))\n",
        "        loss, dU, dV, dW, db, dc, hprev = lossFunc(inputs, targets, hprev)\n",
        "        loss /= len(inputs)\n",
        "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "        for param, dparam, mem in zip([U, V, W, b, c],\n",
        "                                      [dU, dV, dW, db, dc],\n",
        "                                      [mU, mV, mW, mb, mc]):\n",
        "            mem += dparam * dparam  # element-wise squares of the parameters\n",
        "            param -= learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
        "\n",
        "        # sample a sentence from the current model\n",
        "        if n % 2000 == 0:\n",
        "            ixes = sample(np.zeros_like(hprev), inputs[0], 200)\n",
        "            print(f'-----{[ix_to_char[ix] for ix in ixes]}-----')\n",
        "            valid_loss = run_model(valid_review_in_ix[:30])\n",
        "            test_loss = run_model(test_review_in_ix[:30])\n",
        "            print(f'iter {n}, training loss: {smooth_loss}, validation loss: {valid_loss}, test loss: {test_loss}')\n",
        "            save_model(epoches, n, {'U':U, 'V':V, 'W':W, 'b':b, 'c':c, 'mU':mU, 'mV': mV, 'mW': mW, 'mb': mb, 'mc': mc, 'smooth_loss':smooth_loss})\n",
        "\n",
        "        n += 1\n",
        "\n",
        "# Main codes for model training\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    used_pretrained = False\n",
        "\n",
        "    tr_reviews = read_reviews('train_small.csv')\n",
        "    reviews_to_ix(tr_reviews)\n",
        "    \n",
        "    test_reviews = read_reviews('test_small.csv')\n",
        "    test_review_in_ix = more_review_to_ix(test_reviews)\n",
        "\n",
        "    valid_reviews = read_reviews('valid_small.csv')\n",
        "    valid_review_in_ix = more_review_to_ix(valid_reviews)\n",
        "    \n",
        "    # optionally load a pre-trained model\n",
        "    if used_pretrained:\n",
        "        pre_trained_epoches = 1\n",
        "        pre_trained_iterations = 800\n",
        "        load_model(pre_trained_epoches, pre_trained_iterations)\n",
        "    else:\n",
        "        # randomly initialized the model\n",
        "        init_model()\n",
        "\n",
        "    # valid_loss = run_model(valid_review_in_ix[:30])\n",
        "    # test_loss = run_model(test_review_in_ix[:30])\n",
        "    # print(f'validation loss: {valid_loss}, test loss: {test_loss}')\n",
        "    train()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_small.csv\n",
            "15989 reviews loaded.\n",
            "15989 reviews; 94 unique chars (input_size); 9418362 chars in total\n",
            "test_small.csv\n",
            "4997 reviews loaded.\n",
            "valid_small.csv\n",
            "3998 reviews loaded.\n",
            "-----['-', 'v', 'X', '~', 'H', 'M', 'R', '~', 'U', '^', 'h', ')', '3', '!', 'r', 'E', 'r', '6', '6', '+', 'eos', '{', 'X', '/', 'n', 'M', 'B', 'X', 'l', '&', 'G', 'G', 'C', '`', 's', '2', '%', 't', 'x', ';', 'o', 'M', '.', '}', '9', '5', '1', 'q', 'K', '?', '-', '|', 'V', '2', '%', 'h', 'R', 'j', 'G', '>', '!', 'L', '7', 'V', 'A', '5', '_', '/', 'K', '.', 'K', '~', '7', 'I', 'M', 'l', 'g', '}', 'e', 'Y', 'D', '@', 'N', 'U', 'm', 'b', 'q', '0', 'b', 'N', '-', '=', 'Z', ';', 'g', 'W', 'B', 'W', '{', 'x', 'Q', 'V', 'M', \"'\", 'z', 'w', 'x', '?', '*', '^', 'h', 't', '`', 'K', 'j', '%', '4', '?', 'F', 'v', 'G', '7', 'U', 'v', 'M', 'T', 'S', 'W', '=', ' ', '8', 'm', 'z', '!', '0', 'f', 'C', '8', 'T', '6', '0', 'eos', 'M', 'T', '+', '}', 'G', 'u', '{', 'x', 'q', '}', 'm', 'F', 't', 'h', '1', 'b', 'W', '5', 'g', 'y', '1', '9', '!', 's', 'U', '0', '-', 'I', '=', '*', '^', 'o', 'M', 'x', 'f', '#', 'B', '&', 'Z', 'Y', 'U', ';', '5', 'n', '0', '(', '$', 'F', 't', 'F', '!', ',', 'd', 'D', '-', 's', '3', 't']-----\n",
            "iter 0, training loss: 4.543294914916102, validation loss: 4.543068166344041, test loss: 4.543043283012218\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}