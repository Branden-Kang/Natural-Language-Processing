{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Schedule Sampling Seq2Seq-Movie_Script Chatbot-PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFMBR3tM6Xf-",
        "colab_type": "code",
        "outputId": "16959f48-a33f-4ffe-dc7d-bb5208ae7edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "!pwd\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks/')\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content\n",
            "/content/gdrive/My Drive/Colab Notebooks\n",
            " assignment0.ipynb\n",
            "'assignment2_(release).ipynb'\n",
            " assignment2_sol.ipynb\n",
            " corpus\n",
            " datasets\n",
            "'Deep Q-learning.ipynb'\n",
            " f_test.dec\n",
            " f_test.enc\n",
            " f_train.dec\n",
            " f_train.enc\n",
            " Glove_solution.ipynb\n",
            " loss_female_epo200.npy\n",
            " loss_female_epo2.npy\n",
            " loss_female_epo40.npy\n",
            " loss_female_epo4.npy\n",
            " loss_female_epo5.npy\n",
            " loss_female_sche_sampling_epo1.npy\n",
            " loss_female_sche_sampling_epo200.npy\n",
            " loss_female_sche_sampling_epo2.npy\n",
            " loss_male_epo200.npy\n",
            " loss_male_epo2.npy\n",
            " loss_male_epo5.npy\n",
            " loss_male_prob_feed_epo200.npy\n",
            " loss_male_sche_sampling_epo200.npy\n",
            " loss.png\n",
            " main.py\n",
            "'Model 4 generator'\n",
            "'Model 5 RL'\n",
            " model_female_prob_feed_epo141.pth\n",
            " model_female_prob_feed_epo199.pth\n",
            " model_female_prob_feed_epo200.pth\n",
            " pth\n",
            " Schedule_Sampling_Seq2Seq-female.ipynb\n",
            "'Schedule Sampling Seq2Seq-Movie_Script Chatbot-PyTorch.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgVrbwvgurCw",
        "colab_type": "code",
        "outputId": "964f8c50-09af-48ad-d111-3b7c2cccde76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "# coding: utf-8\n",
        "# - try to implrment data.py\n",
        "corpus_path = 'corpus/'  #cornell movie-dialogs corpus\n",
        "save_path = 'datasets/'\n",
        "\n",
        "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz ' \n",
        "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\''\n",
        "\n",
        "limit = {\n",
        "        'maxq' : 25,\n",
        "        'minq' : 2,\n",
        "        'maxa' : 25,\n",
        "        'mina' : 2\n",
        "        }\n",
        "\n",
        "UNK = 'unk'\n",
        "VOCAB_SIZE = 8997\n",
        "\n",
        "# idx2w[0]='_'    ...zero padding\n",
        "# idx2w[1]='unk'\n",
        "# idx2w[2]='<G0>'\n",
        "# idx2w[3]='<EOS>'\n",
        "# total 9000\n",
        "\n",
        "import random\n",
        "import nltk\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "''' \n",
        "    1. Read from 'movie_characters_metadata.txt'\n",
        "    2. Create a dictionary with ( key = u_id, value = gender )\n",
        "'''\n",
        "def get_character2gender():\n",
        "    lines=open(corpus_path+'movie_characters_metadata.txt', encoding='utf-8', errors='ignore'\n",
        "               ).read().split('\\n') #encoding='utf-8', errors='ignore'\n",
        "    c2g = {} #Create a dictionary\n",
        "    for line in lines:\n",
        "        _line = line.split(' +++$+++ ')\n",
        "        if len(_line) == 6: #Conform to the format\n",
        "            c2g[ _line[0] ] = _line[4] #Update the dictionary\n",
        "            #_line[0]='u0', _line[4]='f'\n",
        "    return c2g\n",
        "\n",
        "''' \n",
        "    1. Read from 'movie-lines.txt'\n",
        "    2. Create a dictionary with ( key = line_id, value = text )\n",
        "'''\n",
        "def get_id2line():\n",
        "    lines=open(corpus_path+'movie_lines.txt', encoding='utf-8', errors='ignore'\n",
        "               ).read().split('\\n') #encoding='utf-8', errors='ignore'\n",
        "    id2line = {} #Create a dictionary\n",
        "    for line in lines:\n",
        "        _line = line.split(' +++$+++ ')\n",
        "        if len(_line) == 5: \n",
        "            id2line[ _line[0] ] = _line[4] #Update the dictionary\n",
        "            #_line[0]='L1045', _line[4]='They do not!'\n",
        "    return id2line\n",
        "\n",
        "''' \n",
        "    1. Read from 'movie-lines.txt'\n",
        "    2. Create a dictionary with ( key = line_id, value = gender )\n",
        "'''\n",
        "def get_id2gender(u2gender):\n",
        "    lines=open(corpus_path+'movie_lines.txt', encoding='utf-8', errors='ignore'\n",
        "               ).read().split('\\n') #encoding='utf-8', errors='ignore'\n",
        "    id2gender = {} #Create a dictionary\n",
        "    for line in lines:\n",
        "        _line = line.split(' +++$+++ ')\n",
        "        if len(_line) == 5:\n",
        "            id2gender[ _line[0] ] = u2gender [ _line[1] ] #Update the dictionary\n",
        "            #_line[0]='L1045', _line[4]='They do not!'\n",
        "    return id2gender\n",
        "\n",
        "'''\n",
        "    1. Read from 'movie_conversations.txt'\n",
        "    2. Create a list of [list of line_id's]\n",
        "'''\n",
        "def get_conversations():\n",
        "    conv_lines = open(corpus_path+'movie_conversations.txt', encoding='utf-8', errors='ignore'\n",
        "                      ).read().split('\\n') #encoding='utf-8', errors='ignore'\n",
        "    convs = [ ] #Create a list\n",
        "    for line in conv_lines[:-1]:\n",
        "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "        #_line[-1]= \"'L194', 'L195', 'L196', 'L197'\"\n",
        "        #[1:-1]: remove '[',']' \n",
        "        #_line = 'L194,L195,L196,L197'\n",
        "        #_line.split(',')=['L194', 'L195', 'L196', 'L197']\n",
        "        convs.append(_line.split(','))\n",
        "    return convs\n",
        "\n",
        "'''\n",
        "花時間!!暫時不需要!!!\n",
        "    1. Get each conversation\n",
        "    2. Get each line from conversation\n",
        "    3. Save each conversation to file\n",
        "'''\n",
        "def extract_conversations(convs,id2line,path=''):\n",
        "    idx = 0\n",
        "    for conv in convs:\n",
        "        f_conv = open(path + str(idx)+'.txt', 'w')#create file for each conv\n",
        "        for line_id in conv:\n",
        "            f_conv.write(id2line[line_id])\n",
        "            f_conv.write('\\n')\n",
        "        f_conv.close()\n",
        "        idx += 1\n",
        "\n",
        "'''\n",
        "    Get lists of all conversations as Questions and Answers\n",
        "    1. [questions]\n",
        "    2. [answers]\n",
        "'''\n",
        "def gather_dataset(convs, id2line):\n",
        "    questions = []; answers = []\n",
        "\n",
        "    for conv in convs:\n",
        "        '''\n",
        "        #Put['L194', 'L195', 'L196', 'L197']Divide\n",
        "        #['L194', 'L195']和[ 'L196', 'L197']\n",
        "        #130,000 total\n",
        "        if len(conv) %2 != 0:\n",
        "            conv = conv[:-1] #Only get an even number\n",
        "        for i in range(len(conv)):\n",
        "            if i%2 == 0:\n",
        "                questions.append(id2line[conv[i]])\n",
        "            else:\n",
        "                answers.append(id2line[conv[i]])\n",
        "        '''\n",
        "        for i in range(len(conv)-1):\n",
        "            questions.append(id2line[conv[i]])\n",
        "            answers.append(id2line[conv[i+1]])  \n",
        "            #Put['L194', 'L195', 'L196', 'L197']Divide\n",
        "            #['L194', 'L195'],['L195', 'L196'] with [ 'L196', 'L197']\n",
        "            #220,000 pens\n",
        "\n",
        "    return questions, answers\n",
        "\n",
        "'''\n",
        "    Get lists of Pratial conversations as Questions and Answers By Gender\n",
        "    1. [questions]\n",
        "    2. [answers]\n",
        "'''\n",
        "def gather_dataset_by_gender(convs, id2line, id2gender,\n",
        "                             que_gender='?', ans_gender='?' ): \n",
        "    #if ans_gender = 'f', just female answer\n",
        "    #if ans_gender = 'm', just male answer\n",
        "    #if ans_gender = '?', both male/female answer, including gender unknown\n",
        "    \n",
        "    questions = []; answers = []\n",
        "\n",
        "    for conv in convs:\n",
        "        for i in range(len(conv)-1):\n",
        "            # check gender\n",
        "            bool_1 = que_gender == id2gender[conv[i]] \n",
        "            bool_2 = ans_gender == id2gender[conv[i+1]] \n",
        "            bool_3 = que_gender == '?' \n",
        "            bool_4 = ans_gender == '?' \n",
        "            bool_que = bool_1 or bool_3 \n",
        "            bool_ans = bool_2 or bool_4 \n",
        "            if ( bool_que and bool_ans ) :    \n",
        "                questions.append(id2line[conv[i]])\n",
        "                answers.append(id2line[conv[i+1]])  \n",
        "                #Put ['L194', 'L195', 'L196', 'L197'] Divide\n",
        "                #['L194', 'L195'],['L195', 'L196'] iwth [ 'L196', 'L197']\n",
        "                #220,000 pens\n",
        "    return questions, answers\n",
        "\n",
        "'''\n",
        "暫時沒用到!!!\n",
        "    Shuffle and split to train/test dataset\n",
        "    We need 4 files\n",
        "        1. train.enc : Encoder input for training\n",
        "        2. train.dec : Decoder input for training\n",
        "        3. test.enc  : Encoder input for testing\n",
        "        4. test.dec  : Decoder input for testing\n",
        "    -> Encoder input is question, and Decoder input is answer \n",
        "'''\n",
        "def prepare_seq2seq_files(questions, answers, gender = 'f' ,\n",
        "                          path='',TESTSET_SIZE = 10000):\n",
        "    \n",
        "    # open files\n",
        "    train_enc = open(path + gender+'_train.enc','w')\n",
        "    train_dec = open(path + gender+'_train.dec','w')\n",
        "    test_enc  = open(path + gender+'_test.enc', 'w')\n",
        "    test_dec  = open(path + gender+'_test.dec', 'w')\n",
        "\n",
        "    # choose 10,000 (TESTSET_SIZE) items to put into testset\n",
        "    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n",
        "\n",
        "    for i in range(len(questions)):\n",
        "        if i in test_ids:\n",
        "            test_enc.write(questions[i]+'\\n')\n",
        "            test_dec.write(answers[i]+ '\\n' )\n",
        "        else:\n",
        "            train_enc.write(questions[i]+'\\n')\n",
        "            train_dec.write(answers[i]+ '\\n' )\n",
        "        if i%10000 == 0:\n",
        "            print('>> written {} lines'.format(i))\n",
        "\n",
        "    # close files\n",
        "    train_enc.close()\n",
        "    train_dec.close()\n",
        "    test_enc.close()\n",
        "    test_dec.close()\n",
        "    print('>>written finish')\n",
        "\n",
        "'''\n",
        " remove anything that isn't in the vocabulary\n",
        "    return str(pure en)\n",
        "'''\n",
        "def filter_line(line, whitelist): # whitelist:Safe list / only left whitelist\n",
        "    return ''.join([ ch for ch in line if ch in whitelist ])\n",
        "\n",
        "#filter_line('fweijwhf8328r8uAAAgerg19,,',EN_WHITELIST)\n",
        "\n",
        "'''\n",
        " filter too long and too short sequences\n",
        "    return tuple( filtered_ta, filtered_en )\n",
        "'''\n",
        "def filter_data(qseq, aseq):\n",
        "    filtered_q, filtered_a = [], []\n",
        "    raw_data_len = len(qseq)\n",
        "\n",
        "    assert len(qseq) == len(aseq)\n",
        "\n",
        "    for i in range(raw_data_len):\n",
        "        qlen, alen = len(qseq[i].split(' ')), len(aseq[i].split(' '))\n",
        "        if qlen >= limit['minq'] and qlen <= limit['maxq']:\n",
        "            if alen >= limit['mina'] and alen <= limit['maxa']:\n",
        "                filtered_q.append(qseq[i])\n",
        "                filtered_a.append(aseq[i])\n",
        "\n",
        "    # print the fraction of the original data, filtered\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n",
        "    print(str(filtered) + '% filtered from original data')\n",
        "\n",
        "    return filtered_q, filtered_a\n",
        "\n",
        "'''\n",
        " read list of words, create index to word,\n",
        "  word to index dictionaries\n",
        "    return tuple( vocab->(word, count), idx2w, w2idx )\n",
        "'''\n",
        "def index_(tokenized_sentences, vocab_size):\n",
        "    # get frequency distribution\n",
        "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    # get vocabulary of 'vocab_size' most used words\n",
        "    vocab = freq_dist.most_common(vocab_size)\n",
        "    # index2word (as a list)\n",
        "    index2word = ['_'] + [UNK] + [ x[0] for x in vocab ]\n",
        "    # word2index (as a dict)\n",
        "    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n",
        "    return index2word, word2index, freq_dist\n",
        "\n",
        "'''\n",
        " filter based on number of unknowns (words not in vocabulary)\n",
        "  filter out the worst sentences\n",
        "'''\n",
        "def filter_unk(qtokenized, atokenized, w2idx):\n",
        "    data_len = len(qtokenized)\n",
        "\n",
        "    filtered_q, filtered_a = [], []\n",
        "\n",
        "    for qline, aline in zip(qtokenized, atokenized):\n",
        "        unk_count_q = len([ w for w in qline if w not in w2idx ])\n",
        "        unk_count_a = len([ w for w in aline if w not in w2idx ])\n",
        "        if unk_count_a <= 2:\n",
        "            if unk_count_q > 0:\n",
        "                if unk_count_q/len(qline) > 0.2:\n",
        "                    pass\n",
        "            filtered_q.append(qline)\n",
        "            filtered_a.append(aline)\n",
        "\n",
        "    # print the fraction of the original data, filtered\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered = int((data_len - filt_data_len)*100/data_len)\n",
        "    print(str(filtered) + '% filtered from original data')\n",
        "\n",
        "    return filtered_q, filtered_a\n",
        "\n",
        "'''\n",
        " create the final dataset : \n",
        "  - convert list of items to arrays of indices\n",
        "  - add zero padding\n",
        "      return ( [array_en([indices]), array_ta([indices]) )\n",
        " \n",
        "'''\n",
        "def zero_pad(qtokenized, atokenized, w2idx):\n",
        "    # num of rows\n",
        "    data_len = len(qtokenized)\n",
        "\n",
        "    # numpy arrays to store indices\n",
        "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32) \n",
        "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
        "\n",
        "    for i in range(data_len):\n",
        "        q_indices = pad_seq(qtokenized[i], w2idx, limit['maxq'])\n",
        "        a_indices = pad_seq(atokenized[i], w2idx, limit['maxa'])\n",
        "\n",
        "        #print(len(idx_q[i]), len(q_indices))\n",
        "        #print(len(idx_a[i]), len(a_indices))\n",
        "        idx_q[i] = np.array(q_indices)\n",
        "        idx_a[i] = np.array(a_indices)\n",
        "\n",
        "    return idx_q, idx_a\n",
        "\n",
        "'''\n",
        " replace words with indices in a sequence\n",
        "  replace with unknown if word not in lookup\n",
        "    return [list of indices]\n",
        "'''\n",
        "def pad_seq(seq, lookup, maxlen):\n",
        "    indices = []\n",
        "    for word in seq:\n",
        "        if word in lookup:\n",
        "            indices.append(lookup[word])\n",
        "        else:\n",
        "            indices.append(lookup[UNK])\n",
        "    return indices + [0]*(maxlen - len(seq))\n",
        "\n",
        "'''\n",
        "    main process\n",
        "    for chatbot\n",
        "'''\n",
        "def process_data(): \n",
        "    \n",
        "    u2gender = get_character2gender()\n",
        "    print('>> gathered u2gender dictionary.\\n')\n",
        "    id2line = get_id2line()\n",
        "    print('>> gathered id2line dictionary.\\n')\n",
        "    id2gender = get_id2gender(u2gender)\n",
        "    print('>> gathered id2gender dictionary.\\n')\n",
        "    convs = get_conversations() # [ ['L750', 'L751'], [...] ,... ]\n",
        "    print('>> gathered all conversations.\\n')\n",
        "    #questions, answers = gather_dataset(convs,id2line)\n",
        "    f_questions, f_answers = gather_dataset_by_gender(convs, \n",
        "                                                  id2line, id2gender,\n",
        "                                                  ans_gender='f')\n",
        "    m_questions, m_answers = gather_dataset_by_gender(convs, \n",
        "                                                  id2line, id2gender,\n",
        "                                                  ans_gender='m')\n",
        "    print('\\n Female dataset len : ' + str(len(f_questions)))\n",
        "    print('\\n Male dataset len : ' + str(len(m_questions)))\n",
        "    \n",
        "    \n",
        "    # change to lower case (just for en)\n",
        "    f_questions = [ line.lower() for line in f_questions ]\n",
        "    f_answers = [ line.lower() for line in f_answers ]\n",
        "    m_questions = [ line.lower() for line in m_questions ]\n",
        "    m_answers = [ line.lower() for line in m_answers ]\n",
        "    \n",
        "    # filter out unnecessary characters #Blanks cannot be filtered out, otherwise they cannot be separated\n",
        "    # Make sure all characters in the sentence are on the safe list\n",
        "    print('\\n>> Filter lines')\n",
        "    f_questions = [ filter_line(line, EN_WHITELIST) for line in f_questions ]\n",
        "    f_answers = [ filter_line(line, EN_WHITELIST) for line in f_answers ]\n",
        "    m_questions = [ filter_line(line, EN_WHITELIST) for line in m_questions ]\n",
        "    m_answers = [ filter_line(line, EN_WHITELIST) for line in m_answers ]\n",
        "    \n",
        "    # filter out too long or too short sequences\n",
        "    print('\\n>> Filter out too long or too short sequences')\n",
        "    f_qlines, f_alines = filter_data(f_questions, f_answers)\n",
        "    m_qlines, m_alines = filter_data(m_questions, m_answers)\n",
        "    print('\\n Female dataset len : ' + str(len(f_qlines)))\n",
        "    print('\\n Male dataset len : ' + str(len(m_qlines)))\n",
        "\n",
        "    print('\\n Before token : ')\n",
        "    for q,a in zip(f_qlines[141:145], f_alines[141:145]):\n",
        "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
        "\n",
        "    # tokenize: convert list of [lines of text] into list of [list of words ]\n",
        "    print('\\n>> Segment lines into words')\n",
        "    f_qtokenized = [ [w.strip() for w in wordlist.split(' ') if w] \n",
        "                  for wordlist in f_qlines ]\n",
        "    f_atokenized = [ [w.strip() for w in wordlist.split(' ') if w] \n",
        "                  for wordlist in f_alines ]\n",
        "    m_qtokenized = [ [w.strip() for w in wordlist.split(' ') if w] \n",
        "                  for wordlist in m_qlines ]\n",
        "    m_atokenized = [ [w.strip() for w in wordlist.split(' ') if w] \n",
        "                  for wordlist in m_alines ]\n",
        "    print('\\n:: Sample from segmented list of words')\n",
        "    \n",
        "    print('\\n After token : ')\n",
        "    for q,a in zip(f_qtokenized[141:145], f_atokenized[141:145]):\n",
        "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
        "\n",
        "    # indexing -> idx2w, w2idx \n",
        "    all_tokenized = f_qtokenized + f_atokenized + m_qtokenized + m_atokenized\n",
        "    print('\\n >> Index words')\n",
        "    idx2w, w2idx, freq_dist = index_( all_tokenized, \n",
        "                                     vocab_size=VOCAB_SIZE)\n",
        "    \n",
        "    # filter out sentences with too many unknown words\n",
        "    print('\\n >> Filter out sentences with too many unknown words')\n",
        "    f_qtokenized, f_atokenized = filter_unk(f_qtokenized, f_atokenized, w2idx)\n",
        "    m_qtokenized, m_atokenized = filter_unk(m_qtokenized, m_atokenized, w2idx)\n",
        "    print('\\n Final Female dataset len : ' + str(len(f_qtokenized)))\n",
        "    print('\\n Final Male dataset len : ' + str(len(m_qtokenized)))\n",
        "\n",
        "    print('\\n >> Zero Padding')\n",
        "    f_idx_q, f_idx_a = zero_pad(f_qtokenized, f_atokenized, w2idx)\n",
        "    m_idx_q, m_idx_a = zero_pad(m_qtokenized, m_atokenized, w2idx)\n",
        "    \n",
        "    print('\\n >> Save numpy arrays to disk')\n",
        "    # save them\n",
        "    np.save(save_path+'f_idx_q.npy', f_idx_q)\n",
        "    np.save(save_path+'f_idx_a.npy', f_idx_a)\n",
        "    np.save(save_path+'m_idx_q.npy', m_idx_q)\n",
        "    np.save(save_path+'m_idx_a.npy', m_idx_a)\n",
        "    \n",
        "    # let us now save the necessary dictionaries\n",
        "    metadata = {\n",
        "            'w2idx' : w2idx,\n",
        "            'idx2w' : idx2w,\n",
        "            'limit' : limit,\n",
        "            'freq_dist' : freq_dist\n",
        "                }\n",
        "\n",
        "    # write to disk : data control dictionaries\n",
        "    with open(save_path+'metadata.pkl', 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    # count of unknowns\n",
        "    unk_count = (f_idx_q == 1).sum() + (f_idx_a == 1).sum() + (m_idx_q == 1).sum() + (m_idx_a == 1).sum()\n",
        "    # count of words\n",
        "    word_count = (f_idx_q > 1).sum() + (f_idx_a > 1).sum() + (m_idx_q > 1).sum() + (m_idx_a > 1).sum()\n",
        "\n",
        "    print('% unknown : {0}'.format(100 * (unk_count/word_count)))\n",
        "    print('Female Dataset count : ' + str(f_idx_q.shape[0]))\n",
        "    print('Male Dataset count : ' + str(m_idx_q.shape[0]))\n",
        "\n",
        "    #print '>> gathered questions and answers.\\n'\n",
        "    #prepare_seq2seq_files(f_idx_q,f_idx_a,'f')\n",
        "    #prepare_seq2seq_files(m_idx_q,m_idx_a,'m')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    process_data()\n",
        "\n",
        "'''\n",
        "    load_data\n",
        "    for chatbot\n",
        "'''\n",
        "def load_data(PATH=''):\n",
        "    # read data control dictionaries\n",
        "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)\n",
        "    # read numpy arrays\n",
        "    f_idx_q = np.load(PATH + 'f_idx_q.npy')\n",
        "    f_idx_a = np.load(PATH + 'f_idx_a.npy')\n",
        "    m_idx_q = np.load(PATH + 'm_idx_q.npy')\n",
        "    m_idx_a = np.load(PATH + 'm_idx_a.npy')\n",
        "    return metadata, f_idx_q, f_idx_a, m_idx_q, m_idx_a\n",
        "\n",
        "def load_data_female(PATH=''):\n",
        "    # read numpy arrays\n",
        "    f_idx_q = np.load(PATH + 'f_idx_q.npy')\n",
        "    f_idx_a = np.load(PATH + 'f_idx_a.npy')\n",
        "    return f_idx_q, f_idx_a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> gathered u2gender dictionary.\n",
            "\n",
            ">> gathered id2line dictionary.\n",
            "\n",
            ">> gathered id2gender dictionary.\n",
            "\n",
            ">> gathered all conversations.\n",
            "\n",
            "\n",
            " Female dataset len : 48650\n",
            "\n",
            " Male dataset len : 113671\n",
            "\n",
            ">> Filter lines\n",
            "\n",
            ">> Filter out too long or too short sequences\n",
            "29% filtered from original data\n",
            "29% filtered from original data\n",
            "\n",
            " Female dataset len : 34164\n",
            "\n",
            " Male dataset len : 79827\n",
            "\n",
            " Before token : \n",
            "q : [so tell me about this dance was it fun]; a : [parts of it]\n",
            "q : [which parts]; a : [the part where bianca beat the hell out of some guy]\n",
            "q : [bianca did what]; a : [whats the matter  upset that i rubbed off on her]\n",
            "q : [katarina stratford  my my  youve been terrorizing ms blaise again]; a : [expressing my opinion is not a terrorist action]\n",
            "\n",
            ">> Segment lines into words\n",
            "\n",
            ":: Sample from segmented list of words\n",
            "\n",
            " After token : \n",
            "q : [['so', 'tell', 'me', 'about', 'this', 'dance', 'was', 'it', 'fun']]; a : [['parts', 'of', 'it']]\n",
            "q : [['which', 'parts']]; a : [['the', 'part', 'where', 'bianca', 'beat', 'the', 'hell', 'out', 'of', 'some', 'guy']]\n",
            "q : [['bianca', 'did', 'what']]; a : [['whats', 'the', 'matter', 'upset', 'that', 'i', 'rubbed', 'off', 'on', 'her']]\n",
            "q : [['katarina', 'stratford', 'my', 'my', 'youve', 'been', 'terrorizing', 'ms', 'blaise', 'again']]; a : [['expressing', 'my', 'opinion', 'is', 'not', 'a', 'terrorist', 'action']]\n",
            "\n",
            " >> Index words\n",
            "\n",
            " >> Filter out sentences with too many unknown words\n",
            "1% filtered from original data\n",
            "1% filtered from original data\n",
            "\n",
            " Final Female dataset len : 33681\n",
            "\n",
            " Final Male dataset len : 78382\n",
            "\n",
            " >> Zero Padding\n",
            "\n",
            " >> Save numpy arrays to disk\n",
            "% unknown : 3.530673927105614\n",
            "Female Dataset count : 33681\n",
            "Male Dataset count : 78382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjyEARtR6NXd",
        "colab_type": "code",
        "outputId": "80e3e57b-bc98-46ee-8038-bd07bf28150c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " assignment0.ipynb\n",
            "'assignment2_(release).ipynb'\n",
            " assignment2_sol.ipynb\n",
            " corpus\n",
            " datasets\n",
            "'Deep Q-learning.ipynb'\n",
            " f_test.dec\n",
            " f_test.enc\n",
            " f_train.dec\n",
            " f_train.enc\n",
            " Glove_solution.ipynb\n",
            " loss_female_epo200.npy\n",
            " loss_female_epo2.npy\n",
            " loss_female_epo40.npy\n",
            " loss_female_epo4.npy\n",
            " loss_female_epo5.npy\n",
            " loss_female_sche_sampling_epo1.npy\n",
            " loss_female_sche_sampling_epo200.npy\n",
            " loss_female_sche_sampling_epo2.npy\n",
            " loss_male_epo200.npy\n",
            " loss_male_epo2.npy\n",
            " loss_male_epo5.npy\n",
            " loss_male_prob_feed_epo200.npy\n",
            " loss.png\n",
            " main.py\n",
            "'Model 4 generator'\n",
            "'Model 5 RL'\n",
            " model_female_prob_feed_epo141.pth\n",
            " model_female_prob_feed_epo199.pth\n",
            " model_female_prob_feed_epo200.pth\n",
            " pth\n",
            " Schedule_Sampling_Seq2Seq-female.ipynb\n",
            "'Schedule Sampling Seq2Seq-Movie_Script Chatbot-PyTorch.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrCwWIIf_O5",
        "colab_type": "text"
      },
      "source": [
        "Data_add_EOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnNVH3qkgBlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7XnxpaWgTxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(PATH='datasets/'):\n",
        "    # read data control dictionaries\n",
        "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)\n",
        "    # read numpy arrays\n",
        "    f_idx_q = np.load(PATH + 'f_idx_q.npy')\n",
        "    f_idx_a = np.load(PATH + 'f_idx_a.npy')\n",
        "    m_idx_q = np.load(PATH + 'm_idx_q.npy')\n",
        "    m_idx_a = np.load(PATH + 'm_idx_a.npy')\n",
        "    return metadata, f_idx_q, f_idx_a, m_idx_q, m_idx_a\n",
        "\n",
        "metadata, f_idx_q, f_idx_a, m_idx_q, m_idx_a = load_data(PATH='datasets/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6v64hBngUze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_EOS_metadata(meta, save_path='datasets/'):\n",
        "    \n",
        "    # metadata\n",
        "    idx2w = meta['idx2w']\n",
        "    w2idx = meta['w2idx']\n",
        "    eos_idx = len(w2idx)  # eos_idx=8002\n",
        "    idx2w.append('<EOS>')\n",
        "    w2idx.update({'<EOS>': eos_idx})\n",
        "    metadata = {\n",
        "            'w2idx' : w2idx,\n",
        "            'idx2w' : idx2w,\n",
        "                }\n",
        "    with open(save_path+'metadata_1.pkl', 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "        \n",
        "    return eos_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aLnb8PzgXZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_EOS_line(idxs, eos_idx=8002):\n",
        "    new_idxs = np.copy(idxs)\n",
        "    for i,line in enumerate(idxs):\n",
        "        zreo_pos_arr = np.where(line==0)[0]\n",
        "        if len(zreo_pos_arr)>0:\n",
        "            new_idxs[i,25-len(zreo_pos_arr)]=eos_idx\n",
        "    return new_idxs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvtOk9YygZeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def idxs_add_EOS(f_idx_q, f_idx_a, m_idx_q, m_idx_a, eos_idx=8002, save_path='datasets/'):\n",
        "    \n",
        "    f_idx_q_1 = add_EOS_line(f_idx_q, eos_idx=eos_idx)\n",
        "    f_idx_a_1 = add_EOS_line(f_idx_a, eos_idx=eos_idx)\n",
        "    m_idx_q_1 = add_EOS_line(m_idx_q, eos_idx=eos_idx)\n",
        "    m_idx_a_1 = add_EOS_line(m_idx_a, eos_idx=eos_idx)\n",
        "    np.save(save_path+'f_idx_q_1.npy', f_idx_q_1)\n",
        "    np.save(save_path+'f_idx_a_1.npy', f_idx_a_1)\n",
        "    np.save(save_path+'m_idx_q_1.npy', m_idx_q_1)\n",
        "    np.save(save_path+'m_idx_a_1.npy', m_idx_a_1)\n",
        "    \n",
        "idxs_add_EOS(f_idx_q, f_idx_a, m_idx_q, m_idx_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9R6ow5kgbnN",
        "colab_type": "code",
        "outputId": "3b055be3-c77f-4b1b-a87a-da7b3816a30d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def add_EOS(path='datasets/'):\n",
        "    metadata, f_idx_q, f_idx_a, m_idx_q, m_idx_a  = load_data()\n",
        "    eos_idx = add_EOS_metadata(metadata)\n",
        "    idxs_add_EOS(f_idx_q, f_idx_a, m_idx_q, m_idx_a, eos_idx=eos_idx)\n",
        "    \n",
        "    \n",
        "    f_idx_q_1 = np.load('datasets/f_idx_q_1.npy')\n",
        "    print(f_idx_q_1 [0])\n",
        "    print(f_idx_q [0])\n",
        "    print(f_idx_q_1 [350])\n",
        "    print(f_idx_q [350])\n",
        "    \n",
        "add_EOS()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  41    3  133  666  327   34    1   46   44  108   34    2 8999    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0]\n",
            "[ 41   3 133 666 327  34   1  46  44 108  34   2   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0]\n",
            "[   3   14   18   20    4  674  284    3  180    3   33  133    2   18\n",
            "  666 2753    6  212 1750   31 1386   35   27  309    5]\n",
            "[   3   14   18   20    4  674  284    3  180    3   33  133    2   18\n",
            "  666 2753    6  212 1750   31 1386   35   27  309    5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqvl0cpSjnLS",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq-female"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h1XPoun4Rlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecqCzLW0iCg9",
        "colab_type": "text"
      },
      "source": [
        "### Data Handling\n",
        "load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFRY62_WiIlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = np.load(\"./datasets/f_idx_a_1.npy\")\n",
        "ques = np.load(\"./datasets/f_idx_q_1.npy\")\n",
        "\n",
        "with open('./datasets/metadata_1.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkP2P20b4Vd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FemaleDataset(data.Dataset): \n",
        "    def __init__(self,ques,ans):\n",
        "        self.ques = ques\n",
        "        self.ans = ans\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ques_tensor = torch.from_numpy(self.ques[index]).long()\n",
        "        ans_tensor = torch.from_numpy(self.ans[index]).long()\n",
        "        \n",
        "        return ques_tensor , ans_tensor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return 33589      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO7xcWBY4ciM",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8JIfvIS4Xjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "female_dataset = FemaleDataset(ques,ans)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=female_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3XCz0uqnJbB",
        "colab_type": "code",
        "outputId": "48b72d8b-a5a8-47a4-c76f-05fa4b74ac4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "GO = Variable(torch.zeros(32,1,256)).cuda()\n",
        "GO.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_a0hFpwnUU5",
        "colab_type": "code",
        "outputId": "d92325f2-9823-47b9-a806-a4c0324e1fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inputs = GO\n",
        "inputs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-RvdzOkloWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_voc_size=9000,\n",
        "                 trg_voc_size=9000,\n",
        "                 src_embedding_size=256,\n",
        "                 trg_embedding_size=256,\n",
        "                 enc_hidden_size=200,\n",
        "                 dec_hidden_size=200):\n",
        "        \n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.trg_embedding_size = trg_embedding_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "        \n",
        "        self.src_embedder = nn.Embedding(src_voc_size , src_embedding_size)\n",
        "        self.encoder = nn.LSTM(src_embedding_size ,enc_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        \n",
        "        self.trg_embedder = nn.Embedding(trg_voc_size , trg_embedding_size)\n",
        "        self.decoder = nn.LSTM(trg_embedding_size ,dec_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        self.cls = nn.Linear(dec_hidden_size , trg_voc_size)\n",
        "    \n",
        "    def forward(self,source,target,feed_previous=False):\n",
        "        batch_size = source.size()[0]\n",
        "        src_em = self.src_embedder(source)\n",
        "        trg_em = self.trg_embedder(target)\n",
        "        \n",
        "        _ , enc_state = self.encoder(src_em)\n",
        "        \n",
        "        GO = Variable(torch.zeros(batch_size,1,self.trg_embedding_size)).cuda()\n",
        "        dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "        if feed_previous: #test phase\n",
        "            logits_ = []\n",
        "#            inputs = torch.unsqueeze(dec_in[:,i,:],1)\n",
        "            h = enc_state\n",
        "            for i in range(25):\n",
        "                inputs = torch.unsqueeze(dec_in[:,i,:],1)\n",
        "                output , h = self.decoder(inputs,h)\n",
        "                logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "                logits_.append(logits)\n",
        "                \n",
        "                predicted = logits.max(1)[1]\n",
        "                inputs = self.trg_embedder(predicted)\n",
        "                    \n",
        "            return torch.cat(logits_,0)\n",
        "\n",
        "        else: #train phase\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "            outputs , _ = self.decoder(dec_in,enc_state)\n",
        "            outputs = outputs.contiguous().view(-1,self.dec_hidden_size)\n",
        "            logits = self.cls(outputs)\n",
        "        \n",
        "            return logits\n",
        "\n",
        "        # else: #train phase -- schedule sampling\n",
        "        #     '''\n",
        "        #     dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)  # trg_em.shape=(batch_size, time step, trg_embedding_size )\n",
        "        #     outputs , _ = self.decoder(dec_in,enc_state)\n",
        "        #     outputs = outputs.contiguous().view(-1,self.dec_hidden_size)\n",
        "        #     logits = self.cls(outputs)        \n",
        "        #     return logits  \n",
        "        #     '''\n",
        "        #     logits_ = []\n",
        "        #     dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "            \n",
        "        #     h = enc_state\n",
        "        #     for i in range(25):\n",
        "        #         inputs = torch.unsqueeze(dec_in[:,i,:],1)\n",
        "                \n",
        "        #         if i < 5:\n",
        "        #             inputs =  torch.unsqueeze(dec_in[:,i,:],1)\n",
        "        #         else:\n",
        "        #             if random.random() < 0.5 : \n",
        "        #                 inputs =  torch.unsqueeze(dec_in[:,i,:],1)  #usual training policy \n",
        "        #             else:\n",
        "        #                 inputs = self.trg_embedder(predicted)  #schedule sampling\n",
        "                \n",
        "        #         output , h = self.decoder(inputs,h)\n",
        "        #         logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "        #         logits_.append(logits)\n",
        "        #         predicted = logits.max(1)[1]  # predicted.shape=(batch_size, time step=1)\n",
        "                \n",
        "        #     return torch.cat(logits_,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFTwt1Q-4jUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Seq2Seq(nn.Module):\n",
        "#     def __init__(self,\n",
        "#                  src_voc_size=9000,\n",
        "#                  trg_voc_size=9000,\n",
        "#                  src_embedding_size=256,\n",
        "#                  trg_embedding_size=256,\n",
        "#                  enc_hidden_size=200,\n",
        "#                  dec_hidden_size=200):\n",
        "        \n",
        "#         super(Seq2Seq, self).__init__()\n",
        "#         self.trg_embedding_size = trg_embedding_size\n",
        "#         self.dec_hidden_size = dec_hidden_size\n",
        "        \n",
        "#         self.src_embedder = nn.Embedding(src_voc_size , src_embedding_size)\n",
        "#         self.encoder = nn.LSTM(src_embedding_size ,enc_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        \n",
        "#         self.trg_embedder = nn.Embedding(trg_voc_size , trg_embedding_size)\n",
        "#         self.decoder = nn.LSTM(trg_embedding_size ,dec_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "#         self.cls = nn.Linear(dec_hidden_size , trg_voc_size)\n",
        "    \n",
        "#     def forward(self,source,target,feed_previous=False):\n",
        "#         batch_size = source.size()[0]\n",
        "#         src_em = self.src_embedder(source)\n",
        "#         trg_em = self.trg_embedder(target)\n",
        "        \n",
        "#         _ , enc_state = self.encoder(src_em)\n",
        "        \n",
        "#         GO = Variable(torch.zeros(batch_size,1,self.trg_embedding_size)).cuda()\n",
        "        \n",
        "#         if feed_previous: #test phase\n",
        "#             logits_ = []\n",
        "#             inputs = GO\n",
        "#             h = enc_state\n",
        "#             for i in range(25):\n",
        "#                 inputs = torch.cat([GO,trg_em[:,:-1,:]],1) # I modify this part\n",
        "#                 output , h = self.decoder(inputs,h)\n",
        "#                 logits = self.cls(output.reshape(-1, self.dec_hidden_size))\n",
        "# #                logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "#                 logits_.append(logits)\n",
        "                \n",
        "#                 predicted = logits.max(1)[1]\n",
        "#                 inputs = self.trg_embedder(predicted)\n",
        "                    \n",
        "#             return torch.cat(logits_,0)      \n",
        "            \n",
        "#         else: #train phase\n",
        "#             dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "#             outputs , _ = self.decoder(dec_in,enc_state)\n",
        "#             outputs = outputs.contiguous().view(-1,self.dec_hidden_size)\n",
        "#             logits = self.cls(outputs)\n",
        "        \n",
        "#             return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFg9w5K64laP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0UsNiy4nX7",
        "colab_type": "code",
        "outputId": "3990fd7e-c0d0-4c63-813e-206c749f9a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (src_embedder): Embedding(9000, 256)\n",
              "  (encoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (trg_embedder): Embedding(9000, 256)\n",
              "  (decoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (cls): Linear(in_features=200, out_features=9000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61w936zd4o7I",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yoXKR-z4qsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_op = optim.Adam(model.parameters() ,lr=3e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOdYck3t4ssh",
        "colab_type": "code",
        "outputId": "5d035e91-015b-420e-dc4b-6d047cbd7687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "epochs = 200\n",
        "loss_hist = []\n",
        "loss_ = 3\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_mean_loss = []\n",
        "\n",
        "    for i , (q,a) in enumerate(train_loader):\n",
        "        q = Variable(q).cuda()\n",
        "        a = Variable(a).cuda()\n",
        "   \n",
        "        logits = model(q,a,feed_previous=False)\n",
        "        _,predict = logits.max(1)\n",
        "        \n",
        "        loss = F.cross_entropy(logits ,a.view(-1))\n",
        "        train_op.zero_grad()\n",
        "        loss.backward()\n",
        "        train_op.step()\n",
        "#        print('loss', loss.data.item())\n",
        "        epoch_mean_loss.append(loss.data.item())\n",
        "    \n",
        "    loss_ = np.mean(epoch_mean_loss)\n",
        "    loss_hist.append(loss_)\n",
        "    if epoch % 10 == 0  or epoch == epochs-1:\n",
        "        print(\"epoch:%s , loss:%s\" % (epoch , loss_ ))\n",
        "    if epoch % 50 == 0 or epoch == epochs-1:\n",
        "        torch.save(model.state_dict() , 'pth/model_female_epo%s.pth'%epoch) #save model\n",
        "        \n",
        "np.save('loss_female_epo%s.npy'%epochs,loss_hist)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 , loss:2.3142165168126425\n",
            "epoch:1 , loss:1.9915728185290382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8aZL97I4vO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,predict = logits.max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZe8pDwW4z5w",
        "colab_type": "text"
      },
      "source": [
        "### Print Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBiIRZF-4xnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self,idx2word,word2idx):\n",
        "        self.idx2word = idx2word\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = 25\n",
        "        self.eos_idx = 8002\n",
        "        self.EN_WHITELIST  = '0123456789abcdefghijklmnopqrstuvwxyz '             \n",
        "            \n",
        "    '''\n",
        "    idx -> word with EOS\n",
        "    '''        \n",
        "    def decode_line(self,sentence_idx,remove_pad=True,remove_eos=True):  #sentence_idx: 1d_matrix     \n",
        "        sentence = []\n",
        "        for w in sentence_idx:\n",
        "            if remove_eos and w==self.eos_idx:\n",
        "                continue\n",
        "            if remove_pad and w==0 : \n",
        "                continue\n",
        "            sentence.append(self.idx2word[w])\n",
        "            #if w==self.eos_idx:\n",
        "            #    break\n",
        "        sentence = ' '.join(sentence)\n",
        "        return sentence\n",
        "    \n",
        "    def decode(self,sentence_idxs,remove_pad=True,remove_eos=True): #sentence_idxs: 2d_matrix \n",
        "        sentences = []\n",
        "        for s in sentence_idxs: \n",
        "            sentences.append(self.decode_line(s,\n",
        "                                              remove_pad=remove_pad,\n",
        "                                              remove_eos=remove_eos))\n",
        "        return sentences\n",
        "            \n",
        "    '''\n",
        "    word -> idx with EOS\n",
        "    '''\n",
        "    def encode_line(self,sentence):  #sentence: 1d_matrix\n",
        "        sentence = sentence.lower()\n",
        "        s_list = ''.join([ ch for ch in sentence if ch in self.EN_WHITELIST ]).split()\n",
        "        sentence_idx = []\n",
        "        for w in s_list:\n",
        "            sentence_idx.append(self.word2idx[w])\n",
        "        n = len(sentence_idx)\n",
        "        if  n > self.max_len:\n",
        "            sentence_idx = sentence_idx[:self.max_len] \n",
        "        elif n < self.max_len:\n",
        "            sentence_idx = sentence_idx + [self.eos_idx] + [0]*(self.max_len-n-1)  \n",
        "        return sentence_idx\n",
        "    \n",
        "    def encode(self,sentences): #sentences: 2d_matrix   \n",
        "        sentence_idxs = []\n",
        "        for s in sentences: \n",
        "            sentence_idxs.append(self.encode_line(s))\n",
        "        return np.array(sentence_idxs)\n",
        "    \n",
        "    def print_QA(self, ques , pred_ans, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[2])\n",
        "            print('pred A :'+sents[1]) \n",
        "            \n",
        "    def print_QA_1(self, ques , pred_ans_train, pred_ans_test, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans_train[i], pred_ans_test[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[3])\n",
        "            print('train A:'+sents[1])    \n",
        "            print('test A :'+sents[2]) \n",
        "            \n",
        "    def print_QA_2(self, ques , ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i], ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh1jPDCeixE9",
        "colab_type": "code",
        "outputId": "7550973c-f68f-4c2a-af70-2fd809872c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([525])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfw-the044Ls",
        "colab_type": "code",
        "outputId": "fccf0f88-53a9-43a2-caea-2f0a5bfd44da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict.unsqueeze(1).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([525, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxgxpfjIXmGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict.unsqueeze(1).cpu().view(-1,21)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkIJpu_jWvBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict.cpu().view(-1,n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BesQbFtvSzkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict.squeeze(1).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJTx7aQN453_",
        "colab_type": "code",
        "outputId": "226e9fff-52e7-4e42-ae5f-736e7ebc4f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n = 525/25\n",
        "int(n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VcDxJ_X47mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab(metadata['idx2w'] , metadata['w2idx'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0FPxBw_4_rO",
        "colab_type": "text"
      },
      "source": [
        "### Try train corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlkUrbTD49ut",
        "colab_type": "code",
        "outputId": "d66d6c09-631b-4395-8440-6ecb53c14910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pred_ans = predict.cpu().view(-1,int(n)).data.numpy().T #predicted answer in train phase\n",
        "strd_ans = a.cpu().view(-1,int(n)).data.numpy().T #standard answer\n",
        "ques     = q.cpu().view(-1,int(n)).data.numpy().T #quenstions\n",
        "vocab.print_QA(ques , pred_ans, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :smoke what on of well to about dad my\n",
            "A      :for might youre dont <EOS> it mark\n",
            "pred A :i know <EOS> dont you <EOS> unk\n",
            "\n",
            "Q      :screen we unk this not talk this gave i brother\n",
            "A      :your think talking watch <EOS> of you\n",
            "pred A :unk know unk know <EOS> <EOS> i\n",
            "\n",
            "Q      :for become its coffin like to tell is me cant will\n",
            "A      :own you about the the quality want\n",
            "pred A :unk to <EOS> <EOS> i <EOS> dont\n",
            "\n",
            "Q      :what cowards like we barbie you you them cuff fifty tell feel\n",
            "A      :fear were <EOS> new whos gear <EOS> me\n",
            "pred A :<EOS> <EOS> <EOS> unk i unk <EOS> <EOS>\n",
            "\n",
            "Q      :<EOS> whats included that have <EOS> at helena the an grand you great\n",
            "A      :<EOS> plan more ones sophie is to\n",
            "pred A :<EOS> i <EOS> unk unk <EOS> <EOS>\n",
            "\n",
            "Q      :plan we movie to all <EOS> gear link <EOS> except i now\n",
            "A      :c into <EOS> <EOS> down tell tell\n",
            "pred A :you <EOS> <EOS> <EOS> <EOS> know i\n",
            "\n",
            "Q      :c unk harold prepare actually is i <EOS> only just hes\n",
            "A      :is eleanor <EOS> i you me\n",
            "pred A :<EOS> <EOS> <EOS> i <EOS> i\n",
            "\n",
            "Q      :<EOS> this and you yeah but down meant i had not\n",
            "A      :me than im think its is\n",
            "pred A :<EOS> <EOS> i dont <EOS> <EOS>\n",
            "\n",
            "Q      :how by maude for unk <EOS> and about can a the\n",
            "A      :<EOS> the you afraid its okay it\n",
            "pred A :<EOS> i <EOS> unk you <EOS> <EOS>\n",
            "\n",
            "Q      :high unk <EOS> an i were my do phone life only\n",
            "A      :numbers are i time to bad what\n",
            "pred A :unk <EOS> <EOS> know <EOS> <EOS> i\n",
            "\n",
            "Q      :is things audience saw ready idea a it call is unk\n",
            "A      :stop me cannot we yeah leave news kinda\n",
            "pred A :<EOS> <EOS> know <EOS> i unk <EOS> you\n",
            "\n",
            "Q      :that a with star my to <EOS> paper <EOS> <EOS> a <EOS>\n",
            "A      :at <EOS> unk who got im okay <EOS> trash\n",
            "pred A :<EOS> <EOS> <EOS> i know you <EOS> <EOS> unk\n",
            "\n",
            "Q      :<EOS> what little sophie wars lady land <EOS> bitch\n",
            "A      :unk nothing <EOS> is some working go talk\n",
            "pred A :<EOS> i <EOS> dont <EOS> unk <EOS> <EOS>\n",
            "\n",
            "Q      :<EOS> but <EOS> now <EOS> <EOS> and nah\n",
            "A      :<EOS> just there professional on on is whats\n",
            "pred A :<EOS> i <EOS> unk <EOS> <EOS> you i\n",
            "\n",
            "Q      :its how heres then unk\n",
            "A      :that <EOS> help a i go that that\n",
            "pred A :<EOS> <EOS> <EOS> <EOS> i <EOS> <EOS> not\n",
            "\n",
            "Q      :never about originally the you address\n",
            "A      :if can <EOS> masters sold <EOS> <EOS> like\n",
            "pred A :<EOS> i <EOS> unk dont <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :we that you just guys marry <EOS>\n",
            "A      :i you you in them a\n",
            "pred A :you i you <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :all complicated help another i one they\n",
            "A      :was seem show unk to po its\n",
            "pred A :know dont know <EOS> <EOS> unk i\n",
            "\n",
            "Q      :have really me species was none <EOS> unk\n",
            "A      :less to me unk you wood box started\n",
            "pred A :<EOS> <EOS> <EOS> <EOS> <EOS> i <EOS> unk\n",
            "\n",
            "Q      :to <EOS> get like i tellin but me\n",
            "A      :secure know <EOS> i away <EOS> unk <EOS> <EOS>\n",
            "pred A :<EOS> unk <EOS> i <EOS> <EOS> i <EOS> <EOS>\n",
            "\n",
            "Q      :like come out yourselves have ya his well\n",
            "A      :i what i know at the\n",
            "pred A :<EOS> <EOS> i dont <EOS> <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkHruBBOSVm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pred_ans = predict.unsqueeze(1).cpu().view(-1,int(n)).data.numpy().T #predicted answer in train phase\n",
        "# strd_ans = a.cpu().unsqueeze(1).view(-1,int(n)).data.numpy().T #standard answer\n",
        "# ques     = q.cpu().unsqueeze(1).view(-1,int(n)).data.numpy().T #quenstions\n",
        "# vocab.print_QA(ques , pred_ans, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psTz87qs5GoN",
        "colab_type": "text"
      },
      "source": [
        "### Try test corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eVpQBNNm-SH",
        "colab_type": "code",
        "outputId": "7228885c-4285-4cfb-ac32-c958550741b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()\n",
        "o = model(q,a,feed_previous=True) #logits\n",
        "_,predict_test = o.max(1)\n",
        "#vocab.decode(predict_test.cpu().view(-1,10).data.numpy().T,remove_eos=False,remove_pad=False)\n",
        "pred_ans_test = predict_test.cpu().view(-1,int(n)).data.numpy().T #predicted answer in test phase\n",
        "vocab.print_QA_1(ques , pred_ans, pred_ans_test, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :smoke what on of well to about dad my\n",
            "A      :for might youre dont <EOS> it mark\n",
            "train A:i know <EOS> dont you <EOS> unk\n",
            "test A :i unk unk <EOS> <EOS>\n",
            "\n",
            "Q      :screen we unk this not talk this gave i brother\n",
            "A      :your think talking watch <EOS> of you\n",
            "train A:unk know unk know <EOS> <EOS> i\n",
            "test A :i you <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :for become its coffin like to tell is me cant will\n",
            "A      :own you about the the quality want\n",
            "train A:unk to <EOS> <EOS> i <EOS> dont\n",
            "test A :i unk <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :what cowards like we barbie you you them cuff fifty tell feel\n",
            "A      :fear were <EOS> new whos gear <EOS> me\n",
            "train A:<EOS> <EOS> <EOS> unk i unk <EOS> <EOS>\n",
            "test A :i i <EOS> <EOS> you know <EOS> <EOS> <EOS> know know <EOS> know <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :<EOS> whats included that have <EOS> at helena the an grand you great\n",
            "A      :<EOS> plan more ones sophie is to\n",
            "train A:<EOS> i <EOS> unk unk <EOS> <EOS>\n",
            "test A :i dont <EOS> know <EOS> <EOS> unk <EOS> <EOS>\n",
            "\n",
            "Q      :plan we movie to all <EOS> gear link <EOS> except i now\n",
            "A      :c into <EOS> <EOS> down tell tell\n",
            "train A:you <EOS> <EOS> <EOS> <EOS> know i\n",
            "test A :i dont know <EOS> unk unk <EOS>\n",
            "\n",
            "Q      :c unk harold prepare actually is i <EOS> only just hes\n",
            "A      :is eleanor <EOS> i you me\n",
            "train A:<EOS> <EOS> <EOS> i <EOS> i\n",
            "test A :i unk <EOS>\n",
            "\n",
            "Q      :<EOS> this and you yeah but down meant i had not\n",
            "A      :me than im think its is\n",
            "train A:<EOS> <EOS> i dont <EOS> <EOS>\n",
            "test A :i dont <EOS> know <EOS> <EOS>\n",
            "\n",
            "Q      :how by maude for unk <EOS> and about can a the\n",
            "A      :<EOS> the you afraid its okay it\n",
            "train A:<EOS> i <EOS> unk you <EOS> <EOS>\n",
            "test A :i the <EOS> <EOS>\n",
            "\n",
            "Q      :high unk <EOS> an i were my do phone life only\n",
            "A      :numbers are i time to bad what\n",
            "train A:unk <EOS> <EOS> know <EOS> <EOS> i\n",
            "test A :i you know <EOS> <EOS>\n",
            "\n",
            "Q      :is things audience saw ready idea a it call is unk\n",
            "A      :stop me cannot we yeah leave news kinda\n",
            "train A:<EOS> <EOS> know <EOS> i unk <EOS> you\n",
            "test A :i dont you\n",
            "\n",
            "Q      :that a with star my to <EOS> paper <EOS> <EOS> a <EOS>\n",
            "A      :at <EOS> unk who got im okay <EOS> trash\n",
            "train A:<EOS> <EOS> <EOS> i know you <EOS> <EOS> unk\n",
            "test A :i unk <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :<EOS> what little sophie wars lady land <EOS> bitch\n",
            "A      :unk nothing <EOS> is some working go talk\n",
            "train A:<EOS> i <EOS> dont <EOS> unk <EOS> <EOS>\n",
            "test A :i dont you unk <EOS> know <EOS> unk <EOS> <EOS>\n",
            "\n",
            "Q      :<EOS> but <EOS> now <EOS> <EOS> and nah\n",
            "A      :<EOS> just there professional on on is whats\n",
            "train A:<EOS> i <EOS> unk <EOS> <EOS> you i\n",
            "test A :i i unk <EOS> <EOS> unk <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :its how heres then unk\n",
            "A      :that <EOS> help a i go that that\n",
            "train A:<EOS> <EOS> <EOS> <EOS> i <EOS> <EOS> not\n",
            "test A :i dont <EOS> <EOS> unk <EOS>\n",
            "\n",
            "Q      :never about originally the you address\n",
            "A      :if can <EOS> masters sold <EOS> <EOS> like\n",
            "train A:<EOS> i <EOS> unk dont <EOS> <EOS> <EOS>\n",
            "test A :i i <EOS> unk <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :we that you just guys marry <EOS>\n",
            "A      :i you you in them a\n",
            "train A:you i you <EOS> <EOS> <EOS>\n",
            "test A :i dont you <EOS> know <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :all complicated help another i one they\n",
            "A      :was seem show unk to po its\n",
            "train A:know dont know <EOS> <EOS> unk i\n",
            "test A :i i <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :have really me species was none <EOS> unk\n",
            "A      :less to me unk you wood box started\n",
            "train A:<EOS> <EOS> <EOS> <EOS> <EOS> i <EOS> unk\n",
            "test A :i i unk <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :to <EOS> get like i tellin but me\n",
            "A      :secure know <EOS> i away <EOS> unk <EOS> <EOS>\n",
            "train A:<EOS> unk <EOS> i <EOS> <EOS> i <EOS> <EOS>\n",
            "test A :i i <EOS> <EOS> unk <EOS> <EOS>\n",
            "\n",
            "Q      :like come out yourselves have ya his well\n",
            "A      :i what i know at the\n",
            "train A:<EOS> <EOS> i dont <EOS> <EOS>\n",
            "test A :i unk <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46IB8CXaqvqt",
        "colab_type": "code",
        "outputId": "88f5d793-b4ff-4d98-c1d7-605ff519df7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "q.dim()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubtmCj9Nqx1Z",
        "colab_type": "code",
        "outputId": "cefa32fa-fcf3-4bae-c2db-50ee217f8eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a.dim()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQuVsuixq2QM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a.unsqueeze(-3).dim()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfo4x2DIrPbr",
        "colab_type": "code",
        "outputId": "508702ba-1ada-4369-e8cd-92a70c78a55f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "q.unsqueeze(-3).dim()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_KCHxl-iBt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "o = model(q,a,feed_previous=True) #logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLpw98drNe2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.eval()\n",
        "# o = model(q,a,feed_previous=True) #logits\n",
        "# _,predict_test = o.max(1)\n",
        "# #vocab.decode(predict_test.cpu().view(-1,10).data.numpy().T,remove_eos=False,remove_pad=False)\n",
        "# pred_ans_test = predict_test.cpu().view(-1,int(n)).data.numpy().T #predicted answer in test phase\n",
        "# vocab.print_QA_1(ques , pred_ans, pred_ans_test, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJQw23Tg5LPz",
        "colab_type": "code",
        "outputId": "a285bad4-639f-47cf-9af3-7e0f501e7fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()\n",
        "o = model(q,a,feed_previous=True) #logits\n",
        "_,predict_test = o.max(1)\n",
        "#vocab.decode(predict_test.cpu().view(-1,10).data.numpy().T,remove_eos=False,remove_pad=False)\n",
        "pred_ans_test = predict_test.cpu().view(-1,int(n)).data.numpy().T #predicted answer in test phase\n",
        "vocab.print_QA_1(ques , pred_ans, pred_ans_test, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :smoke what on of well to about dad my\n",
            "A      :for might youre dont <EOS> it mark\n",
            "train A:i know <EOS> dont you <EOS> unk\n",
            "test A :i unk unk <EOS> <EOS>\n",
            "\n",
            "Q      :screen we unk this not talk this gave i brother\n",
            "A      :your think talking watch <EOS> of you\n",
            "train A:unk know unk know <EOS> <EOS> i\n",
            "test A :i you <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :for become its coffin like to tell is me cant will\n",
            "A      :own you about the the quality want\n",
            "train A:unk to <EOS> <EOS> i <EOS> dont\n",
            "test A :i unk <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :what cowards like we barbie you you them cuff fifty tell feel\n",
            "A      :fear were <EOS> new whos gear <EOS> me\n",
            "train A:<EOS> <EOS> <EOS> unk i unk <EOS> <EOS>\n",
            "test A :i i <EOS> <EOS> you know <EOS> <EOS> <EOS> know know <EOS> know <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :<EOS> whats included that have <EOS> at helena the an grand you great\n",
            "A      :<EOS> plan more ones sophie is to\n",
            "train A:<EOS> i <EOS> unk unk <EOS> <EOS>\n",
            "test A :i dont <EOS> know <EOS> <EOS> unk <EOS> <EOS>\n",
            "\n",
            "Q      :plan we movie to all <EOS> gear link <EOS> except i now\n",
            "A      :c into <EOS> <EOS> down tell tell\n",
            "train A:you <EOS> <EOS> <EOS> <EOS> know i\n",
            "test A :i dont know <EOS> unk unk <EOS>\n",
            "\n",
            "Q      :c unk harold prepare actually is i <EOS> only just hes\n",
            "A      :is eleanor <EOS> i you me\n",
            "train A:<EOS> <EOS> <EOS> i <EOS> i\n",
            "test A :i unk <EOS>\n",
            "\n",
            "Q      :<EOS> this and you yeah but down meant i had not\n",
            "A      :me than im think its is\n",
            "train A:<EOS> <EOS> i dont <EOS> <EOS>\n",
            "test A :i dont <EOS> know <EOS> <EOS>\n",
            "\n",
            "Q      :how by maude for unk <EOS> and about can a the\n",
            "A      :<EOS> the you afraid its okay it\n",
            "train A:<EOS> i <EOS> unk you <EOS> <EOS>\n",
            "test A :i the <EOS> <EOS>\n",
            "\n",
            "Q      :high unk <EOS> an i were my do phone life only\n",
            "A      :numbers are i time to bad what\n",
            "train A:unk <EOS> <EOS> know <EOS> <EOS> i\n",
            "test A :i you know <EOS> <EOS>\n",
            "\n",
            "Q      :is things audience saw ready idea a it call is unk\n",
            "A      :stop me cannot we yeah leave news kinda\n",
            "train A:<EOS> <EOS> know <EOS> i unk <EOS> you\n",
            "test A :i dont you\n",
            "\n",
            "Q      :that a with star my to <EOS> paper <EOS> <EOS> a <EOS>\n",
            "A      :at <EOS> unk who got im okay <EOS> trash\n",
            "train A:<EOS> <EOS> <EOS> i know you <EOS> <EOS> unk\n",
            "test A :i unk <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :<EOS> what little sophie wars lady land <EOS> bitch\n",
            "A      :unk nothing <EOS> is some working go talk\n",
            "train A:<EOS> i <EOS> dont <EOS> unk <EOS> <EOS>\n",
            "test A :i dont you unk <EOS> know <EOS> unk <EOS> <EOS>\n",
            "\n",
            "Q      :<EOS> but <EOS> now <EOS> <EOS> and nah\n",
            "A      :<EOS> just there professional on on is whats\n",
            "train A:<EOS> i <EOS> unk <EOS> <EOS> you i\n",
            "test A :i i unk <EOS> <EOS> unk <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :its how heres then unk\n",
            "A      :that <EOS> help a i go that that\n",
            "train A:<EOS> <EOS> <EOS> <EOS> i <EOS> <EOS> not\n",
            "test A :i dont <EOS> <EOS> unk <EOS>\n",
            "\n",
            "Q      :never about originally the you address\n",
            "A      :if can <EOS> masters sold <EOS> <EOS> like\n",
            "train A:<EOS> i <EOS> unk dont <EOS> <EOS> <EOS>\n",
            "test A :i i <EOS> unk <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :we that you just guys marry <EOS>\n",
            "A      :i you you in them a\n",
            "train A:you i you <EOS> <EOS> <EOS>\n",
            "test A :i dont you <EOS> know <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :all complicated help another i one they\n",
            "A      :was seem show unk to po its\n",
            "train A:know dont know <EOS> <EOS> unk i\n",
            "test A :i i <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :have really me species was none <EOS> unk\n",
            "A      :less to me unk you wood box started\n",
            "train A:<EOS> <EOS> <EOS> <EOS> <EOS> i <EOS> unk\n",
            "test A :i i unk <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :to <EOS> get like i tellin but me\n",
            "A      :secure know <EOS> i away <EOS> unk <EOS> <EOS>\n",
            "train A:<EOS> unk <EOS> i <EOS> <EOS> i <EOS> <EOS>\n",
            "test A :i i <EOS> <EOS> unk <EOS> <EOS>\n",
            "\n",
            "Q      :like come out yourselves have ya his well\n",
            "A      :i what i know at the\n",
            "train A:<EOS> <EOS> i dont <EOS> <EOS>\n",
            "test A :i unk <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZK_ZYC55NRd",
        "colab_type": "text"
      },
      "source": [
        "### Try Chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxzM3DlE5EhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = []\n",
        "lines.append( 'you can do it'  )\n",
        "lines.append( 'how are you'    )\n",
        "lines.append( 'fuck you'  )\n",
        "lines.append( 'jesus christ you scared the shit out of me'  )\n",
        "lines.append( 'youre terrible'  )\n",
        "lines.append( 'is something wrong' )\n",
        "lines.append( 'nobodys gonna get inside' )\n",
        "lines.append( 'im sorry'  )\n",
        "lines.append( 'shut up'  )\n",
        "N = len(lines)\n",
        "lines = vocab.encode(lines)\n",
        "q_o = Variable(torch.from_numpy(lines).long()).cuda()\n",
        "#vocab.decode(vocab.encode(lines))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBazLuWI5QHW",
        "colab_type": "code",
        "outputId": "0841298e-d2bb-46cd-a4bc-4a1799a86d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "model.eval()\n",
        "o = model(q_o,a[:N],feed_previous=True)\n",
        "_,predict_o = o.max(1)\n",
        "#vocab.decode(predict_o.cpu().view(-1,3).data.numpy().T)\n",
        "pred_ans_o = predict_o.cpu().view(-1,N).data.numpy().T #predicted answer \n",
        "vocab.print_QA_2(lines, pred_ans_o)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :you can do it\n",
            "A      :i unk unk <EOS> <EOS>\n",
            "\n",
            "Q      :how are you\n",
            "A      :i you <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :fuck you\n",
            "A      :i unk <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :jesus christ you scared the shit out of me\n",
            "A      :i i <EOS> <EOS> you know <EOS> <EOS> <EOS> know know <EOS> know <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :youre terrible\n",
            "A      :i dont <EOS> know <EOS> <EOS> unk <EOS> <EOS>\n",
            "\n",
            "Q      :is something wrong\n",
            "A      :i dont know <EOS> unk unk <EOS>\n",
            "\n",
            "Q      :nobodys gonna get inside\n",
            "A      :i unk <EOS>\n",
            "\n",
            "Q      :im sorry\n",
            "A      :i dont <EOS> know <EOS> <EOS>\n",
            "\n",
            "Q      :shut up\n",
            "A      :i the <EOS> <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDVOetNF5Wfr",
        "colab_type": "text"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBushY7V5SQx",
        "colab_type": "code",
        "outputId": "8244f76f-bf78-4156-8be8-495925069810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "epoch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dRJY-kH5ZL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#np.save('loss_female_prob_feed_epo141.npy',epoch_mean_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU5J7MaF5bEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict() , 'model_female_prob_feed_epo141.pth') #save model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xAYWZ7xSUyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict() , 'model_female_prob_feed_epo199.pth') #save model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSvqzYpD5etY",
        "colab_type": "text"
      },
      "source": [
        "### Back up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ8HttgO5efj",
        "colab_type": "code",
        "outputId": "fd0103ca-c45c-4abf-ceb8-d8edbdc5d48d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  59,  164, 8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   8,   16,    2,   68, 8999,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   2,   74, 8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  72,    3,   16, 8999,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  17,  395,   36,   17,   21,    9,  395,   68,    2,  427, 2235, 8999,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  30,    2, 2837, 8999,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  72,   41,    1, 8999,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  77,   65,    5,   29,    6, 5331, 8999,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   3, 5903,   50,    5,  131,   10,  193,    9,   31, 8999,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [ 376, 8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  37,   65,    5,  157,  239,   31,    4,  944, 8999,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  36, 8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  44,   58, 8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   3,   90,   28,   61,  248,    3,  133,    3,  258,    6,  905, 8999,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  16,    2,  211, 1241,    1,    4, 3913, 1424,   80, 1004,    4,    1,\n",
              "         6148,   94,  271,  198,   11,  292,   98,  466, 8999,    0,    0,    0,\n",
              "            0],\n",
              "        [   8,   16,    2,   90, 8999,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   3,   74, 8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  17,  107,    3,   14,   18, 8999,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  26, 1453,    1,    9, 1069,  124,  981,    6, 1112, 3291, 8999,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   7,  106,    6, 1371,   13,    4,  944,   82,   30,   27,   65, 8999,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  11,    4, 6516, 6516,   17,   70,   60,  104,  199,    9,   29,  542,\n",
              "         8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiU501ST5h-9",
        "colab_type": "code",
        "outputId": "908016df-76ad-4fbe-c07a-4476114b72dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "x = Variable(torch.rand(3,25)*200).long()\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[121,  54, 142, 130, 184, 196,  78,  91,  73, 181, 134, 117, 145,  88,\n",
              "          69, 116, 148, 111,  19,  33, 140, 163, 190, 168,  19],\n",
              "        [139, 177,  65, 151,  24, 163, 142, 139, 111, 100,  69, 145,  26,  45,\n",
              "          57,  85,  50, 139,  86, 146, 159, 188,  17,  78,  73],\n",
              "        [101,  29, 197,  51,  25, 158,  13,  55,  35, 105, 133, 189,  34,  75,\n",
              "          78,  53,  98, 153,  47, 118, 163,  69, 130, 108,   5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glS4EvYE5jzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,p = model(x,q,feed_previous=True).max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUcolkLl5lOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.decode(p.view(3,25).data.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEpimz1ZdUbk",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq-male"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8tmmD-i5p_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaKI0pcu5sEp",
        "colab_type": "text"
      },
      "source": [
        "### Data Handling\n",
        "load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7tDvLai5uLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = np.load(\"./datasets/m_idx_a_1.npy\")\n",
        "ques = np.load(\"./datasets/m_idx_q_1.npy\")\n",
        "\n",
        "with open('./datasets/metadata_1.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDAALWWq5xhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaleDataset(data.Dataset): \n",
        "    def __init__(self,ques,ans):\n",
        "        self.ques = ques\n",
        "        self.ans = ans\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ques_tensor = torch.from_numpy(self.ques[index]).long()\n",
        "        ans_tensor = torch.from_numpy(self.ans[index]).long()\n",
        "        \n",
        "        return ques_tensor , ans_tensor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return 78119"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owpKeooS5ze7",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFZAXFti5yNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "male_dataset = MaleDataset(ques,ans)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=male_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr40H1Rf52cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_voc_size=9000,\n",
        "                 trg_voc_size=9000,\n",
        "                 src_embedding_size=256,\n",
        "                 trg_embedding_size=256,\n",
        "                 enc_hidden_size=200,\n",
        "                 dec_hidden_size=200):\n",
        "        \n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.trg_embedding_size = trg_embedding_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "        \n",
        "        self.src_embedder = nn.Embedding(src_voc_size , src_embedding_size)\n",
        "        self.encoder = nn.LSTM(src_embedding_size ,enc_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        \n",
        "        self.trg_embedder = nn.Embedding(trg_voc_size , trg_embedding_size)\n",
        "        self.decoder = nn.LSTM(trg_embedding_size ,dec_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        self.cls = nn.Linear(dec_hidden_size , trg_voc_size)\n",
        "\n",
        "    # def forward(self,source,target,feed_previous=False):\n",
        "    #     batch_size = source.size()[0]\n",
        "    #     src_em = self.src_embedder(source)\n",
        "    #     trg_em = self.trg_embedder(target)\n",
        "        \n",
        "    #     _ , enc_state = self.encoder(src_em)\n",
        "\n",
        "    #     GO = Variable(torch.zeros(batch_size,1,self.trg_embedding_size)).cuda()\n",
        "    #     dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "    #     if feed_previous: #test phase\n",
        "    #         logits_ = []        \n",
        "    #         h = enc_state\n",
        "    #         for i in range(25):\n",
        "    #             inputs = torch.unsqueeze(dec_in[:,i,:],1)\n",
        "    #             output , h = self.decoder(inputs,h)\n",
        "    #             logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "    #             logits_.append(logits)\n",
        "                \n",
        "    #             predicted = logits.max(1)[1]\n",
        "    #             inputs = self.trg_embedder(predicted)\n",
        "                    \n",
        "    #         return torch.cat(logits_,0)\n",
        "\n",
        "    #     else: #train phase\n",
        "    #         dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "    #         outputs , _ = self.decoder(dec_in,enc_state)\n",
        "    #         outputs = outputs.contiguous().view(-1,self.dec_hidden_size)\n",
        "    #         logits = self.cls(outputs)\n",
        "        \n",
        "    #         return logits\n",
        "\n",
        "    def forward(self,source,target,feed_previous=False):\n",
        "        batch_size = source.size()[0]\n",
        "        src_em = self.src_embedder(source)\n",
        "        trg_em = self.trg_embedder(target)\n",
        "        \n",
        "        _ , enc_state = self.encoder(src_em)\n",
        "        \n",
        "        GO = Variable(torch.zeros(batch_size,1,self.trg_embedding_size)).cuda()\n",
        "        \n",
        "        if feed_previous: #test phase\n",
        "            logits_ = []\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "            h = enc_state\n",
        "            for i in range(25):\n",
        "                inputs = torch.unsqueeze(dec_in[:,i,:],1)    \n",
        "                output , h = self.decoder(inputs,h)\n",
        "                logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "                logits_.append(logits)\n",
        "                \n",
        "                predicted = logits.max(1)[1]\n",
        "                inputs = self.trg_embedder(predicted)\n",
        "                    \n",
        "            return torch.cat(logits_,0)\n",
        "            \n",
        "        else: #train phase\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "            outputs , _ = self.decoder(dec_in,enc_state)\n",
        "            outputs = outputs.contiguous().view(-1,self.dec_hidden_size)\n",
        "            logits = self.cls(outputs)\n",
        "        \n",
        "            return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPPJi5TqO1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxgSZcB9O5TN",
        "colab_type": "code",
        "outputId": "b0413bdf-148f-4022-e42a-2dbecde90eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (src_embedder): Embedding(9000, 256)\n",
              "  (encoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (trg_embedder): Embedding(9000, 256)\n",
              "  (decoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (cls): Linear(in_features=200, out_features=9000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhfN2mBwQMec",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvsdjak3PRjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_op = optim.Adam(model.parameters() ,lr=3e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx5c2kz4QPI4",
        "colab_type": "code",
        "outputId": "7ea319e2-7269-42a6-ca67-dd4823e67234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "epochs = 200\n",
        "loss_hist = []\n",
        "loss_ = 3\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_mean_loss = []\n",
        "\n",
        "    for i , (q,a) in enumerate(train_loader):\n",
        "        q = Variable(q).cuda()\n",
        "        a = Variable(a).cuda()\n",
        "   \n",
        "        logits = model(q,a,feed_previous=False)\n",
        "        _,predict = logits.max(1)\n",
        "        \n",
        "        loss = F.cross_entropy(logits ,a.view(-1))\n",
        "        train_op.zero_grad()\n",
        "        loss.backward()\n",
        "        train_op.step()\n",
        "        \n",
        "        epoch_mean_loss.append(loss.data.item())\n",
        "    \n",
        "    loss_ = np.mean(epoch_mean_loss)\n",
        "    loss_hist.append(loss_)\n",
        "    if epoch % 10 == 0  or epoch == epochs-1:\n",
        "        print(\"epoch:%s , loss:%s\" % (epoch , loss_ ))\n",
        "    if epoch % 50 == 0 or epoch == epochs-1:\n",
        "        torch.save(model.state_dict() , 'pth/model_male_epo%s.pth'%epoch) #save model\n",
        "        \n",
        "np.save('loss_male_epo%s.npy'%epochs,loss_hist)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 , loss:2.18168046252831\n",
            "epoch:10 , loss:1.665916821099421\n",
            "epoch:20 , loss:1.5558263949445776\n",
            "epoch:30 , loss:1.4736007693163398\n",
            "epoch:40 , loss:1.405958204680442\n",
            "epoch:50 , loss:1.3493248187402151\n",
            "epoch:60 , loss:1.3015521544349866\n",
            "epoch:70 , loss:1.2607862579588223\n",
            "epoch:80 , loss:1.2260799525362072\n",
            "epoch:90 , loss:1.1940073153457127\n",
            "epoch:100 , loss:1.1658223389749347\n",
            "epoch:110 , loss:1.1413029303421844\n",
            "epoch:120 , loss:1.117913889894712\n",
            "epoch:130 , loss:1.0969615390464773\n",
            "epoch:140 , loss:1.0786263346672058\n",
            "epoch:150 , loss:1.0606620327602343\n",
            "epoch:160 , loss:1.0445559724347695\n",
            "epoch:170 , loss:1.0299259537707384\n",
            "epoch:180 , loss:1.0166153528473594\n",
            "epoch:190 , loss:1.0028257887691479\n",
            "epoch:199 , loss:0.9916714292437595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jir6ULXWQWrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,predict = logits.max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diAyTKIpQrTh",
        "colab_type": "text"
      },
      "source": [
        "### Print Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhO1QLxnQper",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self,idx2word,word2idx):\n",
        "        self.idx2word = idx2word\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = 25\n",
        "        self.eos_idx = 8002\n",
        "        self.EN_WHITELIST  = '0123456789abcdefghijklmnopqrstuvwxyz '             \n",
        "            \n",
        "    '''\n",
        "    idx -> word with EOS\n",
        "    '''        \n",
        "    def decode_line(self,sentence_idx,remove_pad=True,remove_eos=True):  #sentence_idx: 1d_matrix     \n",
        "        sentence = []\n",
        "        for w in sentence_idx:\n",
        "            if remove_eos and w==self.eos_idx:\n",
        "                continue\n",
        "            if remove_pad and w==0 : \n",
        "                continue\n",
        "            sentence.append(self.idx2word[w])\n",
        "            #if w==self.eos_idx:\n",
        "            #    break\n",
        "        sentence = ' '.join(sentence)\n",
        "        return sentence\n",
        "    \n",
        "    def decode(self,sentence_idxs,remove_pad=True,remove_eos=True): #sentence_idxs: 2d_matrix \n",
        "        sentences = []\n",
        "        for s in sentence_idxs: \n",
        "            sentences.append(self.decode_line(s,\n",
        "                                              remove_pad=remove_pad,\n",
        "                                              remove_eos=remove_eos))\n",
        "        return sentences\n",
        "            \n",
        "    '''\n",
        "    word -> idx with EOS\n",
        "    '''\n",
        "    def encode_line(self,sentence):  #sentence: 1d_matrix\n",
        "        sentence = sentence.lower()\n",
        "        s_list = ''.join([ ch for ch in sentence if ch in self.EN_WHITELIST ]).split()\n",
        "        sentence_idx = []\n",
        "        for w in s_list:\n",
        "            sentence_idx.append(self.word2idx[w])\n",
        "        n = len(sentence_idx)\n",
        "        if  n > self.max_len:\n",
        "            sentence_idx = sentence_idx[:self.max_len] \n",
        "        elif n < self.max_len:\n",
        "            sentence_idx = sentence_idx + [self.eos_idx] + [0]*(self.max_len-n-1)  \n",
        "        return sentence_idx\n",
        "    \n",
        "    def encode(self,sentences): #sentences: 2d_matrix   \n",
        "        sentence_idxs = []\n",
        "        for s in sentences: \n",
        "            sentence_idxs.append(self.encode_line(s))\n",
        "        return np.array(sentence_idxs)\n",
        "    \n",
        "    def print_QA(self, ques , pred_ans, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[2])\n",
        "            print('pred A :'+sents[1]) \n",
        "            \n",
        "    def print_QA_1(self, ques , pred_ans_train, pred_ans_test, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans_train[i], pred_ans_test[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[3])\n",
        "            print('train A:'+sents[1])    \n",
        "            print('test A :'+sents[2]) \n",
        "            \n",
        "    def print_QA_2(self, ques , ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i], ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmkfh_1aQt6y",
        "colab_type": "code",
        "outputId": "cc15bbaf-5320-45d6-d2db-a0a5530c800f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict.unsqueeze(1).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([175, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V17ZAyNDQvWp",
        "colab_type": "code",
        "outputId": "fd44e5be-40db-4775-9fb7-a594ee647704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n = 175/25\n",
        "int(n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGCixp6zQxsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab(metadata['idx2w'] , metadata['w2idx'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "livc4AL_QzPA",
        "colab_type": "text"
      },
      "source": [
        "### Try train corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IflRyfITQ5Ap",
        "colab_type": "code",
        "outputId": "215f62a9-3b45-4448-8187-dd515a77cf07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "pred_ans = predict.cpu().view(-1,int(n)).data.numpy().T #predicted answer in train phase\n",
        "strd_ans = a.cpu().view(-1,int(n)).data.numpy().T #standard answer\n",
        "ques     = q.cpu().view(-1,int(n)).data.numpy().T #quenstions\n",
        "vocab.print_QA(ques , pred_ans, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :what unk system here unk dont ill was\n",
            "A      :just id to shipping get find bought to for take no square on\n",
            "pred A :its i <EOS> <EOS> know go get <EOS> <EOS> be <EOS> unk in\n",
            "\n",
            "Q      :were she and not <EOS> ever worry take a\n",
            "A      :the love doesnt get lanes picked out <EOS> get it care way and the\n",
            "pred A :a like its read lanes it the <EOS> get you it way the the\n",
            "\n",
            "Q      :the developed unk a mine come the care strong\n",
            "A      :obvious to need back stick up how we downstairs <EOS> of i we way\n",
            "pred A :obvious to mind me unk up where well us <EOS> of <EOS> i unk\n",
            "\n",
            "Q      :others the the unk too here unk of still unk\n",
            "A      :stuff say to up out right much can then it bought open he in\n",
            "pred A :<EOS> know a in the <EOS> much need <EOS> it just finish hes in\n",
            "\n",
            "Q      :<EOS> it precogs unk <EOS> <EOS> shut buy it and he\n",
            "A      :<EOS> speaking hello be to a <EOS> time use we not it tonight hit <EOS>\n",
            "pred A :<EOS> speaking hello be the a <EOS> we get we <EOS> my on got now\n",
            "\n",
            "Q      :was designed <EOS> were it it <EOS> all m\n",
            "A      :of <EOS> use the thumb okay we the can this fair <EOS> his\n",
            "pred A :of <EOS> a the thumb were we it can the on <EOS> his\n",
            "\n",
            "Q      :iris the all no down back carl <EOS>\n",
            "A      :unk this unk youll lets just unk run youll time and head\n",
            "pred A :unk this unk <EOS> ill can car get im way unk neck\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwJMC8onRGRZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Try test corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ9J1h3yQ9Nj",
        "colab_type": "code",
        "outputId": "7139d4c7-7db2-4683-c680-18c89acbd207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "model.eval()\n",
        "o = model(q,a,feed_previous=True) #logits\n",
        "_,predict_test = o.max(1)\n",
        "#vocab.decode(predict_test.cpu().view(-1,10).data.numpy().T,remove_eos=False,remove_pad=False)\n",
        "pred_ans_test = predict_test.cpu().view(-1,int(n)).data.numpy().T #predicted answer in test phase\n",
        "vocab.print_QA_1(ques , pred_ans, pred_ans_test, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :what unk system here unk dont ill was\n",
            "A      :just id to shipping get find bought to for take no square on\n",
            "train A:its i <EOS> <EOS> know go get <EOS> <EOS> be <EOS> unk in\n",
            "test A :its a obvious unk <EOS>\n",
            "\n",
            "Q      :were she and not <EOS> ever worry take a\n",
            "A      :the love doesnt get lanes picked out <EOS> get it care way and the\n",
            "train A:a like its read lanes it the <EOS> get you it way the the\n",
            "test A :speaking of wood id like to see hello <EOS>\n",
            "\n",
            "Q      :the developed unk a mine come the care strong\n",
            "A      :obvious to need back stick up how we downstairs <EOS> of i we way\n",
            "train A:obvious to mind me unk up where well us <EOS> of <EOS> i unk\n",
            "test A :its mind it be careful this <EOS> get rid on here the unk shipping lanes fix off the thumb blow be paid up <EOS> <EOS>\n",
            "\n",
            "Q      :others the the unk too here unk of still unk\n",
            "A      :stuff say to up out right much can then it bought open he in\n",
            "train A:<EOS> know a in the <EOS> much need <EOS> it just finish hes in\n",
            "test A :were ill go out where much we we got got <EOS>\n",
            "\n",
            "Q      :<EOS> it precogs unk <EOS> <EOS> shut buy it and he\n",
            "A      :<EOS> speaking hello be to a <EOS> time use we not it tonight hit <EOS>\n",
            "train A:<EOS> speaking hello be the a <EOS> we get we <EOS> my on got now\n",
            "test A :we got get it truck one get outta from we can get and the <EOS>\n",
            "\n",
            "Q      :was designed <EOS> were it it <EOS> all m\n",
            "A      :of <EOS> use the thumb okay we the can this fair <EOS> his\n",
            "train A:of <EOS> a the thumb were we it can the on <EOS> his\n",
            "test A :im be it of it <EOS> the time <EOS> way <EOS> think the on and square <EOS> we get <EOS> <EOS>\n",
            "\n",
            "Q      :iris the all no down back carl <EOS>\n",
            "A      :unk this unk youll lets just unk run youll time and head\n",
            "train A:unk this unk <EOS> ill can car get im way unk neck\n",
            "test A :hes got his neck in the unk in <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvgEIEicRKtn",
        "colab_type": "text"
      },
      "source": [
        "### Try chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vlXBrx3RIPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = []\n",
        "lines.append( 'you can do it'  )\n",
        "lines.append( 'how are you'    )\n",
        "lines.append( 'fuck you'  )\n",
        "lines.append( 'jesus christ you scared the shit out of me'  )\n",
        "lines.append( 'youre terrible'  )\n",
        "lines.append( 'is something wrong' )\n",
        "lines.append( 'nobodys gonna get inside' )\n",
        "#lines.append( 'im sorry'  )\n",
        "#lines.append( 'shut up'  )\n",
        "N = len(lines)\n",
        "lines = vocab.encode(lines)\n",
        "q_o = Variable(torch.from_numpy(lines).long()).cuda()\n",
        "#vocab.decode(vocab.encode(lines))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmiLmtMKazG8",
        "colab_type": "code",
        "outputId": "92d0b893-7f69-4f0c-fdaf-1507c9f663e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.eval()\n",
        "o = model(q_o,a[:N],feed_previous=True)\n",
        "_,predict_o = o.max(1)\n",
        "#vocab.decode(predict_o.cpu().view(-1,3).data.numpy().T)\n",
        "pred_ans_o = predict_o.cpu().view(-1,N).data.numpy().T #predicted answer \n",
        "vocab.print_QA_2(lines, pred_ans_o)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :you can do it\n",
            "A      :youre what cops thing <EOS>\n",
            "\n",
            "Q      :how are you\n",
            "A      :major you you i like to go hello <EOS>\n",
            "\n",
            "Q      :fuck you\n",
            "A      :what argue a be a that <EOS> get a to here the unk shipping lanes get on the thumb <EOS> be paid up <EOS> <EOS>\n",
            "\n",
            "Q      :jesus christ you scared the shit out of me\n",
            "A      :thats i just out <EOS> to i i go got <EOS>\n",
            "\n",
            "Q      :youre terrible\n",
            "A      :i know be it mask unk get the from i can get out a <EOS>\n",
            "\n",
            "Q      :is something wrong\n",
            "A      :we be care of it <EOS> for time <EOS> way <EOS> think it in and square <EOS> we get out <EOS>\n",
            "\n",
            "Q      :nobodys gonna get inside\n",
            "A      :i understands the setup everyday the river to now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZK7FozRQFQ",
        "colab_type": "text"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNT_sm-lROqp",
        "colab_type": "code",
        "outputId": "ed78af7b-04e8-491c-beb0-149e6dfb4961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "epoch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3H_Lw2lRRyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('loss_male_prob_feed_epo200.npy',epoch_mean_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHuRKc-vRUUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict() , 'model_female_prob_feed_epo200.pth') #save model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOH2N5bVRVta",
        "colab_type": "text"
      },
      "source": [
        "### Back up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVm8jt3PRXzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJb7Zr3-RZdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Variable(torch.rand(3,25)*200).long()\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-ALOhNPRbZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,p = model(x,q,feed_previous=True).max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfPhKo2QRdBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.decode(p.view(3,25).data.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nanaP6X_mPsv",
        "colab_type": "text"
      },
      "source": [
        "## Schedule_Sampling_Seq2Seq-female"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8_oLtGPmSt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pickle\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cYBk82WmYRv",
        "colab_type": "text"
      },
      "source": [
        "### Data Handling\n",
        "load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND7zrZ8emU4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = np.load(\"./datasets/f_idx_a_1.npy\")\n",
        "ques = np.load(\"./datasets/f_idx_q_1.npy\")\n",
        "\n",
        "with open('./datasets/metadata_1.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf9gwxPnmkA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FemaleDataset(data.Dataset): \n",
        "    def __init__(self,ques,ans):\n",
        "        self.ques = ques\n",
        "        self.ans = ans\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ques_tensor = torch.from_numpy(self.ques[index]).long()\n",
        "        ans_tensor = torch.from_numpy(self.ans[index]).long()\n",
        "        \n",
        "        return ques_tensor , ans_tensor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return 33589"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrSXso3qmlk7",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mISaIlgFmnNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "female_dataset = FemaleDataset(ques,ans)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=female_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvyayrzVmo4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_voc_size=9000,\n",
        "                 trg_voc_size=9000,\n",
        "                 src_embedding_size=256,\n",
        "                 trg_embedding_size=256,\n",
        "                 enc_hidden_size=200,\n",
        "                 dec_hidden_size=200):\n",
        "        \n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.trg_embedding_size = trg_embedding_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "        \n",
        "        self.src_embedder = nn.Embedding(src_voc_size , src_embedding_size)\n",
        "        self.encoder = nn.LSTM(src_embedding_size ,enc_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        \n",
        "        self.trg_embedder = nn.Embedding(trg_voc_size , trg_embedding_size)\n",
        "        self.decoder = nn.LSTM(trg_embedding_size ,dec_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        self.cls = nn.Linear(dec_hidden_size , trg_voc_size)\n",
        "    \n",
        "    def forward(self,source,target,feed_previous=False):\n",
        "        batch_size = source.size()[0]\n",
        "        src_em = self.src_embedder(source)\n",
        "        trg_em = self.trg_embedder(target)\n",
        "        \n",
        "        _ , enc_state = self.encoder(src_em)\n",
        "        \n",
        "        GO = Variable(torch.zeros(batch_size,1,self.trg_embedding_size)).cuda()\n",
        "        \n",
        "        if feed_previous: #test phase\n",
        "            logits_ = []\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "#            inputs = GO\n",
        "            h = enc_state\n",
        "            for i in range(25):\n",
        "                inputs =  torch.unsqueeze(dec_in[:,i,:],1)\n",
        "                output , h = self.decoder(inputs,h)\n",
        "                logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "                logits_.append(logits)\n",
        "                \n",
        "                predicted = logits.max(1)[1]\n",
        "                inputs = self.trg_embedder(predicted)\n",
        "                    \n",
        "            return torch.cat(logits_,0)\n",
        "            \n",
        "        else: #train phase -- schedule sampling\n",
        "            '''\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)  # trg_em.shape=(batch_size, time step, trg_embedding_size )\n",
        "            outputs , _ = self.decoder(dec_in,enc_state)\n",
        "            outputs = outputs.contiguous().view(-1,self.dec_hidden_size)\n",
        "            logits = self.cls(outputs)        \n",
        "            return logits  \n",
        "            '''\n",
        "            logits_ = []\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "            \n",
        "            h = enc_state\n",
        "            for i in range(25):\n",
        "                inputs = torch.unsqueeze(dec_in[:,i,:],1)\n",
        "                \n",
        "                if i < 5:\n",
        "                    inputs =  torch.unsqueeze(dec_in[:,i,:],1)\n",
        "                else:\n",
        "                    if random.random() < 0.6 : \n",
        "                        inputs =  torch.unsqueeze(dec_in[:,i,:],1)  #usual training policy \n",
        "                    else:\n",
        "                        inputs = self.trg_embedder(predicted.unsqueeze(1))  #schedule sampling\n",
        "                output , h = self.decoder(inputs,h)\n",
        "                logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "                logits_.append(logits)\n",
        "                predicted = logits.max(1)[1]  # predicted.shape=(batch_size, time step=1)\n",
        "                \n",
        "            return torch.cat(logits_,0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbF3yjx7msHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlxl_OGimteN",
        "colab_type": "code",
        "outputId": "6d162311-f8a5-4f3c-af02-9fdabfa231d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (src_embedder): Embedding(9000, 256)\n",
              "  (encoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (trg_embedder): Embedding(9000, 256)\n",
              "  (decoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (cls): Linear(in_features=200, out_features=9000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIQ7se8ymvCt",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjXv49Dqmw1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_op = optim.Adam(model.parameters() ,lr=3e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3QfQRSomyxH",
        "colab_type": "code",
        "outputId": "4c1b2e32-7c18-4aa5-94a2-1b12b8cfc886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "epochs = 200\n",
        "loss_hist = []\n",
        "loss_ = 3\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_mean_loss = []\n",
        "\n",
        "    for i , (q,a) in enumerate(train_loader):\n",
        "        q = Variable(q).cuda()\n",
        "        a = Variable(a).cuda()\n",
        "        logits = model(q,a,feed_previous=False)\n",
        "        _,predict = logits.max(1)\n",
        "        \n",
        "        loss = F.cross_entropy(logits ,torch.transpose(a,0,1).contiguous().view(-1))\n",
        "        train_op.zero_grad()\n",
        "        loss.backward()\n",
        "        train_op.step()\n",
        "        \n",
        "        epoch_mean_loss.append(loss.data.item())\n",
        "    \n",
        "    loss_ = np.mean(epoch_mean_loss)\n",
        "    loss_hist.append(loss_)\n",
        "    if epoch % 10 == 0  or epoch == epochs-1:\n",
        "        print(\"epoch:%s , loss:%s\" % (epoch , loss_ ))\n",
        "    if epoch % 50 == 0 or epoch == epochs-1:\n",
        "        torch.save(model.state_dict() , 'pth/model_female_sche_sampling_epo%s.pth'%epoch) #save model\n",
        "        \n",
        "np.save('loss_female_sche_sampling_epo%s.npy'%epochs,loss_hist)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 , loss:2.427192337853568\n",
            "epoch:10 , loss:1.8799368535904657\n",
            "epoch:20 , loss:1.7807747239158267\n",
            "epoch:30 , loss:1.7019972261360714\n",
            "epoch:40 , loss:1.6360806639989216\n",
            "epoch:50 , loss:1.5761067229225523\n",
            "epoch:60 , loss:1.5240798574969883\n",
            "epoch:70 , loss:1.4744260081790743\n",
            "epoch:80 , loss:1.4289228570461274\n",
            "epoch:90 , loss:1.387681298199154\n",
            "epoch:100 , loss:1.343914052588599\n",
            "epoch:110 , loss:1.3077637118952614\n",
            "epoch:120 , loss:1.2722587805134908\n",
            "epoch:130 , loss:1.2388128412905193\n",
            "epoch:140 , loss:1.2067650013878233\n",
            "epoch:150 , loss:1.175492449204127\n",
            "epoch:160 , loss:1.1450555730433691\n",
            "epoch:170 , loss:1.1184468880153837\n",
            "epoch:180 , loss:1.094443165404456\n",
            "epoch:190 , loss:1.0675590616180783\n",
            "epoch:199 , loss:1.0489019592603048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7-SrVTnm30e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,predict = logits.max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7K0qgi6m65e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self,idx2word,word2idx):\n",
        "        self.idx2word = idx2word\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = 25\n",
        "        self.eos_idx = 8002\n",
        "        self.EN_WHITELIST  = '0123456789abcdefghijklmnopqrstuvwxyz '             \n",
        "            \n",
        "    '''\n",
        "    idx -> word with EOS\n",
        "    '''        \n",
        "    def decode_line(self,sentence_idx,remove_pad=True,remove_eos=True):  #sentence_idx: 1d_matrix     \n",
        "        sentence = []\n",
        "        for w in sentence_idx:\n",
        "            if remove_eos and w==self.eos_idx:\n",
        "                continue\n",
        "            if remove_pad and w==0 : \n",
        "                continue\n",
        "            sentence.append(self.idx2word[w])\n",
        "            #if w==self.eos_idx:\n",
        "            #    break\n",
        "        sentence = ' '.join(sentence)\n",
        "        return sentence\n",
        "    \n",
        "    def decode(self,sentence_idxs,remove_pad=True,remove_eos=True): #sentence_idxs: 2d_matrix \n",
        "        sentences = []\n",
        "        for s in sentence_idxs: \n",
        "            sentences.append(self.decode_line(s,\n",
        "                                              remove_pad=remove_pad,\n",
        "                                              remove_eos=remove_eos))\n",
        "        return sentences\n",
        "            \n",
        "    '''\n",
        "    word -> idx with EOS\n",
        "    '''\n",
        "    def encode_line(self,sentence):  #sentence: 1d_matrix\n",
        "        sentence = sentence.lower()\n",
        "        s_list = ''.join([ ch for ch in sentence if ch in self.EN_WHITELIST ]).split()\n",
        "        sentence_idx = []\n",
        "        for w in s_list:\n",
        "            sentence_idx.append(self.word2idx[w])\n",
        "        n = len(sentence_idx)\n",
        "        if  n > self.max_len:\n",
        "            sentence_idx = sentence_idx[:self.max_len] \n",
        "        elif n < self.max_len:\n",
        "            sentence_idx = sentence_idx + [self.eos_idx] + [0]*(self.max_len-n-1)  \n",
        "        return sentence_idx\n",
        "    \n",
        "    def encode(self,sentences): #sentences: 2d_matrix   \n",
        "        sentence_idxs = []\n",
        "        for s in sentences: \n",
        "            sentence_idxs.append(self.encode_line(s))\n",
        "        return np.array(sentence_idxs)\n",
        "    \n",
        "    def print_QA(self, ques , pred_ans, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs,remove_eos=True,remove_pad=True)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[2])\n",
        "            print('pred A :'+sents[1]) \n",
        "            \n",
        "    def print_QA_1(self, ques , pred_ans_train, pred_ans_test, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans_train[i], pred_ans_test[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs,remove_eos=True,remove_pad=True)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[3])\n",
        "            print('train A:'+sents[1])    \n",
        "            print('test A :'+sents[2]) \n",
        "            \n",
        "    def print_QA_2(self, ques , ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i], ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x47neCXcm8s1",
        "colab_type": "code",
        "outputId": "92d2a727-eb9c-4ef0-889f-7831e0e8ade7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n = 800/25\n",
        "int(n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyNdEulgzbPc",
        "colab_type": "code",
        "outputId": "1fbe3dbb-94e6-4f2f-a6b4-be17f63464de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n = 525/25\n",
        "int(n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8sFIA2Em-YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab(metadata['idx2w'] , metadata['w2idx'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUw5siHxm_1P",
        "colab_type": "text"
      },
      "source": [
        "### Try train corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oROWzDncnAq9",
        "colab_type": "code",
        "outputId": "e9e649ba-39f3-4fe4-abe4-83545837aa28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pred_ans = predict.cpu().view(-1,int(n)).data.numpy().T #predicted answer in train phase\n",
        "strd_ans = a.cpu().view(-1,int(n)).data.numpy().T #standard answer\n",
        "ques     = q.cpu().view(-1,int(n)).data.numpy().T #quenstions\n",
        "vocab.print_QA(ques , pred_ans, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :everythings <EOS> you lady <EOS> know just with you\n",
            "A      :thanks oil nicolet an a like that you mama\n",
            "pred A :nice for much for planning it unk unk unk unk forward to <EOS> <EOS>\n",
            "\n",
            "Q      :ready didnt who im to not me trying\n",
            "A      :so in with old stalk martin a its know <EOS>\n",
            "pred A :unk that you told me unk unk women evil was escaped <EOS>\n",
            "\n",
            "Q      :for what fall just what flatter this unk to\n",
            "A      :much the alcohol lady of much depression compliment always nothing\n",
            "pred A :he was <EOS>\n",
            "\n",
            "Q      :tomorrow branch down in havin kind me time as do\n",
            "A      :for car tobacco of go corn do anger <EOS> not special\n",
            "pred A :oh otis unk so unk <EOS>\n",
            "\n",
            "Q      :unk well <EOS> at love some of <EOS> <EOS> one throw\n",
            "A      :planning unfortunately <EOS> and course for over you compulsive this <EOS>\n",
            "pred A :no thats not of on oil in the unk <EOS>\n",
            "\n",
            "Q      :the you the <EOS> trouble problems you of me\n",
            "A      :it while firearms <EOS> a without <EOS> unk time is\n",
            "pred A :and nicolet on alcohol tobacco when firearms <EOS>\n",
            "\n",
            "Q      :unk know dock seeing <EOS> whered know my off\n",
            "A      :unk you <EOS> young cracking they from if it\n",
            "pred A :of course <EOS>\n",
            "\n",
            "Q      :<EOS> cows by this you it you investigators my\n",
            "A      :unk told thats kid it had god you drug\n",
            "pred A :thats all i i was have the and i not i was other world be to because i to me prostitute <EOS> <EOS>\n",
            "\n",
            "Q      :and maybe the ones two come cant <EOS> game\n",
            "A      :really that he exactly <EOS> <EOS> physical <EOS> can related\n",
            "pred A :why if i dont you that <EOS>\n",
            "\n",
            "Q      :unk he unk almost together from do sure <EOS>\n",
            "A      :looking stupid wasnt why symptoms do <EOS> were\n",
            "pred A :i about of machine is bend a stalk of corn in without cracking it <EOS>\n",
            "\n",
            "Q      :<EOS> was restaurant thirty <EOS> before im that sure\n",
            "A      :forward story <EOS> i headaches youre it going\n",
            "pred A :you know know a do do you <EOS>\n",
            "\n",
            "Q      :and ever years what that crash cmon <EOS>\n",
            "A      :to dr dont what sensitivity the around over\n",
            "pred A :depression anger compulsive unk they went physical symptoms headaches sensitivity to figures <EOS>\n",
            "\n",
            "Q      :you dont handled old if <EOS> override <EOS>\n",
            "A      :it evil yeah watch if to moron me to\n",
            "pred A :from god <EOS>\n",
            "\n",
            "Q      :just you an its i <EOS> you\n",
            "A      :<EOS> has otis em i light thats you her no\n",
            "pred A :you not moron who not unk one turf <EOS>\n",
            "\n",
            "Q      :didnt know unk about asked what know\n",
            "A      :escaped unk anymore call <EOS> been nothing can place thanks\n",
            "pred A :you i know hear that <EOS>\n",
            "\n",
            "Q      :know who boat a you its do you\n",
            "A      :<EOS> the its you what unk i do to <EOS>\n",
            "pred A :do you want that a compliment <EOS>\n",
            "\n",
            "Q      :it that hey <EOS> young not not you can\n",
            "A      :unk no bullshit nicky kind my didnt it make\n",
            "pred A :youre not a a way its it want do it to to to you you to and you <EOS>\n",
            "\n",
            "Q      :<EOS> man pretty kid to broken mean always ow\n",
            "A      :<EOS> hes why <EOS> of turf say around salad no\n",
            "pred A :is it all related <EOS>\n",
            "\n",
            "Q      :is woman and call <EOS> <EOS> you step hey\n",
            "A      :out the machine <EOS> anything do anyone and baby\n",
            "pred A :theyre going to back your unk to a a to pasta to you can what it <EOS>\n",
            "\n",
            "Q      :<EOS> sailor an me i say in what\n",
            "A      :unk hell can you <EOS> you <EOS> pasta come\n",
            "pred A :no it <EOS>\n",
            "\n",
            "Q      :here no old shooter dont that work are\n",
            "A      :the ray would bend dont consider just to\n",
            "pred A :no its do to now <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBPKTLACnFxC",
        "colab_type": "text"
      },
      "source": [
        "### Try test corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYMdbo5HnHYl",
        "colab_type": "code",
        "outputId": "4bb789eb-7846-4c3a-c251-cbd5ae21cb7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()\n",
        "o = model(q,a,feed_previous=True) #logits\n",
        "_,predict_test = o.max(1)\n",
        "#vocab.decode(predict_test.cpu().view(-1,10).data.numpy().T,remove_eos=False,remove_pad=False)\n",
        "pred_ans_test = predict_test.cpu().view(-1,int(n)).data.numpy().T #predicted answer in test phase\n",
        "vocab.print_QA_1(ques , pred_ans, pred_ans_test, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :everythings <EOS> you lady <EOS> know just with you\n",
            "A      :thanks oil nicolet an a like that you mama\n",
            "train A:nice for much for planning it unk unk unk unk forward to <EOS> <EOS>\n",
            "test A :nice and much for planning the unk unk unk unk forward to this <EOS>\n",
            "\n",
            "Q      :ready didnt who im to not me trying\n",
            "A      :so in with old stalk martin a its know <EOS>\n",
            "train A:unk that you told me unk unk women evil was escaped <EOS>\n",
            "test A :unk that you told me unk unk women evil was escaped <EOS>\n",
            "\n",
            "Q      :for what fall just what flatter this unk to\n",
            "A      :much the alcohol lady of much depression compliment always nothing\n",
            "train A:he was <EOS>\n",
            "test A :he did <EOS>\n",
            "\n",
            "Q      :tomorrow branch down in havin kind me time as do\n",
            "A      :for car tobacco of go corn do anger <EOS> not special\n",
            "train A:oh otis unk so unk <EOS>\n",
            "test A :the otis unk the unk <EOS>\n",
            "\n",
            "Q      :unk well <EOS> at love some of <EOS> <EOS> one throw\n",
            "A      :planning unfortunately <EOS> and course for over you compulsive this <EOS>\n",
            "train A:no thats not of on oil in the unk <EOS>\n",
            "test A :no hes dead of the oil on the car <EOS>\n",
            "\n",
            "Q      :the you the <EOS> trouble problems you of me\n",
            "A      :it while firearms <EOS> a without <EOS> unk time is\n",
            "train A:and nicolet on alcohol tobacco when firearms <EOS>\n",
            "test A :jerk nicolet at alcohol tobacco and firearms <EOS>\n",
            "\n",
            "Q      :unk know dock seeing <EOS> whered know my off\n",
            "A      :unk you <EOS> young cracking they from if it\n",
            "train A:of course <EOS>\n",
            "test A :of course <EOS>\n",
            "\n",
            "Q      :<EOS> cows by this you it you investigators my\n",
            "A      :unk told thats kid it had god you drug\n",
            "train A:thats all i i was have the and i not i was other world be to because i to me prostitute <EOS> <EOS>\n",
            "test A :thats not what i dont to the in i all i would i was be unk lady in to the unk kid <EOS>\n",
            "\n",
            "Q      :and maybe the ones two come cant <EOS> game\n",
            "A      :really that he exactly <EOS> <EOS> physical <EOS> can related\n",
            "train A:why if i dont you that <EOS>\n",
            "test A :what if i dont you nicky <EOS>\n",
            "\n",
            "Q      :unk he unk almost together from do sure <EOS>\n",
            "A      :looking stupid wasnt why symptoms do <EOS> were\n",
            "train A:i about of machine is bend a stalk of corn in without cracking it <EOS>\n",
            "test A :i was of machine is bend a stalk of corn over with cracking up <EOS>\n",
            "\n",
            "Q      :<EOS> was restaurant thirty <EOS> before im that sure\n",
            "A      :forward story <EOS> i headaches youre it going\n",
            "train A:you know know a do do you <EOS>\n",
            "test A :you know know martin much do you <EOS>\n",
            "\n",
            "Q      :and ever years what that crash cmon <EOS>\n",
            "A      :to dr dont what sensitivity the around over\n",
            "train A:depression anger compulsive unk they went physical symptoms headaches sensitivity to figures <EOS>\n",
            "test A :depression anger compulsive unk they had physical symptoms headaches sensitivity to ride <EOS>\n",
            "\n",
            "Q      :you dont handled old if <EOS> override <EOS>\n",
            "A      :it evil yeah watch if to moron me to\n",
            "train A:from god <EOS>\n",
            "test A :from god <EOS>\n",
            "\n",
            "Q      :just you an its i <EOS> you\n",
            "A      :<EOS> has otis em i light thats you her no\n",
            "train A:you not moron who not unk one turf <EOS>\n",
            "test A :what not moron you not unk in turf <EOS>\n",
            "\n",
            "Q      :didnt know unk about asked what know\n",
            "A      :escaped unk anymore call <EOS> been nothing can place thanks\n",
            "train A:you i know hear that <EOS>\n",
            "test A :you i know know that <EOS>\n",
            "\n",
            "Q      :know who boat a you its do you\n",
            "A      :<EOS> the its you what unk i do to <EOS>\n",
            "train A:do you want that a compliment <EOS>\n",
            "test A :do you want that a compliment <EOS>\n",
            "\n",
            "Q      :it that hey <EOS> young not not you can\n",
            "A      :unk no bullshit nicky kind my didnt it make\n",
            "train A:youre not a a way its it want do it to to to you you to and you <EOS>\n",
            "test A :its not closed a time you i want do to to you you you intimidate it to <EOS> <EOS>\n",
            "\n",
            "Q      :<EOS> man pretty kid to broken mean always ow\n",
            "A      :<EOS> hes why <EOS> of turf say around salad no\n",
            "train A:is it all related <EOS>\n",
            "test A :is it so related <EOS>\n",
            "\n",
            "Q      :is woman and call <EOS> <EOS> you step hey\n",
            "A      :out the machine <EOS> anything do anyone and baby\n",
            "train A:theyre going to back your unk to a a to pasta to you can what it <EOS>\n",
            "test A :its going to to the unk if make a unk pasta can you think me about <EOS>\n",
            "\n",
            "Q      :<EOS> sailor an me i say in what\n",
            "A      :unk hell can you <EOS> you <EOS> pasta come\n",
            "train A:no it <EOS>\n",
            "test A :no it <EOS>\n",
            "\n",
            "Q      :here no old shooter dont that work are\n",
            "A      :the ray would bend dont consider just to\n",
            "train A:no its do to now <EOS>\n",
            "test A :oh baby knows to hold <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5YBo_XfnLNu",
        "colab_type": "text"
      },
      "source": [
        "### Try Chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfePm7ugnMKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = []\n",
        "lines.append( 'you can do it'  )\n",
        "lines.append( 'how are you'    )\n",
        "lines.append( 'fuck you'  )\n",
        "lines.append( 'jesus christ you scared the shit out of me'  )\n",
        "lines.append( 'youre terrible'  )\n",
        "lines.append( 'is something wrong' )\n",
        "lines.append( 'nobodys gonna get inside' )\n",
        "lines.append( 'im sorry'  )\n",
        "lines.append( 'shut up'  )\n",
        "N = len(lines)\n",
        "lines = vocab.encode(lines)\n",
        "q_o = Variable(torch.from_numpy(lines).long()).cuda()\n",
        "#vocab.decode(vocab.encode(lines))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M1OOSN8nOe9",
        "colab_type": "code",
        "outputId": "63b25070-8bdb-4f3e-b737-3207c6dd32fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "model.eval()\n",
        "o = model(q_o,q_o,feed_previous=True)\n",
        "_,predict_o = o.max(1)\n",
        "#vocab.decode(predict_o.cpu().view(-1,3).data.numpy().T)\n",
        "pred_ans_o = predict_o.cpu().view(-1,N).data.numpy().T #predicted answer \n",
        "vocab.print_QA_2(lines, pred_ans_o)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :you can do it\n",
            "A      :why know do it cant <EOS>\n",
            "\n",
            "Q      :how are you\n",
            "A      :fine long you going <EOS>\n",
            "\n",
            "Q      :fuck you\n",
            "A      :i you <EOS> say\n",
            "\n",
            "Q      :jesus christ you scared the shit out of me\n",
            "A      :dont please <EOS> know me unk <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :youre terrible\n",
            "A      :are also <EOS> me\n",
            "\n",
            "Q      :is something wrong\n",
            "A      :yes it a <EOS> come to\n",
            "\n",
            "Q      :nobodys gonna get inside\n",
            "A      :uh sick be out my to\n",
            "\n",
            "Q      :im sorry\n",
            "A      :what not i wont\n",
            "\n",
            "Q      :shut up\n",
            "A      :let up <EOS> <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lecb2qEnR3U",
        "colab_type": "text"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YGb5meVnTrQ",
        "colab_type": "code",
        "outputId": "f882f072-d9a7-4801-b115-7d4411411b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "epoch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd1zmZz6nVTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict() , 'pth/model_female_sche_sampling_epo%s.pth'%epoch) #save model     \n",
        "np.save('loss_female_sche_sampling_epo%s.npy'%epochs,loss_hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6pf-ntnnYjg",
        "colab_type": "text"
      },
      "source": [
        "### Back up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIsEclMenZXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4jdMV9CnbEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Variable(torch.rand(3,25)*200).long()\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibt1HY9und_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,p = model(x,q,feed_previous=True).max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iBMzZ6nnfsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.decode(p.view(3,25).data.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPa0D8dii5bU",
        "colab_type": "text"
      },
      "source": [
        "## Schedule_Sampling_Seq2Seq-male"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmKujaO1i7vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pickle\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hRaFWgpjLal",
        "colab_type": "text"
      },
      "source": [
        "### Data Handling\n",
        "load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Z9qQDTjM2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = np.load(\"./datasets/m_idx_a_1.npy\")\n",
        "ques = np.load(\"./datasets/m_idx_q_1.npy\")\n",
        "\n",
        "with open('./datasets/metadata_1.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)\n",
        "\n",
        "class MaleDataset(data.Dataset): \n",
        "    def __init__(self,ques,ans):\n",
        "        self.ques = ques\n",
        "        self.ans = ans\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ques_tensor = torch.from_numpy(self.ques[index]).long()\n",
        "        ans_tensor = torch.from_numpy(self.ans[index]).long()\n",
        "        \n",
        "        return ques_tensor , ans_tensor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return 78119"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hek-SEr5kDJW",
        "colab_type": "text"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZQw8W-6kD2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "male_dataset = MaleDataset(ques,ans)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=male_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Z3fDP_kTtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_voc_size=9000,\n",
        "                 trg_voc_size=9000,\n",
        "                 src_embedding_size=256,\n",
        "                 trg_embedding_size=256,\n",
        "                 enc_hidden_size=200,\n",
        "                 dec_hidden_size=200):\n",
        "        \n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.trg_embedding_size = trg_embedding_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "        \n",
        "        self.src_embedder = nn.Embedding(src_voc_size , src_embedding_size)\n",
        "        self.encoder = nn.LSTM(src_embedding_size ,enc_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        \n",
        "        self.trg_embedder = nn.Embedding(trg_voc_size , trg_embedding_size)\n",
        "        self.decoder = nn.LSTM(trg_embedding_size ,dec_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        self.cls = nn.Linear(dec_hidden_size , trg_voc_size)\n",
        "    \n",
        "    def forward(self,source,target,feed_previous=False):\n",
        "        batch_size = source.size()[0]\n",
        "        src_em = self.src_embedder(source)\n",
        "        trg_em = self.trg_embedder(target)\n",
        "        \n",
        "        _ , enc_state = self.encoder(src_em)\n",
        "        \n",
        "        GO = Variable(torch.zeros(batch_size,1,self.trg_embedding_size)).cuda()\n",
        "        \n",
        "        if feed_previous: #test phase\n",
        "            logits_ = []\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "#            inputs = GO\n",
        "            h = enc_state\n",
        "            for i in range(25):\n",
        "                inputs =  torch.unsqueeze(dec_in[:,i,:],1)\n",
        "                output , h = self.decoder(inputs,h)\n",
        "                logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "                logits_.append(logits)\n",
        "                \n",
        "                predicted = logits.max(1)[1]\n",
        "                inputs = self.trg_embedder(predicted)\n",
        "                    \n",
        "            return torch.cat(logits_,0)\n",
        "            \n",
        "        else: #train phase -- schedule sampling\n",
        "            '''\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)  # trg_em.shape=(batch_size, time step, trg_embedding_size )\n",
        "            outputs , _ = self.decoder(dec_in,enc_state)\n",
        "            outputs = outputs.contiguous().view(-1,self.dec_hidden_size)\n",
        "            logits = self.cls(outputs)        \n",
        "            return logits  \n",
        "            '''\n",
        "            logits_ = []\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "            \n",
        "            h = enc_state\n",
        "            for i in range(25):\n",
        "                inputs = torch.unsqueeze(dec_in[:,i,:],1)\n",
        "                \n",
        "                if i < 5:\n",
        "                    inputs =  torch.unsqueeze(dec_in[:,i,:],1)\n",
        "                else:\n",
        "                    if random.random() < 0.7 : # 0.93\n",
        "                        inputs =  torch.unsqueeze(dec_in[:,i,:],1)  #usual training policy \n",
        "                    else:\n",
        "                        inputs = self.trg_embedder(predicted.unsqueeze(1))  #schedule sampling\n",
        "                output , h = self.decoder(inputs,h)\n",
        "                logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "                logits_.append(logits)\n",
        "                predicted = logits.max(1)[1]  # predicted.shape=(batch_size, time step=1)\n",
        "                \n",
        "            return torch.cat(logits_,0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5UNCChwkqgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI6USEqjkw5j",
        "colab_type": "code",
        "outputId": "135c472d-9654-4e30-f0f1-176f4b58b495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (src_embedder): Embedding(9000, 256)\n",
              "  (encoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (trg_embedder): Embedding(9000, 256)\n",
              "  (decoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (cls): Linear(in_features=200, out_features=9000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C56bQZXQkx9I",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbuHpxp2k0ni",
        "colab_type": "code",
        "outputId": "84d7ce1c-0a0c-4dd5-f7f9-151ca815a90e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "train_op = optim.Adam(model.parameters() ,lr=3e-3) # 3e-4\n",
        "\n",
        "epochs = 200\n",
        "loss_hist = []\n",
        "loss_ = 3\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_mean_loss = []\n",
        "\n",
        "    for i , (q,a) in enumerate(train_loader):\n",
        "        q = Variable(q).cuda()\n",
        "        a = Variable(a).cuda()\n",
        "        logits = model(q,a,feed_previous=False)\n",
        "        _,predict = logits.max(1)\n",
        "        \n",
        "        loss = F.cross_entropy(logits ,torch.transpose(a,0,1).contiguous().view(-1))\n",
        "        train_op.zero_grad()\n",
        "        loss.backward()\n",
        "        train_op.step()\n",
        "        \n",
        "        epoch_mean_loss.append(loss.data.item())\n",
        "    \n",
        "    loss_ = np.mean(epoch_mean_loss)\n",
        "    loss_hist.append(loss_)\n",
        "    if epoch % 10 == 0  or epoch == epochs-1:\n",
        "        print(\"epoch:%s , loss:%s\" % (epoch , loss_ ))\n",
        "    if epoch % 50 == 0 or epoch == epochs-1:\n",
        "        torch.save(model.state_dict() , 'pth/model_male_sche_sampling_epo%s.pth'%epoch) #save model\n",
        "        \n",
        "np.save('loss_male_sche_sampling_epo%s.npy'%epochs,loss_hist)\n",
        "\n",
        "# epoch:0 , loss:2.18168046252831\n",
        "# epoch:10 , loss:1.665916821099421\n",
        "# epoch:20 , loss:1.5558263949445776\n",
        "# epoch:30 , loss:1.4736007693163398\n",
        "# epoch:40 , loss:1.405958204680442\n",
        "# epoch:50 , loss:1.3493248187402151\n",
        "# epoch:60 , loss:1.3015521544349866\n",
        "# epoch:70 , loss:1.2607862579588223\n",
        "# epoch:80 , loss:1.2260799525362072\n",
        "# epoch:90 , loss:1.1940073153457127"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 , loss:2.1610007849411335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-2634080a8b09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mepoch_mean_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8nLHTM7lA4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,predict = logits.max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2eR2hd1lZxd",
        "colab_type": "text"
      },
      "source": [
        "### Print Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2tKY48HlFfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self,idx2word,word2idx):\n",
        "        self.idx2word = idx2word\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = 25\n",
        "        self.eos_idx = 8002\n",
        "        self.EN_WHITELIST  = '0123456789abcdefghijklmnopqrstuvwxyz '             \n",
        "            \n",
        "    '''\n",
        "    idx -> word with EOS\n",
        "    '''        \n",
        "    def decode_line(self,sentence_idx,remove_pad=True,remove_eos=True):  #sentence_idx: 1d_matrix     \n",
        "        sentence = []\n",
        "        for w in sentence_idx:\n",
        "            if remove_eos and w==self.eos_idx:\n",
        "                continue\n",
        "            if remove_pad and w==0 : \n",
        "                continue\n",
        "            sentence.append(self.idx2word[w])\n",
        "            #if w==self.eos_idx:\n",
        "            #    break\n",
        "        sentence = ' '.join(sentence)\n",
        "        return sentence\n",
        "    \n",
        "    def decode(self,sentence_idxs,remove_pad=True,remove_eos=True): #sentence_idxs: 2d_matrix \n",
        "        sentences = []\n",
        "        for s in sentence_idxs: \n",
        "            sentences.append(self.decode_line(s,\n",
        "                                              remove_pad=remove_pad,\n",
        "                                              remove_eos=remove_eos))\n",
        "        return sentences\n",
        "            \n",
        "    '''\n",
        "    word -> idx with EOS\n",
        "    '''\n",
        "    def encode_line(self,sentence):  #sentence: 1d_matrix\n",
        "        sentence = sentence.lower()\n",
        "        s_list = ''.join([ ch for ch in sentence if ch in self.EN_WHITELIST ]).split()\n",
        "        sentence_idx = []\n",
        "        for w in s_list:\n",
        "            sentence_idx.append(self.word2idx[w])\n",
        "        n = len(sentence_idx)\n",
        "        if  n > self.max_len:\n",
        "            sentence_idx = sentence_idx[:self.max_len] \n",
        "        elif n < self.max_len:\n",
        "            sentence_idx = sentence_idx + [self.eos_idx] + [0]*(self.max_len-n-1)  \n",
        "        return sentence_idx\n",
        "    \n",
        "    def encode(self,sentences): #sentences: 2d_matrix   \n",
        "        sentence_idxs = []\n",
        "        for s in sentences: \n",
        "            sentence_idxs.append(self.encode_line(s))\n",
        "        return np.array(sentence_idxs)\n",
        "    \n",
        "    def print_QA(self, ques , pred_ans, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs,remove_eos=True,remove_pad=True)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[2])\n",
        "            print('pred A :'+sents[1]) \n",
        "            \n",
        "    def print_QA_1(self, ques , pred_ans_train, pred_ans_test, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans_train[i], pred_ans_test[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs,remove_eos=True,remove_pad=True)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[3])\n",
        "            print('train A:'+sents[1])    \n",
        "            print('test A :'+sents[2]) \n",
        "            \n",
        "    def print_QA_2(self, ques , ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i], ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJmul51KlcwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict.unsqueeze(1).size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10EWqPCcleLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 175/25\n",
        "int(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al6WsvNKljZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab(metadata['idx2w'] , metadata['w2idx'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byndu8rTluR9",
        "colab_type": "text"
      },
      "source": [
        "### Try train corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVXWSCOdl22A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_ans = predict.cpu().view(-1,int(n)).data.numpy().T #predicted answer in train phase\n",
        "strd_ans = a.cpu().view(-1,int(n)).data.numpy().T #standard answer\n",
        "ques     = q.cpu().view(-1,int(n)).data.numpy().T #quenstions\n",
        "vocab.print_QA(ques , pred_ans, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KVKMTunl371",
        "colab_type": "text"
      },
      "source": [
        "### Try test corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWh4oXE-l_-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "o = model(q,a,feed_previous=True) #logits\n",
        "_,predict_test = o.max(1)\n",
        "#vocab.decode(predict_test.cpu().view(-1,10).data.numpy().T,remove_eos=False,remove_pad=False)\n",
        "pred_ans_test = predict_test.cpu().view(-1,int(n)).data.numpy().T #predicted answer in test phase\n",
        "vocab.print_QA_1(ques , pred_ans, pred_ans_test, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQyQsY0AmI0y",
        "colab_type": "text"
      },
      "source": [
        "### Try Chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISup_D_YmMqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = []\n",
        "lines.append( 'you can do it'  )\n",
        "lines.append( 'how are you'    )\n",
        "lines.append( 'fuck you'  )\n",
        "lines.append( 'jesus christ you scared the shit out of me'  )\n",
        "lines.append( 'youre terrible'  )\n",
        "lines.append( 'is something wrong' )\n",
        "lines.append( 'nobodys gonna get inside' )\n",
        "lines.append( 'im sorry'  )\n",
        "lines.append( 'shut up'  )\n",
        "N = len(lines)\n",
        "lines = vocab.encode(lines)\n",
        "q_o = Variable(torch.from_numpy(lines).long()).cuda()\n",
        "#vocab.decode(vocab.encode(lines))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsw2jdQbmN34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "o = model(q_o,q_o,feed_previous=True)\n",
        "_,predict_o = o.max(1)\n",
        "#vocab.decode(predict_o.cpu().view(-1,3).data.numpy().T)\n",
        "pred_ans_o = predict_o.cpu().view(-1,N).data.numpy().T #predicted answer \n",
        "vocab.print_QA_2(lines, pred_ans_o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xncmgO7mhRB",
        "colab_type": "text"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX_Kw961mjBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch\n",
        "torch.save(model.state_dict() , 'pth/model_male_sche_sampling_epo%s.pth'%epoch) #save model     \n",
        "np.save('loss_male_sche_sampling_epo%s.npy'%epochs,loss_hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qrwU8jemu8N",
        "colab_type": "text"
      },
      "source": [
        "### Back up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejVDWg_tmyFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0O2BTNrm0RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Variable(torch.rand(3,25)*200).long()\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzRjQR8zm2jQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,p = model(x,q,feed_previous=True).max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh_wJZRDm5Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.decode(p.view(3,25).data.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91veFLojn0g2",
        "colab_type": "text"
      },
      "source": [
        "## loss_hist_plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBPGb6MCnxUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BjBju3Rn2Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = np.load('loss_female_epo200.npy')\n",
        "m = np.load('loss_male_epo200.npy')\n",
        "f_s = np.load('loss_female_sche_sampling_epo200.npy')\n",
        "m_s = np.load('loss_male_sche_sampling_epo200.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLGPwLPn4NV",
        "colab_type": "code",
        "outputId": "d2883a5f-b0f4-45ff-abc3-2f38b9a56fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "fig = plt.figure(1)         \n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(f,label='female',c='r')\n",
        "ax.plot(m,label='male')\n",
        "ax.plot(f_s,label='female_schedule_sampling')\n",
        "ax.plot(m_s,label='male_schedule_sampling')\n",
        "ax.legend()\n",
        "fig.savefig('loss.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3xVRfr48c/c9Ep6r0CAEEIIkFCi\n9OoCCohdQUV+gKK49rairq6FdVVY5SvCsrBWBBE7ICAgTUoCgQAphJAE0nu/ufP744QYICEJpF7m\n/XrlleScc8+ZE/G5c+c884yQUqIoiqIYL117N0BRFEVpXSrQK4qiGDkV6BVFUYycCvSKoihGTgV6\nRVEUI6cCvaIoipFrNNALIXyFENuEEMeFEMeEEI/Vc8wIIUSBECK65utvdfZNEEKcFEIkCCGebekb\nUBRFUa7MtAnH6IEnpJSHhBB2wEEhxGYp5fFLjtsppZxUd4MQwgT4NzAWSAX+EEJsrOe1iqIoSitp\nNNBLKc8B52p+LhJCxAHeQFOCdSSQIKVMAhBCfAHc3NhrXVxcZEBAQBNOryiKogAcPHgwW0rpWt++\npvToawkhAoBwYF89u4cIIWKAdOBJKeUxtDeEs3WOSQUGNXadgIAADhw40JymKYqiXNeEEGca2tfk\nQC+EsAXWAQullIWX7D4E+Espi4UQNwEbgKBmNnIOMAfAz8+vOS9VFEVRrqBJWTdCCDO0IP+plHL9\npfullIVSyuKan38EzIQQLkAa4FvnUJ+abZeRUn4spRwopRzo6lrvpw9FURTlKjQl60YAK4A4KeW7\nDRzjUXMcQojImvPmAH8AQUKIQCGEOXAHsLGlGq8oiqI0rilDN1HAvcBRIUR0zbbnAT8AKeUy4FZg\nnhBCD5QBd0itLKZeCPEI8AtgAqysGbtXlA6nqqqK1NRUysvL27spitIgS0tLfHx8MDMza/JrREcs\nUzxw4ECpHsYqbe306dPY2dnh7OxMzQdURelQpJTk5ORQVFREYGDgRfuEEAellAPre52aGasoNcrL\ny1WQVzo0IQTOzs7N/tSpAr2i1KGCvNLRXc2/UeMK9L+9DQlb2rsViqIoHYpxBfrf34eEre3dCkW5\nah988AHBwcHcfffdrXL+RYsWsXjx4lY5t9JxNWtmbIdnZg2Vxe3dCkW5ah9++CFbtmzBx8envZui\nGBHj6tGb20BVaXu3QlGuyty5c0lKSmLixIm8/vrrPPDAA0RGRhIeHs63334LwKpVq7jlllsYO3Ys\nAQEBLF26lHfffZfw8HAGDx5Mbm4uAMuXLyciIoKwsDCmT59Oaenl/18kJiYyYcIEBgwYwI033siJ\nEyfa9H6VtmNcPXpzW6gsae9WKMZg4UKIjm78uObo1w/ee6/B3cuWLePnn39m27ZtvPvuu4waNYqV\nK1eSn59PZGQkY8aMASA2NpbDhw9TXl5O9+7deeuttzh8+DCPP/44q1evZuHChUybNo2HHnoIgBdf\nfJEVK1awYMGCi643Z84cli1bRlBQEPv27WP+/Pls3aqGPo2RkQV6GzV0oxiFTZs2sXHjxtrx9PLy\nclJSUgAYOXIkdnZ22NnZ0aVLFyZPngxAaGgoR44cAbQ3gxdffJH8/HyKi4sZP378RecvLi5m9+7d\nzJgxo3ZbRUVFW9ya0g6MLNBbQ/ml9dYU5SpcoefdFqSUrFu3jp49e160fd++fVhYWNT+rtPpan/X\n6XTo9XoAZs2axYYNGwgLC2PVqlVs3779ovMYDAYcHByIbulPLUqHZHxj9GroRjEC48ePZ8mSJVyY\nuX748OFmvb6oqAhPT0+qqqr49NNPL9tvb29PYGAga9euBbQ3lpiYmGtvuNIhGVmgV2P0inF46aWX\nqKqqom/fvoSEhPDSSy816/WvvfYagwYNIioqil69etV7zKeffsqKFSsICwsjJCSk9oGvYnyMq9bN\n93+F4xvg6aSWb5Ri9OLi4ggODm7vZihKo+r7t3r91LpRQzeKoiiXMbJAbwv6cqjWt3dLFEVROgwj\nC/TW2vcq1atXFEW5wLgCfWqm9r1SzY5VFEW5wLgC/dv/0r6rcXpFUZRaxhXodTUTSdTsWEVRlFrG\nGehVYTPlOrR9+3YmTZrU3s1QOiDjCvSmVtp3NXSjKIpSy0gDvRq6UTqn5ORkevXqxaxZs+jRowd3\n3303W7ZsISoqiqCgIPbv38/+/fsZMmQI4eHhDB06lJMnT152npKSknrLHCvXp0aLmgkhfIHVgDsg\ngY+llO9fcszdwDOAAIqAeVLKmJp9yTXbqgF9QzO3WoSZjfZdZd0o1+iV745xPL1lC+T19rLn5ckh\njR6XkJDA2rVrWblyJREREXz22Wfs2rWLjRs38sYbb7B69Wp27tyJqakpW7Zs4fnnn2fdunUXneP1\n11+vt8yxjY1Ni96T0jk0pXqlHnhCSnlICGEHHBRCbJZSHq9zzGlguJQyTwgxEfgYGFRn/0gpZXbL\nNbsB5hcCvRq6UTqvwMBAQkNDAQgJCWH06NEIIQgNDSU5OZmCggJmzpxJfHw8QgiqqqouO0dDZY5V\niYfrU6OBXkp5DjhX83ORECIO8AaO1zlmd52X7AXaZx00C1vtuxq6Ua5RU3reraWxMsQvvfQSI0eO\n5JtvviE5OZkRI0Zcdo6Gyhwr16dmjdELIQKAcGDfFQ57EPipzu8S2CSEOCiEmHOFc88RQhwQQhzI\nyspqTrP+ZGULBlSPXjFqBQUFeHt7A9rSgvW51jLHinFpcqAXQtgC64CFUsp6By+FECPRAv0zdTbf\nIKXsD0wEHhZCDKvvtVLKj6WUA6WUA11dXZt8AxexttEGmlR6pWLEnn76aZ577jnCw8NrFxq51LWW\nOVaMS5PKFAshzIDvgV+klO82cExf4BtgopTyVAPHLAKKpZSLr3S9qy5T/NhjYLYKht0HU5Y0//XK\ndU2VKVY6ixYvUyyEEMAKIO4KQd4PWA/cWzfICyFsah7gIoSwAcYBsU28l+aztoZKgxq6URRFqaMp\nWTdRwL3AUSHEhQUmnwf8AKSUy4C/Ac7Ah9r7Qm0apTvwTc02U+AzKeXPLXoHdWx3zser3JQeFSrQ\nK4qiXNCUrJtdaPnxVzpmNjC7nu1JQNhVt66ZnnbZy+1ltjyhFghXFEWpZVQzY20wp1jooLyovZui\nKIrSYRhVoC/RW5FlaaXG6BVFUeowqkBfVm1JvqmZWmFKURSlDqMK9DppQYmJTuXRK4qi1GFUgd5U\nWlJiIkBf1t5NUZSr8sEHHxAcHMzdd9/dKudftGhRbf2b1hIQEEB29tWXthoxYgSNzaNpi/u4VnXX\nB9i4cSNvvvlmu7WlKemVnYa5sKJMB0g9VFeBiVl7N0lRmuXDDz9ky5Yt+Pi0T7kopXVMmTKFKVOm\ntNv1jSrQWwgrSkxqfikvBBvndm2P0on99CycP9qy5/QIhYkN9+rmzp1LUlISEydO5I477iAxMZHY\n2FiqqqpYtGgRN998M6tWrWLDhg2UlJQQHx/Pk08+SWVlJWvWrMHCwoIff/wRJycnli9fzscff0xl\nZSXdu3dnzZo1WFtbX3S9xMREHn74YbKysrC2tmb58uX06tWr3ratXbuWV155BRMTE7p06cKOHTuo\nrq7mmWee4eeff0an0/HQQw+xYMECAJYsWcJ3331HVVUVa9eupVevXpSUlLBgwYLL7qmsrIz777+f\nmJgYevXqRVnZn5/IbW1tKS7WihR+/fXXfP/995fV97nW+0hOTubee++lpER7trd06VKGDh3K9u3b\nefnll3FwcODo0aPcdttthIaG8v7771NWVsaGDRvo1q0bs2bNwtLSkgMHDlBYWMi777572Upfq1at\n4sCBAyxdupRZs2Zhb2/PgQMHOH/+PG+//Ta33norBoOBRx55hK1bt+Lr64uZmRkPPPAAt956a4P/\nZprKqIZuLE2sqTAxIAHOx7R3cxSlWZYtW4aXlxfbtm2jpKSEUaNGsX//frZt28ZTTz1VG4hiY2NZ\nv349f/zxBy+88ALW1tYcPnyYIUOGsHr1agCmTZvGH3/8QUxMDMHBwaxYseKy682ZM4clS5Zw8OBB\nFi9ezPz58xts26uvvsovv/xCTEwMGzduBODjjz8mOTmZ6Ohojhw5ctFwk4uLC4cOHWLevHm1QywX\nauRfek8fffQR1tbWxMXF8corr3Dw4MFm/d2u9T7c3NzYvHkzhw4d4ssvv+TRRx+tPT4mJoZly5YR\nFxfHmjVrOHXqFPv372f27NksWfJnmZXk5GT279/PDz/8wNy5cykvL79im8+dO8euXbv4/vvvefbZ\nZwFYv349ycnJHD9+nDVr1rBnz55m/R2uxKh69FZmNkgklUJgkbIXuo1q7yYpndUVet5toaF68gAj\nR47Ezs4OOzs7unTpwuTJkwEIDQ3lyJEjgPZm8OKLL5Kfn09xcTHjx4+/6PzFxcXs3r2bGTNm1G6r\nqKhosD1RUVHMmjWL2267jWnTpgGwZcsW5s6di6mpFkacnJxqj79wzIABA1i/fv0V72nHjh21wbVv\n37707du3yX+nlriPqqoqHnnkEaKjozExMeHUqT9LdUVERODp6QlAt27dGDduHKD9rbdt21Z73G23\n3YZOpyMoKIiuXbty4sSJK7b7lltuQafT0bt3bzIyMgDYtWsXM2bMQKfT4eHhwciRI5v8d2iMUQV6\nazNbqIJiEw8szuxu/AWK0kE1VE9+3759jdarB5g1axYbNmwgLCyMVatWsX379ovOYzAYcHBwIDo6\nmqZYtmwZ+/bt44cffmDAgAGN9rovtMnExKS2TVdTI7+mfApAvb3klriPJUuW4O7uTkxMDAaDAUtL\ny8vuAxr+W1/azvp+v1Td8zalsOS1MqqhGxsLewBKhBukHtAeyCpKJ3St9eSLiorw9PSkqqqKTz/9\n9LL99vb2BAYGsnbtWkALNjExDQ93JiYmMmjQIF599VVcXV05e/YsY8eO5f/+7/9qA15ubu5V3dOw\nYcP47LPPAO2TyIVPJQDu7u7ExcVhMBj45ptvWuU+CgoK8PT0RKfTsWbNGqqrq694H/VZu3YtBoOB\nxMREkpKSrmrBl6ioKNatW4fBYCAjI+OyN+drYVSB3t7SDoACvYOWYnnuSCOvUJSO6Vrryb/22msM\nGjSIqKioBh9Mfvrpp6xYsYKwsDBCQkKuuID4U089RWhoKH369GHo0KGEhYUxe/Zs/Pz86Nu3L2Fh\nYbXBurn3NG/ePIqLiwkODuZvf/sbAwYMqH3Nm2++yaRJkxg6dGjtEEpL38f8+fP573//S1hYGCdO\nnLiqdXX9/PyIjIxk4sSJLFu27KJPBU01ffp0fHx86N27N/fccw/9+/enS5cuzT5PfZpUj76tXW09\n+lc2f8vX6S/yQXwoI01/gHF/h6ELWqGFijFS9eiVqzFr1iwmTZrUItkxxcXF2NrakpOTQ2RkJL//\n/jseHh6XHdfcevRGNUbvYKmtGZujF+DWDZJ3qUCvKEqnMWnSJPLz86msrOSll16qN8hfDaMK9I6W\n2hh9rqyArsPhyFo1cUpRmuH111+vHe++YMaMGbzwwgvt1KKr05b30dC6vVejJcfl6zKuQG+lBfp8\nqqDrCDiwEtIOgd+gdm2XonQWL7zwQqcL6vUxlvtoKUb1MNbZuibQCz0E3AgISNrerm1SFEVpb0YV\n6J2sbJBSR7GoBmsn8OqnAr2iKNc9owr0tpamYLCg2KQmD7brCEjdDxVqxSlFUa5fxhXoLUyRBguK\nTWsCffexYNDDqV/at2GKoijtqNFAL4TwFUJsE0IcF0IcE0I8Vs8xQgjxgRAiQQhxRAjRv86+mUKI\n+JqvmS19A3XZWJgiqy0oM6sJ9H5DwM4TYte35mUVpUOoW/+8tcyaNYuvv/76ql/flDrybXEfLcHW\nVkvnTk9Pb5Ec+tbUlB69HnhCStkbGAw8LITofckxE4Ggmq85wEcAQggn4GVgEBAJvCyEcGyhtl/G\nzESHMFhQZmrQNuh0EDIVEjZDWX5rXVZRlOuYl5fXNb35tYVG0yullOeAczU/Fwkh4gBv4Hidw24G\nVkttmu1eIYSDEMITGAFsllLmAgghNgMTgM9b9C7qMDGYU2FWp1ZFn+mw90M48QOEt86qPYrxeWv/\nW5zIvXIFwubq5dSLZyKfueIxycnJTJgwgcGDB7N7924iIiK4//77efnll8nMzKytW/PYY49RXl6O\nlZUV//nPfy6rrdJQ7ff6HDt2jPvvv5/KykoMBgPr1q0jKCiI1atXs3jxYoQQ9O3blzVr1gCwY8cO\n3n333YtqqQO88847fPXVV1RUVDB16lReeeUVQMtp/+9//4ubmxu+vr61JQ5GjBjB4sWLGThwINnZ\n2QwcOJDk5OQWv49bbrmFs2fPUl5ezmOPPcacOXMArUc+b948fvzxRzw9PXnjjTd4+umnSUlJ4b33\n3mPKlCmsWrWKb775hoKCAtLS0rjnnnt4+eWXL/tvNmnSJGJjY1m1ahUbN26ktLSUxMREpk6dyttv\nvw3AihUreOutt3BwcCAsLAwLCwuWLl16xX8PLaVZefRCiAAgHNh3yS5v4Gyd31NrtjW0vb5zz0H7\nNICfn19zmnURU4MFlWYGMBi0Hr33AHDwh5jPVaBXOoWEhATWrl3LypUriYiI4LPPPmPXrl1s3LiR\nN954g9WrV7Nz505MTU3ZsmULzz//POvWrbvoHBdqv69cuZL8/HwiIyMZM2ZMvXVcli1bxmOPPcbd\nd99NZWUl1dXVHDt2jL///e/s3r0bFxeXiwqWXailfuLECaZMmcKtt97Kpk2biI+PZ//+/UgpmTJl\nCjt27MDGxoYvvviC6Oho9Ho9/fv3v6iWTWOu9T4AVq5ciZOTE2VlZURERDB9+nScnZ1ra/6/8847\nTJ06lRdffJHNmzdz/PhxZs6cWbsi1P79+4mNjcXa2pqIiAj+8pe/MHBgvZUGAIiOjubw4cNYWFjQ\ns2dPFixYgImJCa+99hqHDh3Czs6OUaNGERYW1uS/w7VqcqAXQtgC64CFUsrClm6IlPJj4GPQat1c\n7XnMpDlV5tVQXg7W1iAE9L8Ptr4G2fHgEtRibVaMV2M979YUGBhIaGgoACEhIYwePRohBKGhoSQn\nJ1NQUMDMmTOJj49HCEFV1eVVWhuq/V5fLZ8hQ4bw+uuvk5qayrRp0wgKCmLr1q3MmDEDFxcX4OJa\n8/XVUt+0aRObNm0iPDwc0Gq2xMfHU1RUxNSpU2tXt2rucnrXeh+grcN7ofLl2bNniY+Px9nZGXNz\ncyZMmABo9eUtLCwwMzOr/TtfMHbsWJydtdXqpk2bxq5du64Y6EePHl1bjKx3796cOXOG7Oxshg8f\nXvt3nDFjxkV171tbk7JuhBBmaEH+UyllfU820wDfOr/71GxraHursZCWVJvpISfnz43h94LOFA78\npzUvrSgtorEa6C+99BIjR44kNjaW7777rt467Rdqv0dHRxMdHd1gcAS466672LhxI1ZWVtx0001s\n3bq1ye27UBRRSslzzz1Xe72EhAQefPDBK57H1NQUg0F7ntbQikzXeh/bt29ny5Yt7Nmzh5iYGMLD\nw2uvZWZmVls3vrVqzdetx9+empJ1I4AVQJyU8t0GDtsI3FeTfTMYKKgZ2/8FGCeEcKx5CDuuZlur\nsTSzwWCqp/pknfFVO3cIngzRn0JlaWteXlFaXUFBAd7e2ghoQ3VWmlPPPikpia5du/Loo49y8803\nc+TIEUaNGsXatWvJqekwNaXW/MqVK2vXd01LSyMzM5Nhw4axYcMGysrKKCoq4rvvvqt9TUBAQO0C\nJg09zLzW+ygoKMDR0RFra2tOnDjB3r17r3gf9dm8eTO5ubm168RGRUU1+xwRERH89ttv5OXlodfr\nLxtqa21N6dFHAfcCo4QQ0TVfNwkh5goh5tYc8yOQBCQAy4H5ADUPYV8D/qj5evXCg9nWYmXpAEDx\niUtq0Uf+PyjP1+rfKEon9vTTT/Pcc88RHh7eYG+xOfXsv/rqK/r06UO/fv2IjY3lvvvuIyQkhBde\neIHhw4cTFhbGX//61yu2ady4cdx1110MGTKE0NBQbr31VoqKiujfvz+33347YWFhTJw4kYiIiNrX\nPPnkk3z00UeEh4eTnZ3dKvcxYcIE9Ho9wcHBPPvsswwePPiK91GfyMhIpk+fTt++fZk+ffoVh20a\n4u3tzfPPP09kZCRRUVEEBAS0WK35pjCqevQA932xnMMVH/DFsf6EvP3fi3eumQbph+GxaLBsuz+y\n0jmoevTKpVatWsWBAwdaJDvmQq15vV7P1KlTeeCBB5g6depVnau59eiNamYsgLtlIAAJ+QmX7xz9\nNyjLhd8/aONWKYpyvVu0aBH9+vWjT58+BAYGcsstt7TZtY2qTDGAu7U3Ms+E+OrMy3d69YPQ2+D3\n97WJVB592r6BitJOfvnlF5555uJsosDAwHrXYu3I2vI+Zs2axaxZs1rkXI3NCG5NRhfo7S3NMVS6\ncsqxEAoLwd7+4gMmvAlJ22DDPHhoq1qURLmIlLLRrIrOavz48YwfP769m3HNjOU+rtbVDLcb39CN\nvSWGCg/ifazgRD0zG22cYdK/4PwRWDcbqts/9UnpGCwtLcnJybmq/5EUpS1IKcnJyWn24uNG16Pv\n6WGHocKdbDcdxXEx2EZGXn5Q8GQY9zpsegGEDqYtBxOj+1MozeTj40NqaipZWVnt3RRFaZClpSU+\nPj7Neo3RRbeuLrZQ5Q5AYvJBwnio/gOHPgKyGjb/rSbYfww6kzZsqdLRmJmZERgY2N7NUJQWZ3SB\n3txUh49NIFlA4vlYrlhNIuoxMFTDr6+Agy+MWdQmbVQURWlLRjdGDxDsEgAGU+LLU7XiZldy419h\nwCzY9S84+VNbNE9RFKVNGWWg7+XRBX2ZP7uDzOt/IHupCW+BZxisnaUWKVEUxegYZaDv6WGHvrgX\nSd4WpO75ufEXmFnC3evAsx98fT/88UnrN1JRFKWNGG+gL9KmB+9IvnIlvlq2rjBzI/SYCD88CTFf\ntmILFUVR2o5RBnpfR2sscce21I4duuSmv9DUAmb8B/yj4Js5WsCvKmu1diqKorQFowz0Op1gYIAj\n1eV92O8rKclqRgl8Myu4Zx0Mfhj+WA7/nQwl9VfWUxRF6QyMMtADjO3tTnZBOFVmOr79paEy+g0w\ns4QJb8Dt/4PzR+GjofDzc1B0vnUaqyiK0oqMNtCPDnbHUO6He5Y1nxZuwyAbSbOsT/BkmPUjePXX\nHtB+cTdUX75sm6IoSkdmtIHe28GK3p72mJaOJsWmih1JW67uRD4D4K4vtDIJaQdgyyJQtVAURelE\njDbQA4zp7c6p0sG45ehZvve9aytWFXILDLgf9iyFT0ZD2sGWa6iiKEorMupAf3M/LwyY0PNkN47o\nz7L1bBNTLRvyl3/ClKVQmA4rxmmzaStLWqaxiqIorcSoA303V1uG93Dljy73EXC+kvf/eBe94RrK\nEutMoP+9MH8P9JyoDeP8s5cW8BsrtaAoitJOjDrQA8yKCiBLZ0PU706cLk7h8xOfX/tJrRzhtjVw\n/88QcIMW8L+4C8ryr/3ciqIoLazRQC+EWCmEyBRCxDaw/ykhRHTNV6wQoloI4VSzL1kIcbRm39Wt\n9n2Nhge50tXVhm3+D3FjfDVLDi/hfEkLpEkKAf5D4I7PYOLbkLAZPh4OZ/+49nMriqK0oKb06FcB\nExraKaV8R0rZT0rZD3gO+E1KmVvnkJE1++tdnby16XSChWN6cNLGncgtzkh9Fa/uebXlVhESAgb9\nPy0NU18BK8bA6lvg9E6VnaMoSofQaKCXUu4Achs7rsadQAuMjbSsSaGe9Pa0Y2XEQyz4pYKdaTv5\n7MRnLXsRv0Hw8H4Y+ypkHIP/ToLP74CKopa9jqIoSjO12Bi9EMIaree/rs5mCWwSQhwUQsxpqWs1\nl04neHpCL1JsnClL7Mnw6gD+eeCfxGbXOxp19SzttcVMFh6FcX+H+M2wcgJkHG/Z6yiKojRDSz6M\nnQz8fsmwzQ1Syv7AROBhIcSwhl4shJgjhDgghDjQGmt2jujpxphgN94fdh+PLD6Fq4Uzj259lIyS\njBa/FmaWMHQB3L0WCtNg2Q3w/eOQf7blr6UoitKIlgz0d3DJsI2UMq3meybwDVDPSt21x34spRwo\npRzo6urags3608uTQ6g2N+f9ntNZctCPkqoS5v06j+yyVipa1n00LDgEAx+AQ2vgg3D49VWoKm+d\n6ymKotSjRQK9EKILMBz4ts42GyGE3YWfgXFAC4+VNI+vkzWPje3JTz2jOPl9Iu/5PkJqUSr3/XQf\n6cXprXNRayf4y2J4LBpCb4Wd/9Syc7ITWud6iqIol2hKeuXnwB6gpxAiVQjxoBBirhBibp3DpgKb\npJR1p4m6A7uEEDHAfuAHKWUTlntqXXNu7EqYpy0vjZtHt8eW8smwpeSX5zNn85zW69kDdPGBqcu0\nEsglWbB8JGx7A/JTWu+aiqIogGixNMMWNHDgQHngQOul3SdkFjP5vd8IO32E/zmlEfvybOZsnoOv\nnS8rx6+ki0WXVrs2oAX3H57QHtbqTKD/fTDyBbBxad3rKopitIQQBxtKYzf6mbH16e5my9+nh7HX\nvy/vxhbRb38q7418j9MFp5n/63xKq0pbtwEOftqD2oVHtUJph9bAR1GQsEXl3iuK0uKuy0APMH2A\nD3f09+LDIbfx1T9WMlTXlbeHvU1sdixzNs+hsLKw9Rvh4KuN38/ZBhZ28L/p2vj97qWQe7r1r68o\nynXhug30AK9OC+MGD0ueHzqTHQ8+wRiPG/nn8H9yLOcYM3+aydmiNkqH9AiF/7cD/vKuVhxt0wuw\nNAIOrW6b6yuKYtSuyzH6uorKq5jx1s+czS/nq6JdhCx/jz3n9vLEb0+gEzreHvY2Q72GtklbauUl\na3n3iVuh2yjodzeETAPddf2+rCjKFagx+iuwszRj1cKx2FuYMtMqgvh3/s0QryF8+ZcvcbVyZd6W\neayMXdlytXGawjEA7loLo17U0jDXPagtUp4d33ZtUBTFaFz3gR7Ao4slaxaORlhYcOdZR+K/+h5f\ne18+velTxvqP5V8H/8VTO65ea/sAACAASURBVJ5q/Ye0dZmYwrCnYOERbbGT80fh35GwYb5WS0dR\nFKWJVKCv0d3Dns8XDEeYmnDnrgJObdmNtZk17wx7h8cHPM7mM5u556d72m7c/gIhtMVOFhyEQfMg\ndh18NBQ+nQGZJ9q2LYqidErX/Rj9pRLjznDnst/RI1h5ex/6DQ0FYHfabp7a8RQAbw97myjvqHZp\nH6W5cPA/sOt9qCgAtxDoexsMng+m5u3TJkVR2p0ao2+GbsH+fHVvX2yqyrlzfTzb9p4CYKj3UL6Y\n9AXuNu7M/3U+r+99ndSi1LZvoLUT3PgEPHoYRr8Mll1gy8uwLAqSfmv79iiK0uGpHn0DMrfv5v7V\nBznp4s+bN3Xn1hG9ASitKmXxgcV8k/ANAPPC5vFAnwcw1Zm2X2NPbYKfntKydbqNhuDJEHYHmFm1\nX5sURWlTV+rRq0B/BUWbfmXeqn3s8gtjbqQnT90SjolOAJBRksE/D/yTn5J/ItwtnMXDF+Nm7dZ+\nja0qg91L4PD/IP8MOHWD0X8D7wHaxCxFUYyaCvTXoHLTZl557zs+7TuekV0deP++SOwtzWr3/5D0\nA6/seQUrUysWD19MhEdEO7YWrYRC0nb47jEt4AP4DoLB86DXZC2bR1EUo6MC/bXavJn/Pb+ERSNn\n4+diw8ezIunuZle7OyEvgce3P87ZorPMDJnJfb3vw9nKuR0bjFbz/lw0nN0PB1ZC3mmw94EbFkL/\nmerBraIYGRXoW8Ivv7B3/nM8MuUZymzteXtGP/7S17N2d0lVCa/vfZ3vk77HwsSC2aGzmdVnFhYm\nFu3Y6BqGaojfBL9/ACm7wcYNfCMh8iHoOqK9W6coSgtQgb6l/PIL5+6dzfwpz3DYJZCHbgzk6Qm9\nMDP5M3kpqSCJpYeXsvnMZnxsfXg28lmG+w5vx0bXIaVWIfPIl5C8C4rOQdidMPJ5raKmoiidlgr0\nLWn/fiqn3MxrEXewJmQMYb4OfHBHP/ydbS46bO+5vfxj3z9IKkjiRu8beSbyGfzt/dup0fWoKoPf\n3oY9S7U3gG6joOcECL8XTMwaf72iKB2KCvQtLTERJkzgR0sfnp3yBAZTM167JYSp4T4XHVZlqOKz\nuM/4KOYjKqsrmRUyi9mhs7E2s26nhtejIA32fggnf4LcRPDoqy2CEngjmNs0/npFUToEFehbQ1YW\nTJ5MWtxpHn/sQ/ZXWnJLPy9eu6UPdpYX94izSrN479B7bEzciLu1OwsHLGRiwERMdCbt1PgGxH0H\n3/8VSjLB1ApCp0O/e7Tx/I7WVkVRLqICfWspLYU770T/3ff8+8kPeN8kAA97S167pQ+jg90vO/xw\n5mHe2PcGJ3JPEGAfwBMDn2CE74i2b/eVVJVrD2yPfwtHvoKqUu3hbf97IewucO6m1d9RFKVDUYG+\nNen18Oij8NFHHLrjIZ4deCenskr5S6gnL0/ujZu95UWHG6SBLWe28O/of5NUkMQwn2HMC5tHH5c+\n7XQDV1BeCAmb4ejX2tAOEuy8tKAfMRts23GCmKIoF1GBvrVJCR98AH/9K5WhYSz/2//x/qFsLEx1\nPDcxmDsifNHpLu4FVxmqWHN8DZ8c/YSiyiKGeg1lbthcwt3C2+kmGpGfomXsnPwZ4n8BE3MIvQ3G\nLAJb1/ZunaJc964p0AshVgKTgEwp5WXdTiHECOBb4MIip+ullK/W7JsAvA+YAJ9IKd9sSoM7XaC/\n4Jdf4PbbwcyM02vW8nyKJXuSchjo78g/poUS5G532UuKK4v58uSXrD6+mtzyXKZ0m8LC/gtxte7A\nwTM7AfZ9pJVbsHWHMS9DcRb0ukmlaSpKO7nWQD8MKAZWXyHQPymlnHTJdhPgFDAWSAX+AO6UUh5v\nrMGdNtADnDwJU6bA6dPIJUv5esBEXv8xjpIKPfNGdOfhkd2wML38wWaZvozlR5bzn2P/QYeOyd0m\nM9BjIMN9hmNnfvkbRIeQehA+vx1KsrTfTa0g4kHwH6qla6qiaorSZq556EYIEQB838xAPwRYJKUc\nX/P7cwBSyn80dr1OHegB8vLgzju1Hv6995K9+H3+vjWZDdHpdHW14R9TQxnUtf4SCSmFKXxy9BN+\nTv6ZMn0ZrlauPBv5LGP9xyI64kPQkhzISQArB9j6dzjxPUiD9gA3cg4EDgOfgSprR1FaWVsE+nVo\nvfZ0tKB/TAhxKzBBSjm75rh7gUFSykcauMYcYA6An5/fgDNnzjR+Zx1ZdTX8/e/wyivQuzd8/TW/\n6Zx5ccNRzuaWcUeEL89NDKaLdf2Tk/QGPTFZMby5/01O5J6gp2NPZofOZqz/2I6XlllXZSmc3Qu7\n3oPTNfXxnbrCDY9D3ztUjR1FaSWtHejtAYOUslgIcRPwvpQyqLmBvq5O36Ova/NmuPtuLRXzo48o\nu/0u3vv1FJ/sPI2jtTkvTQpmcl+vyx7WXlBlqOL7xO9ZGbuS5MJk/O39eaDPA0zuOhmzjj6DtThT\nWwxlzxI4F6MVVYt8CPrdpTJ2FKWFtWqgr+fYZGAgEMT1OnRzqfR0uOsu+O03mDULli7lWIGe59Yf\n5UhqAX287Xl6fC9uDHJpcHim2lDNrym/8snRT4jLjcPd2p2ZITOZHjS9Y820rY+UkPAr7PoXnNkF\nCPAI1WbhuvbQqmlaObR3KxWlU2vtHr0HkCGllEKISOBrwB8t0+YUMBpIQ3sYe5eU8lhj1zO6QA9a\nvv2rr2rDOT17whdfUB3al40xafxz0ylS88oY0tWZpyf0JNzPscHTSCnZnb6b5UeXczDjIA4WDtwU\neBPhbuGM9BvZMaplXknWSTj2DZz5HbLjtcJq1s4w6kUt4HfkYSlF6cCuNevmc2AE4AJkAC8DZgBS\nymVCiEeAeYAeKAP+KqXcXfPam4D30IL+Sinl601psFEG+gt+/RXuvRdycuDtt+HRR6moNvD5vhSW\nbE0gp6SS8SHuPDW+50U17+tzOPMwK2NXsu/cPsr0ZbhbuzOn7xymdp/a8Yd1LkiPhl+e1wK/S0/o\nNhL8BkPQOFVrR1GaQU2Y6miysuDBB+G772DiRFi1CtzcKK7Qs3LXaT7ekURppZ5bB/iwcEwPvByu\nnKaoN+j54/wffBj9IdFZ0XjbenNrj1uZ1HUSHjYebXNP10JKiNsIe/4N549qZRdMrbR1bwfP14Z3\nFEW5IhXoOyIp4cMP4YknwMEBVq6Em24CILekkn9vS2DNnjMg4L7B/jw8sjuONlfOWJFSsittF58c\n/YRDmYcQCAZ7DmZK9ymM9huNlWknyGs3VEPKHoj5Qqu1U10B3cdCyFTwidBm4Vo1PLSlKNcrFeg7\nsqNHtQe1sbHwwAPw7rvQpQsAafllvLf5FOsOpWJjbsqcYV154IZAbCwaX/c1pTCFjYkb+S7xO9JL\n0rExs2F8wHju6HkHwc7BrX1XLaMkW1sG8cB/oCj9z+2Bw2HYU1opZUVRABXoO76KCli0SBuz9/bW\nevdjxtTujs8o4p1fTrLpeAYutubMGdaVewb7Y23eeMA3SAMHMw7ybcK3bDqziTJ9GZEekYwPGM+E\nwAnYm9u34o21ECkhIxYy4yA3SQv8xee1vPzQGeDWC7r4NH4eRTFiKtB3Fnv3aumXJ0/C/Pnw1ltg\na1u7+1BKHu9uOsWuhGycbMyZfWMg9w0JwLYJPXyAwspCvjr5Fd/Ef0NKUQq2ZrZM6TYFHzsfbvC+\ngcAuga10Yy2sqhx2/hN2vQsGvbYt4EYIngzdRoNL9/Ztn6K0AxXoO5OyMnjhBXjvPQgM1B7U3njx\nEMXBM3ks2RrP9pNZOFib8WBUIDOjArC3bFqmjZSS47nHWXF0BdvObkNv0KMTOsb5j2OQ5yCG+wzv\n2EXVLijJ1lI0k3dBzOfaClkAQeOhx3hwCQL/KJWyqVwXVKDvjHbs0Hr3ycmwcCG89hrYXJxuGHM2\nnyVb49kSl4mdpSn3DPbn/qEBl9XAvxIpJZmlmaw5voYNiRsoqCjAwsSCGT1mMNZ/LH1d+2Kqa9on\nhnaXd0Zb+HzfMijN0bZ18YPQW7V0Te/+YNrB5xkoylVSgb6zKi6Gp5+Gjz4CPz9YuhQmT77ssNi0\nAv69LYGfj53HTKdjarg3Dw3rSnc323pO2jApJQn5Caw6toofkn6gWlbjZu3GbT1uY3qP6bhYubTU\nnbUuQzUUZ8DZfXBwFZzeCbIaTC3BewAE3KClbarZuIoRUYG+s9u1C+bOhWPHYOpULeB7eV12WHJ2\nCZ/sSmLtgVQq9AZG9nTl7kH+jOjpiqmJrlmXLKwsZE/6HtbHr2d3+m5MdaYEOQRhaWrJTYE3cXP3\nmztHuiZAWT4k74SUvVrqZvphsPfWCq05+oNXf7B2au9WKso1UYHeGFRWaqmXr7wCFhbw5pswezaY\nXj6sklNcweo9Z/h8fwqZRRV42Ftye4Qv9w7xx8W2+UMXpwtO89XJr0gpSiGzNJMTuSewMbNhlO8o\nJgROYIjnkM4zExe0OvrrH/pzTB/AdxAMmgvBU8CkkwxVKUodKtAbk/h4mDMHtm+HkBD4+GMYOrTe\nQ6uqDWw9kcnn+1PYfjILc1Md0/v78EBUQL2rXTWFlJKDGQfZmLiRLSlbKKoswt7cnjH+YxjiNYQI\n9wicreqvtd+hVOuhMA3yz0DKPoj5TEvd7OKrBXtrRy2TxycSdM37NKQo7UEFemMjJWzYAI8/Dikp\n8PDDWsE0x4ZnjCZmFfPJztOsO5RKpd5AT3c7bo/w5bYI3yanZ16qqrqK3em7+Sn5J7albKNUX4qJ\nMGG4z3DGBowlyisKR8tOMovVUA2nfoa9H0HqH6Av17Y7dYMJb4JnGJhZgmWX9m2nojRABXpjVVQE\nzz+vlVJwcoI33tBm15o0nE6YVVTBdzHpfHckncMp+dhamPKXUE+m9vcmMsCpwbr4jakyVHEi5wSb\nUzazMWEjOeU5CAR9XPpwg/cNDPEaQk/Hnh2/pPIF5YVw8ifY8ba2ghaAiQX0vxf63a0FfpW2qXQg\nKtAbu5gYePRRLSUzPBzef/+y3Pv6HErJ47N9Kfx09BwlldV4O1gxrb83U8O96eravIydugzSwPGc\n4+xM28mutF0czTqKRCIQhDiHMNp/NNOCpuFk2QkegOorIHadVmjt3BGI/gwMVVq9ncBh2uzcHhPU\n8I7S7lSgvx5ICV9+qaVjnj0LM2ZoM2sDG5/tWlqpZ9OxDNYdSuX3hGwMEsL9HJjW34fJfT1xsL62\n5f/yy/M5lHmIuNw4dqft5kj2ESxMLOjn2g8vWy/6uPRhsOdg/Oz9ruk6baI4C5K2Q9I2bTGV4vPg\nGACe/bS1cbuO0MotqyUTlTamAv31pLQUFi/Wgnx1Nfz1r/Dcc2DXtIev5wvK+TY6jfWH0jiZUYSZ\niWBULzem9fdhZE83zE2vveealJ/EZyc+40TuCVIKU8iryAMgyDGIMX5jGO03mh6OPTrmYuh1Veu1\nRVRi10HWCcg7rW0XOvAbChEPag90bTvBLGOl01OB/nqUmqoF+P/9Dzw8tPH7++674vh9XVJKjp8r\nZP2hNL6NTiO7uBJHazMmh3kxrb8PYT5dWiQQSylJKUphR+oOtpzZwuHMw0gk3rbe9HPrR2+n3oS4\nhNDXpW/HT+EsSIUzu7Xia0e/hoIUbbudJ3iFa7V4gieDxdVlPCnKlahAfz3bu1crobBvn5aO+cYb\n2uzaZgRpfbWBnfHZrD+cxqZj56nQG+jqYsO0/t5MDPWk2zWM518quyybbWe3sSN1B8ezj5NZlgmA\njZkNfV364mnryTCfYQzzGYaZrgMHfkO1NkHrXLS2MHrKHshP0RZUCZ6kje13HaFy9pUWowL99U5K\nWLdOK5Z26hRERWlDO1FRzT5VYXkVPx09x7pDaew/nQtAN1cbxoV4MK63O2E+DleduVOf7LJsYrJi\n2Jm6k7jcOFKLUimsLMTRwpFhPsPo5tANXztfIj0jO3bJZSnh7H448gXErofyfC2Lx8FPK8AWdid4\nXLYks6I0mQr0iqaqSqt1/8orcO6c1rN/4w3oc3UBJj2/jM3HM9h0/Dz7knLRGyRudhaM6e3OuN7u\nDOnmjIVpy6Yg6g16dqbu5Ofkn9mVtovCykIATIQJQ7yGcFPgTYQ4h+Bn79dxi7HpKyB+kxb4s05A\n4jYtk8cjVJuh69RV+wocptbNVZpMBXrlYqWlWgrmW29BYaE2dr9oEQQEXPUpC0qr2HYyk03Hz7P9\nZBalldXYWpgyqpcbE/t4MLyna5MWSmkOKSUlVSXE58ez/ex2fjz9I+dLzgNgYWJBsFMwN3W9iSiv\nKDxtPTEVph3zAW9JjvZAN3adNr5fUaBtt/OC4U+BR18ts8fauVlDbsr15ZoCvRBiJTAJyJRSXtb1\nE0LcDTwDCKAImCeljKnZl1yzrRrQN9SIS6lA30ZycrSaOUuWaBk699+vTcC6hoAPUF5Vze7EbH6J\n1Xr7eaVVWJrpGNHDjTG93RnewxVXu5YvF2yQBuLz4jmVd4qTuSfZe24vJ/NOXnSMk6VTbXbPUK+h\neNh4YG7SgVIhpYSyPEg/BFtegfNH/txn6aCN6weNg+5jwM69vVqpdEDXGuiHAcXA6gYC/VAgTkqZ\nJ4SYCCySUg6q2ZcMDJRSZjenwSrQt7HUVC3gL18OBkOLBXzQHuTuT87l59jz/HLsPBmFFQD09enC\niJ5ujOzpSl8fB0xacFy/rpO5J4nLjeN8yXmqZTWZpZnEZMaQWKAVNDMRJgz0GMhgz8EE2gfiZ++H\nv71/xwj+BkNN2mayVpPnfCwkbNFy94UJhN0BPW8CC1ttyMesk1QTVVrFNQ/dCCECgO/rC/SXHOcI\nxEopvWt+T0YF+s7j0oD/4IPw4ovg0zLrsRoMWsrm9pOZbDuZxeGUPAwSHK3NGN7DlZG93BgW5Iqj\nTesGWSklp/JOEZcbR1J+EtvObiO5MLl2v4OFA3cF30Vfl764Wrvib++PhUkHWbBESjh/VJuhe2Al\nVGtvnJjbajN0gyeBvQ84d1Oll68zbRnonwR6SSln1/x+GsgDJPB/UsqPr/DaOcAcAD8/vwFnzpxp\ntF1KK0lN1R7SfvKJNrV/7lx48skWC/gX5JdWsiM+m+0nMtl+Kovckkp0Avr5OtT09t0I8bJv0Sye\nhhRVFpFSlMKZgjP8ePpHfkv9rXafTujwtvXG394fd2t3bvS5kRE+IzBp71o3JTlQcBZKsiDuO+2r\nTMuEwswGhsyHXn8Bx0AwMQfzTlJnSLkqbRLohRAjgQ+BG6SUOTXbvKWUaUIIN2AzsEBKuaOx66ke\nfQeRnKwtYfjf/2oPAe+6C5566qqzdK7EYJAcSSuo7e0fSc1H1vT2IwKciAx0IiLAiRAv+2YvonI1\nzhWf43zpec4Vn+N04WlOF5wmpTCF9JJ0CioKcLRwxMPGAzMTM6xMrOjv3p8w1zB6OPZov/V2q/Va\n3n5pjraG7rFvLt7vHwV9pmsPd736QUefgKY0S6sHeiFEX+AbYKKU8lQDxywCiqWUixu7ngr0HUxy\nMvzrX1oPv7QUJk7UauoMH95qWSDZxRXsOJXFnsQc9ifncianFABrcxMG+DsyKNCJ4T3arsd/gd6g\nZ9vZbexM3Ul2WTZ6g56CygLicuKQaP8vBTkGMdxnOAPdB1JUWYSduR393fu3/YpceWcg7SAUpmt5\n+7HrtJr7ADau2ixdvyFajR7HQJXR08m1aqAXQvgBW4H7pJS762y3AXRSyqKanzcDr0opf27seirQ\nd1A5Odr6tR98AFlZEBGhlVm4+eZWr96YUVjO/tO5/JGcy/7TuZw4XwSAi60Fw3q4MLyHK0O6OeNm\n1/SF0VtSQUUBp/JOEZsdy860nRzKOES1rK7dbypM8bf3p6tDV4IcghjuO5xgp2AqqiuwNG2jNkup\nPdg9F61N2kr4FapKtH12ntD/Pq1KZ2E6RC0Ez75t0y6lRVxr1s3nwAjABcgAXgbMAKSUy4QQnwDT\ngQuD6nop5UAhRFe0Xj6AKfCZlPL1pjRYBfoOrqwMVq+Gd96BxERtKOeFF+DWW+td2rA1ZBVpPf7f\nTmWxIz6L/NIqAPycrBno78iAAEcG+jsR5Gbbpj3+CworCzmRcwJHS0eySrM4kHGAhPwEEvMTSS1O\nxSANmApT9FJPD8cejPMfx2i/0dia26ITOtys3Vq/kYZqLW8/7YA2vp+wBXRmYGYNlUXahC33PuAe\nAoHDoYt367dJuWpqwpTSOvR6+OoreP11OH5cW7D8oYe02vhObZfxUW2QHE0r4I/TuRw4k8vBM3lk\nF1cCYGdpSn8/x9rg38/XocUnbjVXQUUBm89sJrUoFQsTC3an7yY6K/qiY9yt3fG188XTxpMhXkMI\ncQ7By9ardXv/hela9o6shh2LtQXVM09omT3CBHrfrD3cDRyuKnJ2QCrQK63LYIDvv4dly+Cnn8DW\nFmbNgnnzoHfvNm+OlJIzOaUcPJPHgTN5HDyTy6mMYgBMdII+XvYM6eZCZKAjfby64GbfPsM9dWWU\nZLArbRcA5dXlHMk6QkZpBqcLTpNbnlt7nJu1G/1c+zHCdwRdu3TFy9YLBwuH1pvxW62H7FMQ/Skc\n/p821g/gHgo9xmkrbZlaaTV7nLqqOvztSAV6pe0cPaqVVli7Fior4bbb4IkntPH8dnzYV1BaxaGU\nPA6c0cb4D6fkozdo//Zd7SwI8+lCuJ8jA/wdCfNxwMq8YywTaJAGTuaeJKkgibNFZ0kpTGHPuT1k\nl/05NcXa1BovWy98bH3wsfMhzC0MLxsvyvRlhDiHYGveQtVFDdWQHq0tupK4VavOWec5BMIEnAJh\nwCyInAOmHWTuwXVCBXql7WVnw3vvaV8lJdCrF8ycqa1p69YG48+NKKnQcyy9kNi0AmLTCohOzScp\nS3swaaoT9Payp7+fI+F+DoR4dSHQxabVZu82V7WhmsSCRFKLUkkvTietOK3262zRWcr0ZbXHmunM\nCHYOxt7cnhDnEAZ5DiLMNaxlZv6W5Wk1+KvKIPc05MRrwT95pzbWb26trbblGwk+EdBtpFpcvRWp\nQK+0n4ICrXe/ahX8/jvY2MCCBfDII+DdsR7u5ZVUcvhsHgfPaF8xZwsoq9J6rFZmJgR72tHby54Q\nry6EeNnTw90OS7OO0fO/QG/QE5cTR15FHibChF1pu4jPiyevIo+E/AQM0oC5zrx2rN/GzAYbMxuc\nLZ3p69qXAe4D6ObQjZyyHNys3a5uTkDiNm25xcpibRZverQ2zm9qqc3e9RsCtm5a0A+4UQ33tBAV\n6JWOIS4OXn1VW9tWp4Np02D+fBg2rEMurl1VbSA+o5hj6QUcSy/keHohx88VUlyhB7Sefx/vLgzu\n6kyIlz3BnnYEONu0yYSuq1FYWciB8wc4lHGISkMlUkpK9aWUVJVwruQcJ3NPXpQSChDiHMLkbpMZ\n6D4QO3M7bMxssDO3QyeacY/6Sq1I25Gv4ORPUJT+5z5rZ20Cl7WTVrenx3i1AtdVUoFe6ViSkrR8\n/BUrIC8P3N3h9tu1h7e9erV3667IYJCk5JZqwz7pBew/nUvM2T/H+y1MdfT0sCPEy57envb09rKn\nl4c9NhYdtDZ+HSVVJRzJOkJKYQouVi6cLjzN5jObOZ5z/KLjrE2t6e7QHVtzW3ztfBnmMwxfO18c\nLBywN7dvvDRE4TkoL9By+o98qZVxyD/7Z7E2zzDwH6rN5PUbrGr2NJEK9ErHVFoK334L69dr36uq\nYNQorZc/ZQqYdY4p+hX6ahIzSzhxvpC4c1qv/1h6YW1uvxDg72RNdzc7ennY0c/XgX5+DrjYdo6H\nlafyTpFckExJVQlFlUWkFqeSVJBEaVUpCfkJFz0T0AkdXbt0xd3anYzSDFytXAlyDKr98rXzxUSY\nYG1qfXGmkMGgLbeYtE1bdzf1wJ8F2yy7aIXaHHzBewD0nKgt0qJcRAV6pePLzNR6+MuWQUqKlpO/\nYAHMmdOmOfktRUrJ+cJybbgnvZC484UkZBaTmFVCdU3v37OLJUHudgS52dLD3ZbubnYEudtib9k5\n3uAAKqoriM6MJqssi4KKArLLsmufEbhZu5FRkkFSQRIVF4J2DXdrdwZ5DsLb1ht3a3fcrN1ws3bD\nw8YDe3N7hL5CG+5J/UPr7Rekap8Ask5oJ+h3tzart7xQy+3vMx2sHNr+D9CBqECvdB7V1Vou/gcf\nwObN2kzbYcO0Hv7UqeDn194tvCZlldXEphcQnZLP8XOFxGcWkZBZTHmVofYYD3tLgtxtCXKzo6eH\nLSFeXejhboe5accc+29MtaGalKIU4vPiOVdyjmpZTWx2LIczD5NTllNbI+gCCxOL2sDvaeNJL6de\n9HHpQ0/HnhQVnsVs38e4HFil1euxsIOcBG02b9cRIA3g2hO6j9WGfa6jwm0q0CudU0wMfPEFbNyo\nzbwVAsaP12bfTp7caYZ2GlNtkKTllRGfWcSpjOLa4B+fUVyb9WNmIujpYUcfry54OVjhYG1GD3ct\nC6gzfQK4VJWhiuzSbDJKM8gozSCzNJPM0kzt95IM0kvSa5eHrKuPUzAWptZUy2rspMSvMIugwiy6\nY4Z1bjLm1Xq8dFaYdR2uBfzqSnDwh26jjHbMXwV6pfNLSID//U+roJmWpj3AfeABmD0bunZt79a1\niksf/F7I+c+rGfu/wNfJiiA3O7q72dLd1ZZubjZ0d7Wji3XnfQOoK6csh2M5xziVdwoHCwdyynLY\nnb4bndBhojOhsKKQ5MLki54VAJgiGFhpILykkH1WFthXGxhWVkGAWyiern1wN++CmUco+N/w/9s7\n9+C46uuOf35aeaXVrqy3ZfkhS7KNDcQQyx5MCIZgQjCGhjSPApOZEJIM7UySaabT6SRN2zCddkr6\nHJJmmrgNaWhDHoRCSWgAE/NKMAFbNrYxGL9kW2tZ77dWWj1O/zh3vWuj9XO1u1qdz8yde/d37/oe\n//bqe+89v/M7B4IVhCSJ8gAAEttJREFUGfrfpQ4TeiN3GB+HZ56BzZvh6ad1EO/GG+GWW+Cee3JW\n9BMZn5ikayjKvlbP/9+q/v/DnUNEx+MuoMpQAcvmBVk2L8TSqhANVSEaKoMsKA1kzeSvVDEpk4QH\nwhzqO0R0IkpkPMKh3kM8d/RZwoMnuLxsBb2RTlpHuk59J1+EpdExKiaFkaIyovkF5Pv8VMwJURFa\nSMOCtdzScHt6EsylABN6IzdpaYEf/AB+/nPYvVtdO3fcAR/7mPrzy8oybWFamZgUWnqGOdg+GF86\ndD0wMn7qOL8vj9qKIuorg9RXBplXXEBVcQFLq0IsmxfKuklgl8KkTDIQHaCkoAQR4fjAccKDYVqH\nWjnWe4R3Tm5nYCBMwXAP/vERxoBuXx5dPh89Pu2H0rwCqgJVVM1dRFVRNVWBSqq8yWTB/CA9oz3U\nFtdyZeWVFza/IMWY0Bu5TzgM3/mOpk8Oh3UG7n33wac/DevWzeqiGiJCx8AoRzqHaO4a4nDnEM2d\nQzR3DnOk6/S3gDynqZ6XVxdTXxlkYWmAhaUB6iqD1FUUZe1ksJQwORmP5Gnby+GDz/DCid9wItJB\nh89HR34+Hb48On0+Jqa4nor9xSwKLWJBaAFlhWX0jfYRyA+wMLSQ6ESUpaVLua3+NqITUQaiA5QV\nlqW0CL0JvTF7EIGmJnjoIZ2BG41qpM6nPqUJ1jKcXC3bEBEGRsc52TfCgbZB3m0bODUofLx7mNGE\nm4Dfl8fSeSHqK4tYUBKgpjTAgpJCFpQGqC0vorRozvRl0cwkQ50a439CU0lPjo/S0/YmHeHXGUYo\nnZzg7WApTTUrCOfnc1Ki9EwMU1JQylB0iPZIOz7nY0ImKC8sp2+079QM5OqiauYH5xOcE2RF+QrW\nVq/l+oXXX9SbgQm9MTvp7dWInZ/9DJ57TidkLVkSF/21a030z4KI0DkYpaVnmMMdQ7zbNsD+tgGO\ndQ9zojdyWkgoaO7/JRVFLCkPUltRxJLyImrLi1hcXkRNSWHuvQ0MtGnRlokovPUk7P8/3QYIzdcI\nn8ISxoc68TV8iBeLi/ll+GXqSuqoLqqma6SL4/3HaY+0MxAdODXYvPVTWy/qhmlCbxi9vTr7Nib6\n4+NQVxcX/TVrTPQvABGhd3iME30Rwj0RjnUPc6x7mKNdum7pGWZsIq4tvjxHWZGfypCf+sogDVVB\n6iqCVM8tpHpuIfNLCikJzPAoockJ6D2qM3sPPq/J3SbGwB+EoXY9prhGZ/UW12iK58lJzeW/9j4i\ngRLCA2GWlS27qNOb0BtGIj09cdHfskVFf/FiLXr++c/DNddk2sIZz8SkcKI3wvGeYVq6dd05GKWt\nf4QjnUMc6x4+NUM4xoKSQpbOC7GwNMCCU0shC0oCzC8pnHmDxInaGt6hKZxP7oGTu2GoQ1M55/mg\nPwwuTwu0V14G9zx6UaczoTeMZHR3w5NPaoWsLVtgcBBWrYL162HTJvjwh6FgZuSkmUlExycJ90Zo\n7x+hfWCUlp4Ib7f2c7RriHDvCJ2Do+/5TkXQz/ySQuYVF1AW9LNsXoiGyhAlgTksKC2kpiQwM2cP\ndx+GnT/SfP4yCXf990X9Myb0hnE+DAxouOYvfgGvvaaiX1wMt98On/gEbNyoZRKNaWdkbIKTfSOc\n6I1wom+Ek326bu2N0DE4StdglNa+kfd8LzDHR01pIfUVQZZUBKmvLGJhWYCgP5/quYUsKgvk3liB\nxyULvXPuYeAOoF1E3jfFfgc8BGwChoHPikiTt+9e4C+8Q/9GRH54rvOZ0BsZJxqFrVvh8cf1ib+z\nEwoLNQXDxz+u8fozMNlaLtEXGeN49zD9kTFaeiOc7BuhLzJGuCdCc9cQR7uGT6WQiJGf56itKKIi\n6Kcg30dNSSGLy4tYXK6RQzUlASpCum+mkQqhvwEYBB5JIvSbgC+jQr8OeEhE1jnnyoHtwFpAgB3A\nGhHpOdv5TOiNrGJ8XKtjPf64plQOh3XgdsUKde985jNw1VU2mJtliAjtA6OEeyNEohOc6I1wpHOI\nwx1D9EXGGB7Tto6B97qJigvzqQoVUFlccCp6qDzkp7zIT1nQT7m3lAbmZM0bQkpcN865OuCXSYT+\ne8CLIvJj7/N+4EOxRUT+cKrjkmFCb2Qtk5OwfTs8+yxs2xYfzJ03TxOt3XsvXHcd+GbeE+FsJRKd\nINw7zPHuCCf7R+gcGKVzcJTOoSgd/aMc6Rqa8mYQoyQwh+q5BTq5rCzAwtIiFpQWUhUqoDzkpyJY\nQFnR9N8Qzib0qSp7sxA4nvC5xWtL1m4YM5O8PI3KiUXmdHRoBM/WrZpp8/vf19QLH/mI+vRvvRVq\najJrs3FWAn4fy+YVs2xe8hKG0fFJeoajdA/Fl57hKF2Dut3qjSc0HeulLzL2nu87B6WBOVSECqgI\n+qnwbgAVIT8VoQIqg37NRVQVZM403BCypr6Zc+5+4H6A2hmec9yYRVRVaQbNL3xBB2+fflqTrj3z\njM7MBbj6ag3d3LgRPvAB8Fsx7JmGPz/vVMz/uRgcHae1N0LXkN4IuoZGT18PRtl/coCuoa5TVchi\nlBXNoekvb0n5DONUCX0YWJzweZHXFkbdN4ntL071D4jIZmAzqOsmRXYZRvoIhbT27V13aQz17t0q\n+L/6FfzjP8KDD2oUz4YNGra5YQNcfrn59nOMUEG+Vg47j2PHJvRNob1/lIPtg/RFxqYljUSqfPS3\nA18iPhj7LRG5xhuM3QE0eoc2oYOx3Wc7l/nojZyjv1/dO7Gn/aNHtb26Gm66SdMs33knVMz8vOhG\nZkhF1M2P0SfzSqAN+AYwB0BEvuuFV/4rsBENr7xPRLZ73/0c8OfeP/W3IvKDc53PhN7IeY4cUeF/\n4QVdt7bqAO6GDRq3f9llcO21sy7VsnHx2IQpw8hmRGDnTs2r/9hjWk0L4vVy169X//4sT7dsnB0T\nesOYKYhAWxu8807czbN7t7bX1sInP6mRPGvX2oQt4zRM6A1jJtPTo2kZHntM4/fHvEiNZcs0zPPa\na3Wmbn19Zu00MooJvWHkCv398MYburz+uq5bWnTf8uXQ2KihnLffDpWVmbXVSCsm9IaRyxw+DE88\nAa++qsnYTpzQ9iuuiPv4b7gBFi3KrJ3GtGJCbxizBRFN0fD88/DKK/Cb32hWTlDXTkz016/XNwAb\n3M0ZTOgNY7YyMQFvvqmi//LLunR26r7q6tOf+Fet0hQPxozEhN4wDEUE9u9XwY+J/7Fjuq+0FD74\nQU3TUFcH73+/un/sqX9GYEJvGEZyjh6Ni/4rr2hoZ4z58+Hmm3X27o03wtKlJvxZigm9YRjnz+Cg\nPuVv2wa//rUu7V5x65oajexpaIDrr9ebgKVtyApM6A3DuHhE9Cn/5ZfhpZfg7bd19u7goD7dr16t\n8fyrV6u7Z9UqCAQybfWsw4TeMIzUMj6u0T1btmiunp07oa9P9+XlwcqVKvw336wJ2xYuNJfPNGNC\nbxjG9CICzc2wa5eK/q5deiNobdX9gYCmbbjpJl2uukoTtpn4pwwTesMw0o+ICv62bXDggMb0NzVp\nOUbQKJ9167T04nXXqftn7tzM2jyDSUcpQcMwjNOJ+e9Xr4639faq4B84oH7/bdvggQf0puCczt6d\nP1+f+BsbYc0a3Taf/yVhQm8YRvooLdUEbIn09Wnenldf1Tz9LS3w5JNafxc0T//ll8eFv7FRB31D\nofTbP0Mx141hGNmHCBw/rq6epibYsUOXtjbd7xysWKERPvX16vq56aZZ7foxH71hGLlBa2tc+Jua\nYN8+nfAVjar4r1ypk7rq6lT816+fNcncTOgNw8hdolH47W81xn/nTp3sFYvzBy3YsnKllmdcvlzX\nV16pN4AcivqxwVjDMHIXvz8ethljfDyezO2NN+Ddd+GRRzSff4yqqri/f9UqeN/79IZQUJD+/8M0\nY0JvGEbukZ+vA7dr1sTbRDSVw/79sGdP3O+/dWu8apfPp2J/9dW6xKKGZngRFxN6wzBmB85pauZY\neuYY0aiGe+7ZA3v3xt8EHn00fkxZmUb5LF+uwt/YqOvLLtObQ5ZzXkLvnNsIPAT4gP8QkQfP2P8v\nQOy9qQiYJyKl3r4JYI+375iIfDQVhhuGYaQEv1999ldeeXp7V1d8pm9zsxZw2bcPvv1tvTkAFBXF\nY/5jT//Ll0NxcVb5/885GOuc8wHvArcALcAbwD0isi/J8V8GVovI57zPgyJyQQGvNhhrGEbWMjam\nid127tSlqUlvCLFKXqA3gJoaDf284grYsEGjgCorp+0GcElRN865DwAPiMit3uevAYjI3yU5/lXg\nGyKyxftsQm8YRm4zOam1e2NP/62tuhw+rO6g4WE9LhhU8a+r03Vsu6FBJ4X5/RdtwqVG3SwEjid8\nbgHWJTnREqAe2JrQXOic2w6MAw+KyJNJvns/cD9AbW3teZhlGIaRJeTlwbJlupxJLPxz926d+dvc\nrOuXXjr9LcDv19w/L76Y8pKOqR6MvRv4uYhMJLQtEZGwc64B2Oqc2yMih878oohsBjaDPtGn2C7D\nMIzMMFX4J2gUUE+Piv7BgxoB1Ns7LXV7z0fow8DihM+LvLapuBv4YmKDiIS99WHn3IvAauA9Qm8Y\nhjGrcA7Ky3VZswbuumvaTnU+t443gOXOuXrnnB8V86fOPMg5txIoA7YltJU55wq87Urgg8CUg7iG\nYRjG9HDOJ3oRGXfOfQl4Fg2vfFhE3nLO/TWwXURion838BM5fXT3cuB7zrlJ9KbyYLJoHcMwDGN6\nsFw3hmEYOcDZom5S7/U3DMMwsgoTesMwjBzHhN4wDCPHMaE3DMPIcUzoDcMwcpysjLpxznUARy/y\n65VAZwrNSRVm14WTrbaZXReG2XXhXIxtS0SkaqodWSn0l4JzbnuyEKNMYnZdONlqm9l1YZhdF06q\nbTPXjWEYRo5jQm8YhpHj5KLQb860AUkwuy6cbLXN7LowzK4LJ6W25ZyP3jAMwzidXHyiNwzDMBLI\nGaF3zm10zu13zh10zn01g3Ysds694Jzb55x7yzn3x177A865sHNul7dsypB9zc65PZ4N2722cufc\nFufcAW9dlmabViT0yy7nXL9z7iuZ6DPn3MPOuXbn3N6Etin7xynf8q653c65xgzY9g/OuXe88z/h\nnCv12uucc5GEvvtumu1K+ts5577m9dl+59ytabbrpwk2NTvndnnt6eyvZBoxfdeZiMz4BU2ffAho\nAPzAm8AVGbKlBmj0tovRwupXAA8Af5oFfdUMVJ7R9vfAV73trwLfzPBveRJYkok+A24AGoG95+of\nYBPwK8AB1wK/y4BtHwHyve1vJthWl3hcBuya8rfz/hbeBArQsqOHAF+67Dpj/z8Bf5WB/kqmEdN2\nneXKE/01wEEROSwiUeAnwJ2ZMEREWkWkydseAN5G6+5mM3cCP/S2fwh8LIO23AwcEpGLnTB3SYjI\ny0D3Gc3J+udO4BFRXgNKnXM16bRNRJ4TkXHv42toBbi0kqTPknEnWrdiVESOAAfRv9+02uWcc8Af\nAD+ejnOfjbNoxLRdZ7ki9FMVMM+4uDrn6tDSib/zmr7kvXo9nG73SAICPOec2+G0IDtAtYi0etsn\ngerMmAZoAZvEP75s6LNk/ZNt193n0Ce/GPXOuZ3OuZecc+szYM9Uv1229Nl6oE1EDiS0pb2/ztCI\nabvOckXosw7nXAh4HPiKiPQD/wYsBd4PtKKvjZngehFpBG4DvuicuyFxp+i7YkZCsZyWqvwo8JjX\nlC19dopM9s/ZcM59HRgHfuQ1tQK1IrIa+BPgUefc3DSalHW/3Rncw+kPFGnvryk04hSpvs5yRegv\npID5tOOcm4P+gD8Skf8BEJE2EZkQkUng35mm19VzIfFi7e3AE54dbbFXQW/dngnb0JtPk4i0eTZm\nRZ+RvH+y4rpzzn0WuAP4tCcQeK6RLm97B+oLvyxdNp3lt8t4nznn8oGPAz+NtaW7v6bSCKbxOssV\noT+vAubpwPP9fR94W0T+OaE90af2+8DeM7+bBtuCzrni2DY6kLcX7at7vcPuBf433bZ5nPaUlQ19\n5pGsf54CPuNFRVwL9CW8eqcF59xG4M+Aj4rIcEJ7lXPO5203AMuBw2m0K9lv9xRwt3OuwDlX79n1\nerrs8vgw8I6ItMQa0tlfyTSC6bzO0jHKnI4FHZl+F70Tfz2DdlyPvnLtBnZ5yybgv4A9XvtTQE0G\nbGtAIx7eBN6K9RNQAfwaOAA8D5RnwLYg0AWUJLSlvc/QG00rMIb6Qj+frH/QKIjveNfcHmBtBmw7\niPpvY9fad71jP+H9xruAJuD30mxX0t8O+LrXZ/uB29Jpl9f+n8AfnXFsOvsrmUZM23VmM2MNwzBy\nnFxx3RiGYRhJMKE3DMPIcUzoDcMwchwTesMwjBzHhN4wDCPHMaE3DMPIcUzoDcMwchwTesMwjBzn\n/wHFEDYjkAdxyAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az0fbx8OTO_1",
        "colab_type": "text"
      },
      "source": [
        "## Load demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_VVpm7_TRU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ehL63XmTWXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_voc_size=9000,\n",
        "                 trg_voc_size=9000,\n",
        "                 src_embedding_size=256,\n",
        "                 trg_embedding_size=256,\n",
        "                 enc_hidden_size=200,\n",
        "                 dec_hidden_size=200):\n",
        "        \n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.trg_embedding_size = trg_embedding_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "        \n",
        "        self.src_embedder = nn.Embedding(src_voc_size , src_embedding_size)\n",
        "        self.encoder = nn.LSTM(src_embedding_size ,enc_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        \n",
        "        self.trg_embedder = nn.Embedding(trg_voc_size , trg_embedding_size)\n",
        "        self.decoder = nn.LSTM(trg_embedding_size ,dec_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        self.cls = nn.Linear(dec_hidden_size , trg_voc_size)\n",
        "    \n",
        "    def forward(self,source,target,feed_previous=False):\n",
        "        batch_size = source.size()[0]\n",
        "        src_em = self.src_embedder(source)\n",
        "        trg_em = self.trg_embedder(target)\n",
        "        \n",
        "        _ , enc_state = self.encoder(src_em)\n",
        "        \n",
        "        GO = Variable(torch.zeros(batch_size,1,self.trg_embedding_size))\n",
        "        \n",
        "        if feed_previous: #test phase\n",
        "            logits_ = []\n",
        "            inputs = GO\n",
        "            h = enc_state\n",
        "            for i in range(25):\n",
        "                output , h = self.decoder(inputs,h)\n",
        "                logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "                logits_.append(logits)\n",
        "                \n",
        "                predicted = logits.max(1)[1]\n",
        "                inputs = self.trg_embedder(predicted)\n",
        "                    \n",
        "            return torch.cat(logits_,0)\n",
        "            \n",
        "        else: #train phase\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "            outputs , _ = self.decoder(dec_in,enc_state)\n",
        "            outputs = outputs.contiguous().view(-1,self.dec_hidden_size)\n",
        "            logits = self.cls(outputs)\n",
        "        \n",
        "            return logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEdgGOElTXY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "female = Seq2Seq()\n",
        "male = Seq2Seq()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi61aQKgTbXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "female.load_state_dict(torch.load('model_female_cpu.pth'))\n",
        "male.load_state_dict(torch.load('model_male_cpu.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls99sQLgTeAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "female_sche = Seq2Seq()\n",
        "female_sche.load_state_dict(torch.load('model_female_sche_samplling_cpu.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAF2XEylThl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self,idx2word,word2idx):\n",
        "        self.idx2word = idx2word\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = 25\n",
        "        self.eos_idx = 8002\n",
        "        self.EN_WHITELIST  = '0123456789abcdefghijklmnopqrstuvwxyz '             \n",
        "            \n",
        "    '''\n",
        "    idx -> word with EOS\n",
        "    '''        \n",
        "    def decode_line(self,sentence_idx,remove_pad=True,remove_eos=True):  #sentence_idx: 1d_matrix     \n",
        "        sentence = []\n",
        "        for w in sentence_idx:\n",
        "            if remove_eos and w==self.eos_idx:\n",
        "                continue\n",
        "            if remove_pad and w==0 : \n",
        "                continue\n",
        "            sentence.append(self.idx2word[w])\n",
        "            #if w==self.eos_idx:\n",
        "            #    break\n",
        "        sentence = ' '.join(sentence)\n",
        "        return sentence\n",
        "    \n",
        "    def decode(self,sentence_idxs,remove_pad=True,remove_eos=True): #sentence_idxs: 2d_matrix \n",
        "        sentences = []\n",
        "        for s in sentence_idxs: \n",
        "            sentences.append(self.decode_line(s,\n",
        "                                              remove_pad=remove_pad,\n",
        "                                              remove_eos=remove_eos))\n",
        "        return sentences\n",
        "            \n",
        "    '''\n",
        "    word -> idx with EOS\n",
        "    '''\n",
        "    def encode_line(self,sentence):  #sentence: 1d_matrix\n",
        "        sentence = sentence.lower()\n",
        "        s_list = ''.join([ ch for ch in sentence if ch in self.EN_WHITELIST ]).split()\n",
        "        sentence_idx = []\n",
        "        for w in s_list:\n",
        "            sentence_idx.append(self.word2idx[w])\n",
        "        n = len(sentence_idx)\n",
        "        if  n > self.max_len:\n",
        "            sentence_idx = sentence_idx[:self.max_len] \n",
        "        elif n < self.max_len:\n",
        "            sentence_idx = sentence_idx + [self.eos_idx] + [0]*(self.max_len-n-1)  \n",
        "        return sentence_idx\n",
        "    \n",
        "    def encode(self,sentences): #sentences: 2d_matrix   \n",
        "        sentence_idxs = []\n",
        "        for s in sentences: \n",
        "            sentence_idxs.append(self.encode_line(s))\n",
        "        return np.array(sentence_idxs)\n",
        "    \n",
        "    def print_QA(self, ques , pred_ans, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs,remove_eos=True,remove_pad=True)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[2])\n",
        "            print('pred A :'+sents[1]) \n",
        "            \n",
        "    def print_QA_1(self, ques , pred_ans_train, pred_ans_test, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans_train[i], pred_ans_test[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs,remove_eos=True,remove_pad=True)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[3])\n",
        "            print('train A:'+sents[1])    \n",
        "            print('test A :'+sents[2]) \n",
        "            \n",
        "    def print_QA_2(self, ques , ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i], ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYoP4gk-Tk-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./metadata_1.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)\n",
        "        \n",
        "vocab = Vocab(metadata['idx2w'] , metadata['w2idx'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ygZrntZToRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = []\n",
        "lines.append( 'you can do it'  )\n",
        "lines.append( 'how are you'    )\n",
        "lines.append( 'fuck you'  )\n",
        "lines.append( 'jesus christ you scared the shit out of me'  )\n",
        "lines.append( 'youre terrible'  )\n",
        "lines.append( 'is something wrong' )\n",
        "lines.append( 'nobodys gonna get inside' )\n",
        "lines.append( 'im sorry'  )\n",
        "lines.append( 'shut up'  )\n",
        "N = len(lines)\n",
        "lines = vocab.encode(lines)\n",
        "q_o = Variable(torch.from_numpy(lines).long())\n",
        "#vocab.decode(vocab.encode(lines))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9nOXh2UTqh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "female.eval()\n",
        "o = female(q_o,q_o,feed_previous=True)\n",
        "_,predict_o = o.max(1)\n",
        "#vocab.decode(predict_o.cpu().view(-1,3).data.numpy().T)\n",
        "pred_ans_o = predict_o.view(-1,N).data.numpy().T #predicted answer \n",
        "vocab.print_QA_2(lines, pred_ans_o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ31lwRoTs0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "male.eval()\n",
        "o = male(q_o,q_o,feed_previous=True)\n",
        "_,predict_o = o.max(1)\n",
        "#vocab.decode(predict_o.cpu().view(-1,3).data.numpy().T)\n",
        "pred_ans_o = predict_o.view(-1,N).data.numpy().T #predicted answer \n",
        "vocab.print_QA_2(lines, pred_ans_o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E48p5mdwTuvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "female_sche.eval()\n",
        "o = female_sche(q_o,q_o,feed_previous=True)\n",
        "_,predict_o = o.max(1)\n",
        "#vocab.decode(predict_o.cpu().view(-1,3).data.numpy().T)\n",
        "pred_ans_o = predict_o.view(-1,N).data.numpy().T #predicted answer \n",
        "vocab.print_QA_2(lines, pred_ans_o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_jmToNkT3ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chat():\n",
        "    while 1:\n",
        "        #input_box\n",
        "        r = raw_input('Question:To whom{m,f,_}')\n",
        "        if ':' in r:\n",
        "            x,m= r.split(':')\n",
        "        else:\n",
        "            x,m = r,''\n",
        "        x,m = x.strip(), m.strip()\n",
        "        if x=='' :\n",
        "            break\n",
        "        \n",
        "        #decide model\n",
        "        if m == 'm':\n",
        "            model = male\n",
        "        elif m== 'f':\n",
        "            model = female\n",
        "        else:\n",
        "            model = female_sche\n",
        "        \n",
        "        #print answer\n",
        "        lines = []\n",
        "        lines.append( x )\n",
        "        N = len(lines)\n",
        "        lines = vocab.encode(lines)\n",
        "        q_o = Variable(torch.from_numpy(lines).long()) \n",
        "        o = model(q_o,q_o,feed_previous=True)\n",
        "        _,predict_o = o.max(1)\n",
        "        pred_ans_o = predict_o.view(-1,N).data.numpy().T #predicted answer \n",
        "        vocab.print_QA_2(lines, pred_ans_o)\n",
        "        print('')\n",
        "        \n",
        "    print('...end of conversation')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66ap2J3PT4V-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def two_gender_chatting(characters=['m','f']):\n",
        "\n",
        "    \n",
        "    #input_box\n",
        "    r = raw_input('Srart conversation: ')\n",
        "    x = r\n",
        "    print('')\n",
        "\n",
        "    for i in range(3):   \n",
        "        for ch in characters: \n",
        "            #decide model\n",
        "            if ch == 'm':\n",
        "                model = male\n",
        "                ch = 'm     '\n",
        "            elif ch == 'f':\n",
        "                model = female\n",
        "                ch = 'f     '\n",
        "            else:\n",
        "                model = female_sche\n",
        "                ch = 'f_sche'\n",
        "                \n",
        "            lines = vocab.encode([x])\n",
        "            q_o = Variable(torch.from_numpy(lines).long()) \n",
        "            o = model(q_o,q_o,feed_previous=True)\n",
        "            _,predict_o = o.max(1)\n",
        "            pred_ans_o = predict_o.view(-1,1).data.numpy().T #predicted answer \n",
        "            x = vocab.decode(pred_ans_o)[0]\n",
        "            print(ch+' :'+x)   \n",
        "    \n",
        "    print('\\n...end of conversation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkjY1ZZUT74K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSX4fllCT9-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "two_gender_chatting(['f_s','f'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}