{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Schedule Sampling Seq2Seq-Movie_Script Chatbot-PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFMBR3tM6Xf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive',force_remount=True)\n",
        "# !pwd\n",
        "# os.chdir('gdrive/My Drive/Colab Notebooks/')\n",
        "# !pwd\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgVrbwvgurCw",
        "colab_type": "code",
        "outputId": "0211f57d-c469-41c0-b285-86ea66a87f9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "# coding: utf-8\n",
        "# - try to implrment data.py\n",
        "corpus_path = 'corpus/'  #cornell movie-dialogs corpus\n",
        "save_path = 'datasets/'\n",
        "\n",
        "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz ' \n",
        "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\''\n",
        "\n",
        "limit = {\n",
        "        'maxq' : 25,\n",
        "        'minq' : 2,\n",
        "        'maxa' : 25,\n",
        "        'mina' : 2\n",
        "        }\n",
        "\n",
        "UNK = 'unk'\n",
        "VOCAB_SIZE = 8997\n",
        "\n",
        "# idx2w[0]='_'    ...zero padding\n",
        "# idx2w[1]='unk'\n",
        "# idx2w[2]='<G0>'\n",
        "# idx2w[3]='<EOS>'\n",
        "# total 9000\n",
        "\n",
        "import random\n",
        "import nltk\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "''' \n",
        "    1. Read from 'movie_characters_metadata.txt'\n",
        "    2. Create a dictionary with ( key = u_id, value = gender )\n",
        "'''\n",
        "def get_character2gender():\n",
        "    lines=open(corpus_path+'movie_characters_metadata.txt', encoding='utf-8', errors='ignore'\n",
        "               ).read().split('\\n') #encoding='utf-8', errors='ignore'\n",
        "    c2g = {} #Create a dictionary\n",
        "    for line in lines:\n",
        "        _line = line.split(' +++$+++ ')\n",
        "        if len(_line) == 6: #符合格式\n",
        "            c2g[ _line[0] ] = _line[4] #Update the dictionary\n",
        "            #_line[0]='u0', _line[4]='f'\n",
        "    return c2g\n",
        "\n",
        "''' \n",
        "    1. Read from 'movie-lines.txt'\n",
        "    2. Create a dictionary with ( key = line_id, value = text )\n",
        "'''\n",
        "def get_id2line():\n",
        "    lines=open(corpus_path+'movie_lines.txt', encoding='utf-8', errors='ignore'\n",
        "               ).read().split('\\n') #encoding='utf-8', errors='ignore'\n",
        "    id2line = {} #Create a dictionary\n",
        "    for line in lines:\n",
        "        _line = line.split(' +++$+++ ')\n",
        "        if len(_line) == 5: \n",
        "            id2line[ _line[0] ] = _line[4] #Update the dictionary\n",
        "            #_line[0]='L1045', _line[4]='They do not!'\n",
        "    return id2line\n",
        "\n",
        "''' \n",
        "    1. Read from 'movie-lines.txt'\n",
        "    2. Create a dictionary with ( key = line_id, value = gender )\n",
        "'''\n",
        "def get_id2gender(u2gender):\n",
        "    lines=open(corpus_path+'movie_lines.txt', encoding='utf-8', errors='ignore'\n",
        "               ).read().split('\\n') #encoding='utf-8', errors='ignore'\n",
        "    id2gender = {} #Create a dictionary\n",
        "    for line in lines:\n",
        "        _line = line.split(' +++$+++ ')\n",
        "        if len(_line) == 5:\n",
        "            id2gender[ _line[0] ] = u2gender [ _line[1] ] #Update the dictionary\n",
        "            #_line[0]='L1045', _line[4]='They do not!'\n",
        "    return id2gender\n",
        "\n",
        "'''\n",
        "    1. Read from 'movie_conversations.txt'\n",
        "    2. Create a list of [list of line_id's]\n",
        "'''\n",
        "def get_conversations():\n",
        "    conv_lines = open(corpus_path+'movie_conversations.txt', encoding='utf-8', errors='ignore'\n",
        "                      ).read().split('\\n') #encoding='utf-8', errors='ignore'\n",
        "    convs = [ ] #Create a list\n",
        "    for line in conv_lines[:-1]:\n",
        "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "        #_line[-1]= \"'L194', 'L195', 'L196', 'L197'\"\n",
        "        #[1:-1]: remove '[',']' \n",
        "        #_line = 'L194,L195,L196,L197'\n",
        "        #_line.split(',')=['L194', 'L195', 'L196', 'L197']\n",
        "        convs.append(_line.split(','))\n",
        "    return convs\n",
        "\n",
        "'''\n",
        "花時間!!暫時不需要!!!\n",
        "    1. Get each conversation\n",
        "    2. Get each line from conversation\n",
        "    3. Save each conversation to file\n",
        "'''\n",
        "def extract_conversations(convs,id2line,path=''):\n",
        "    idx = 0\n",
        "    for conv in convs:\n",
        "        f_conv = open(path + str(idx)+'.txt', 'w')#create file for each conv\n",
        "        for line_id in conv:\n",
        "            f_conv.write(id2line[line_id])\n",
        "            f_conv.write('\\n')\n",
        "        f_conv.close()\n",
        "        idx += 1\n",
        "\n",
        "'''\n",
        "    Get lists of all conversations as Questions and Answers\n",
        "    1. [questions]\n",
        "    2. [answers]\n",
        "'''\n",
        "def gather_dataset(convs, id2line):\n",
        "    questions = []; answers = []\n",
        "\n",
        "    for conv in convs:\n",
        "        '''\n",
        "        #把['L194', 'L195', 'L196', 'L197']分成\n",
        "        #['L194', 'L195']和[ 'L196', 'L197']\n",
        "        #共13萬筆\n",
        "        if len(conv) %2 != 0:\n",
        "            conv = conv[:-1] #只取到偶數個\n",
        "        for i in range(len(conv)):\n",
        "            if i%2 == 0:\n",
        "                questions.append(id2line[conv[i]])\n",
        "            else:\n",
        "                answers.append(id2line[conv[i]])\n",
        "        '''\n",
        "        for i in range(len(conv)-1):\n",
        "            questions.append(id2line[conv[i]])\n",
        "            answers.append(id2line[conv[i+1]])  \n",
        "            #把['L194', 'L195', 'L196', 'L197']分成\n",
        "            #['L194', 'L195'],['L195', 'L196']和[ 'L196', 'L197']\n",
        "            #共22萬筆\n",
        "\n",
        "    return questions, answers\n",
        "\n",
        "'''\n",
        "    Get lists of Pratial conversations as Questions and Answers By Gender\n",
        "    1. [questions]\n",
        "    2. [answers]\n",
        "'''\n",
        "def gather_dataset_by_gender(convs, id2line, id2gender,\n",
        "                             que_gender='?', ans_gender='?' ): \n",
        "    #if ans_gender = 'f', just female answer\n",
        "    #if ans_gender = 'm', just male answer\n",
        "    #if ans_gender = '?', both male/female answer, including gender unknown\n",
        "    \n",
        "    questions = []; answers = []\n",
        "\n",
        "    for conv in convs:\n",
        "        for i in range(len(conv)-1):\n",
        "            # check gender\n",
        "            bool_1 = que_gender == id2gender[conv[i]] \n",
        "            bool_2 = ans_gender == id2gender[conv[i+1]] \n",
        "            bool_3 = que_gender == '?' \n",
        "            bool_4 = ans_gender == '?' \n",
        "            bool_que = bool_1 or bool_3 \n",
        "            bool_ans = bool_2 or bool_4 \n",
        "            if ( bool_que and bool_ans ) :    \n",
        "                questions.append(id2line[conv[i]])\n",
        "                answers.append(id2line[conv[i+1]])  \n",
        "                #把['L194', 'L195', 'L196', 'L197']分成\n",
        "                #['L194', 'L195'],['L195', 'L196']和[ 'L196', 'L197']\n",
        "                #共22萬筆\n",
        "    return questions, answers\n",
        "\n",
        "'''\n",
        "暫時沒用到!!!\n",
        "    Shuffle and split to train/test dataset\n",
        "    We need 4 files\n",
        "        1. train.enc : Encoder input for training\n",
        "        2. train.dec : Decoder input for training\n",
        "        3. test.enc  : Encoder input for testing\n",
        "        4. test.dec  : Decoder input for testing\n",
        "    -> Encoder input is question, and Decoder input is answer \n",
        "'''\n",
        "def prepare_seq2seq_files(questions, answers, gender = 'f' ,\n",
        "                          path='',TESTSET_SIZE = 10000):\n",
        "    \n",
        "    # open files\n",
        "    train_enc = open(path + gender+'_train.enc','w')\n",
        "    train_dec = open(path + gender+'_train.dec','w')\n",
        "    test_enc  = open(path + gender+'_test.enc', 'w')\n",
        "    test_dec  = open(path + gender+'_test.dec', 'w')\n",
        "\n",
        "    # choose 10,000 (TESTSET_SIZE) items to put into testset\n",
        "    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n",
        "\n",
        "    for i in range(len(questions)):\n",
        "        if i in test_ids:\n",
        "            test_enc.write(questions[i]+'\\n')\n",
        "            test_dec.write(answers[i]+ '\\n' )\n",
        "        else:\n",
        "            train_enc.write(questions[i]+'\\n')\n",
        "            train_dec.write(answers[i]+ '\\n' )\n",
        "        if i%10000 == 0:\n",
        "            print('>> written {} lines'.format(i))\n",
        "\n",
        "    # close files\n",
        "    train_enc.close()\n",
        "    train_dec.close()\n",
        "    test_enc.close()\n",
        "    test_dec.close()\n",
        "    print('>>written finish')\n",
        "\n",
        "'''\n",
        " remove anything that isn't in the vocabulary\n",
        "    return str(pure en)\n",
        "'''\n",
        "def filter_line(line, whitelist): # whitelist:安全名單  只留下whitelist\n",
        "    return ''.join([ ch for ch in line if ch in whitelist ])\n",
        "\n",
        "#filter_line('fweijwhf8328r8uAAAgerg19,,',EN_WHITELIST)\n",
        "\n",
        "'''\n",
        " filter too long and too short sequences\n",
        "    return tuple( filtered_ta, filtered_en )\n",
        "'''\n",
        "def filter_data(qseq, aseq):\n",
        "    filtered_q, filtered_a = [], []\n",
        "    raw_data_len = len(qseq)\n",
        "\n",
        "    assert len(qseq) == len(aseq)\n",
        "\n",
        "    for i in range(raw_data_len):\n",
        "        qlen, alen = len(qseq[i].split(' ')), len(aseq[i].split(' '))\n",
        "        if qlen >= limit['minq'] and qlen <= limit['maxq']:\n",
        "            if alen >= limit['mina'] and alen <= limit['maxa']:\n",
        "                filtered_q.append(qseq[i])\n",
        "                filtered_a.append(aseq[i])\n",
        "\n",
        "    # print the fraction of the original data, filtered\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n",
        "    print(str(filtered) + '% filtered from original data')\n",
        "\n",
        "    return filtered_q, filtered_a\n",
        "\n",
        "'''\n",
        " read list of words, create index to word,\n",
        "  word to index dictionaries\n",
        "    return tuple( vocab->(word, count), idx2w, w2idx )\n",
        "'''\n",
        "def index_(tokenized_sentences, vocab_size):\n",
        "    # get frequency distribution\n",
        "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    # get vocabulary of 'vocab_size' most used words\n",
        "    vocab = freq_dist.most_common(vocab_size)\n",
        "    # index2word (as a list)\n",
        "    index2word = ['_'] + [UNK] + [ x[0] for x in vocab ]\n",
        "    # word2index (as a dict)\n",
        "    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n",
        "    return index2word, word2index, freq_dist\n",
        "\n",
        "'''\n",
        " filter based on number of unknowns (words not in vocabulary)\n",
        "  filter out the worst sentences\n",
        "'''\n",
        "def filter_unk(qtokenized, atokenized, w2idx):\n",
        "    data_len = len(qtokenized)\n",
        "\n",
        "    filtered_q, filtered_a = [], []\n",
        "\n",
        "    for qline, aline in zip(qtokenized, atokenized):\n",
        "        unk_count_q = len([ w for w in qline if w not in w2idx ])\n",
        "        unk_count_a = len([ w for w in aline if w not in w2idx ])\n",
        "        if unk_count_a <= 2:\n",
        "            if unk_count_q > 0:\n",
        "                if unk_count_q/len(qline) > 0.2:\n",
        "                    pass\n",
        "            filtered_q.append(qline)\n",
        "            filtered_a.append(aline)\n",
        "\n",
        "    # print the fraction of the original data, filtered\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered = int((data_len - filt_data_len)*100/data_len)\n",
        "    print(str(filtered) + '% filtered from original data')\n",
        "\n",
        "    return filtered_q, filtered_a\n",
        "\n",
        "'''\n",
        " create the final dataset : \n",
        "  - convert list of items to arrays of indices\n",
        "  - add zero padding\n",
        "      return ( [array_en([indices]), array_ta([indices]) )\n",
        " \n",
        "'''\n",
        "def zero_pad(qtokenized, atokenized, w2idx):\n",
        "    # num of rows\n",
        "    data_len = len(qtokenized)\n",
        "\n",
        "    # numpy arrays to store indices\n",
        "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32) \n",
        "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
        "\n",
        "    for i in range(data_len):\n",
        "        q_indices = pad_seq(qtokenized[i], w2idx, limit['maxq'])\n",
        "        a_indices = pad_seq(atokenized[i], w2idx, limit['maxa'])\n",
        "\n",
        "        #print(len(idx_q[i]), len(q_indices))\n",
        "        #print(len(idx_a[i]), len(a_indices))\n",
        "        idx_q[i] = np.array(q_indices)\n",
        "        idx_a[i] = np.array(a_indices)\n",
        "\n",
        "    return idx_q, idx_a\n",
        "\n",
        "'''\n",
        " replace words with indices in a sequence\n",
        "  replace with unknown if word not in lookup\n",
        "    return [list of indices]\n",
        "'''\n",
        "def pad_seq(seq, lookup, maxlen):\n",
        "    indices = []\n",
        "    for word in seq:\n",
        "        if word in lookup:\n",
        "            indices.append(lookup[word])\n",
        "        else:\n",
        "            indices.append(lookup[UNK])\n",
        "    return indices + [0]*(maxlen - len(seq))\n",
        "\n",
        "'''\n",
        "    main process\n",
        "    for chatbot\n",
        "'''\n",
        "def process_data(): \n",
        "    \n",
        "    u2gender = get_character2gender()\n",
        "    print('>> gathered u2gender dictionary.\\n')\n",
        "    id2line = get_id2line()\n",
        "    print('>> gathered id2line dictionary.\\n')\n",
        "    id2gender = get_id2gender(u2gender)\n",
        "    print('>> gathered id2gender dictionary.\\n')\n",
        "    convs = get_conversations() # [ ['L750', 'L751'], [...] ,... ]\n",
        "    print('>> gathered all conversations.\\n')\n",
        "    #questions, answers = gather_dataset(convs,id2line)\n",
        "    f_questions, f_answers = gather_dataset_by_gender(convs, \n",
        "                                                  id2line, id2gender,\n",
        "                                                  ans_gender='f')\n",
        "    m_questions, m_answers = gather_dataset_by_gender(convs, \n",
        "                                                  id2line, id2gender,\n",
        "                                                  ans_gender='m')\n",
        "    print('\\n Female dataset len : ' + str(len(f_questions)))\n",
        "    print('\\n Male dataset len : ' + str(len(m_questions)))\n",
        "    \n",
        "    \n",
        "    # change to lower case (just for en)\n",
        "    f_questions = [ line.lower() for line in f_questions ]\n",
        "    f_answers = [ line.lower() for line in f_answers ]\n",
        "    m_questions = [ line.lower() for line in m_questions ]\n",
        "    m_answers = [ line.lower() for line in m_answers ]\n",
        "    \n",
        "    # filter out unnecessary characters #空白不能濾掉, 否則無法分字\n",
        "    # 確保句子裡出現的所有字元, 都是在安全名單內\n",
        "    print('\\n>> Filter lines')\n",
        "    f_questions = [ filter_line(line, EN_WHITELIST) for line in f_questions ]\n",
        "    f_answers = [ filter_line(line, EN_WHITELIST) for line in f_answers ]\n",
        "    m_questions = [ filter_line(line, EN_WHITELIST) for line in m_questions ]\n",
        "    m_answers = [ filter_line(line, EN_WHITELIST) for line in m_answers ]\n",
        "    \n",
        "    # filter out too long or too short sequences\n",
        "    print('\\n>> Filter out too long or too short sequences')\n",
        "    f_qlines, f_alines = filter_data(f_questions, f_answers)\n",
        "    m_qlines, m_alines = filter_data(m_questions, m_answers)\n",
        "    print('\\n Female dataset len : ' + str(len(f_qlines)))\n",
        "    print('\\n Male dataset len : ' + str(len(m_qlines)))\n",
        "\n",
        "    print('\\n Before token : ')\n",
        "    for q,a in zip(f_qlines[141:145], f_alines[141:145]):\n",
        "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
        "\n",
        "    # tokenize: convert list of [lines of text] into list of [list of words ]\n",
        "    print('\\n>> Segment lines into words')\n",
        "    f_qtokenized = [ [w.strip() for w in wordlist.split(' ') if w] \n",
        "                  for wordlist in f_qlines ]\n",
        "    f_atokenized = [ [w.strip() for w in wordlist.split(' ') if w] \n",
        "                  for wordlist in f_alines ]\n",
        "    m_qtokenized = [ [w.strip() for w in wordlist.split(' ') if w] \n",
        "                  for wordlist in m_qlines ]\n",
        "    m_atokenized = [ [w.strip() for w in wordlist.split(' ') if w] \n",
        "                  for wordlist in m_alines ]\n",
        "    print('\\n:: Sample from segmented list of words')\n",
        "    \n",
        "    print('\\n After token : ')\n",
        "    for q,a in zip(f_qtokenized[141:145], f_atokenized[141:145]):\n",
        "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
        "\n",
        "    # indexing -> idx2w, w2idx \n",
        "    all_tokenized = f_qtokenized + f_atokenized + m_qtokenized + m_atokenized\n",
        "    print('\\n >> Index words')\n",
        "    idx2w, w2idx, freq_dist = index_( all_tokenized, \n",
        "                                     vocab_size=VOCAB_SIZE)\n",
        "    \n",
        "    # filter out sentences with too many unknown words\n",
        "    print('\\n >> Filter out sentences with too many unknown words')\n",
        "    f_qtokenized, f_atokenized = filter_unk(f_qtokenized, f_atokenized, w2idx)\n",
        "    m_qtokenized, m_atokenized = filter_unk(m_qtokenized, m_atokenized, w2idx)\n",
        "    print('\\n Final Female dataset len : ' + str(len(f_qtokenized)))\n",
        "    print('\\n Final Male dataset len : ' + str(len(m_qtokenized)))\n",
        "\n",
        "    print('\\n >> Zero Padding')\n",
        "    f_idx_q, f_idx_a = zero_pad(f_qtokenized, f_atokenized, w2idx)\n",
        "    m_idx_q, m_idx_a = zero_pad(m_qtokenized, m_atokenized, w2idx)\n",
        "    \n",
        "    print('\\n >> Save numpy arrays to disk')\n",
        "    # save them\n",
        "    np.save(save_path+'f_idx_q.npy', f_idx_q)\n",
        "    np.save(save_path+'f_idx_a.npy', f_idx_a)\n",
        "    np.save(save_path+'m_idx_q.npy', m_idx_q)\n",
        "    np.save(save_path+'m_idx_a.npy', m_idx_a)\n",
        "    \n",
        "    # let us now save the necessary dictionaries\n",
        "    metadata = {\n",
        "            'w2idx' : w2idx,\n",
        "            'idx2w' : idx2w,\n",
        "            'limit' : limit,\n",
        "            'freq_dist' : freq_dist\n",
        "                }\n",
        "\n",
        "    # write to disk : data control dictionaries\n",
        "    with open(save_path+'metadata.pkl', 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    # count of unknowns\n",
        "    unk_count = (f_idx_q == 1).sum() + (f_idx_a == 1).sum() + (m_idx_q == 1).sum() + (m_idx_a == 1).sum()\n",
        "    # count of words\n",
        "    word_count = (f_idx_q > 1).sum() + (f_idx_a > 1).sum() + (m_idx_q > 1).sum() + (m_idx_a > 1).sum()\n",
        "\n",
        "    print('% unknown : {0}'.format(100 * (unk_count/word_count)))\n",
        "    print('Female Dataset count : ' + str(f_idx_q.shape[0]))\n",
        "    print('Male Dataset count : ' + str(m_idx_q.shape[0]))\n",
        "\n",
        "    #print '>> gathered questions and answers.\\n'\n",
        "    #prepare_seq2seq_files(f_idx_q,f_idx_a,'f')\n",
        "    #prepare_seq2seq_files(m_idx_q,m_idx_a,'m')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    process_data()\n",
        "\n",
        "'''\n",
        "    load_data\n",
        "    for chatbot\n",
        "'''\n",
        "def load_data(PATH=''):\n",
        "    # read data control dictionaries\n",
        "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)\n",
        "    # read numpy arrays\n",
        "    f_idx_q = np.load(PATH + 'f_idx_q.npy')\n",
        "    f_idx_a = np.load(PATH + 'f_idx_a.npy')\n",
        "    m_idx_q = np.load(PATH + 'm_idx_q.npy')\n",
        "    m_idx_a = np.load(PATH + 'm_idx_a.npy')\n",
        "    return metadata, f_idx_q, f_idx_a, m_idx_q, m_idx_a\n",
        "\n",
        "def load_data_female(PATH=''):\n",
        "    # read numpy arrays\n",
        "    f_idx_q = np.load(PATH + 'f_idx_q.npy')\n",
        "    f_idx_a = np.load(PATH + 'f_idx_a.npy')\n",
        "    return f_idx_q, f_idx_a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> gathered u2gender dictionary.\n",
            "\n",
            ">> gathered id2line dictionary.\n",
            "\n",
            ">> gathered id2gender dictionary.\n",
            "\n",
            ">> gathered all conversations.\n",
            "\n",
            "\n",
            " Female dataset len : 48650\n",
            "\n",
            " Male dataset len : 113671\n",
            "\n",
            ">> Filter lines\n",
            "\n",
            ">> Filter out too long or too short sequences\n",
            "29% filtered from original data\n",
            "29% filtered from original data\n",
            "\n",
            " Female dataset len : 34164\n",
            "\n",
            " Male dataset len : 79827\n",
            "\n",
            " Before token : \n",
            "q : [so tell me about this dance was it fun]; a : [parts of it]\n",
            "q : [which parts]; a : [the part where bianca beat the hell out of some guy]\n",
            "q : [bianca did what]; a : [whats the matter  upset that i rubbed off on her]\n",
            "q : [katarina stratford  my my  youve been terrorizing ms blaise again]; a : [expressing my opinion is not a terrorist action]\n",
            "\n",
            ">> Segment lines into words\n",
            "\n",
            ":: Sample from segmented list of words\n",
            "\n",
            " After token : \n",
            "q : [['so', 'tell', 'me', 'about', 'this', 'dance', 'was', 'it', 'fun']]; a : [['parts', 'of', 'it']]\n",
            "q : [['which', 'parts']]; a : [['the', 'part', 'where', 'bianca', 'beat', 'the', 'hell', 'out', 'of', 'some', 'guy']]\n",
            "q : [['bianca', 'did', 'what']]; a : [['whats', 'the', 'matter', 'upset', 'that', 'i', 'rubbed', 'off', 'on', 'her']]\n",
            "q : [['katarina', 'stratford', 'my', 'my', 'youve', 'been', 'terrorizing', 'ms', 'blaise', 'again']]; a : [['expressing', 'my', 'opinion', 'is', 'not', 'a', 'terrorist', 'action']]\n",
            "\n",
            " >> Index words\n",
            "\n",
            " >> Filter out sentences with too many unknown words\n",
            "1% filtered from original data\n",
            "1% filtered from original data\n",
            "\n",
            " Final Female dataset len : 33681\n",
            "\n",
            " Final Male dataset len : 78382\n",
            "\n",
            " >> Zero Padding\n",
            "\n",
            " >> Save numpy arrays to disk\n",
            "% unknown : 3.530673927105614\n",
            "Female Dataset count : 33681\n",
            "Male Dataset count : 78382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjyEARtR6NXd",
        "colab_type": "code",
        "outputId": "93fd1707-a304-47ba-844d-176005285f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " assignment0.ipynb\n",
            "'assignment2_(release).ipynb'\n",
            " assignment2_sol.ipynb\n",
            " Bonus02_structuredComputation.ipynb\n",
            " corpus\n",
            " datasets\n",
            "'Deep Q-learning.ipynb'\n",
            " f_test.dec\n",
            " f_test.enc\n",
            " f_train.dec\n",
            " f_train.enc\n",
            " Glove_solution.ipynb\n",
            " loss_female_epo200.npy\n",
            " loss_female_epo40.npy\n",
            " loss_female_epo4.npy\n",
            " loss_female_epo5.npy\n",
            " loss_male_epo5.npy\n",
            " main.py\n",
            "'Model 4 generator'\n",
            "'Model 5 RL'\n",
            " model_female_prob_feed_epo141.pth\n",
            " pth\n",
            "'Schedule Sampling Seq2Seq-Movie_Script Chatbot-PyTorch.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrCwWIIf_O5",
        "colab_type": "text"
      },
      "source": [
        "data_add_EOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnNVH3qkgBlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7XnxpaWgTxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(PATH='datasets/'):\n",
        "    # read data control dictionaries\n",
        "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)\n",
        "    # read numpy arrays\n",
        "    f_idx_q = np.load(PATH + 'f_idx_q.npy')\n",
        "    f_idx_a = np.load(PATH + 'f_idx_a.npy')\n",
        "    m_idx_q = np.load(PATH + 'm_idx_q.npy')\n",
        "    m_idx_a = np.load(PATH + 'm_idx_a.npy')\n",
        "    return metadata, f_idx_q, f_idx_a, m_idx_q, m_idx_a\n",
        "\n",
        "metadata, f_idx_q, f_idx_a, m_idx_q, m_idx_a = load_data(PATH='datasets/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6v64hBngUze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_EOS_metadata(meta, save_path='datasets/'):\n",
        "    \n",
        "    # metadata\n",
        "    idx2w = meta['idx2w']\n",
        "    w2idx = meta['w2idx']\n",
        "    eos_idx = len(w2idx)  # eos_idx=8002\n",
        "    idx2w.append('<EOS>')\n",
        "    w2idx.update({'<EOS>': eos_idx})\n",
        "    metadata = {\n",
        "            'w2idx' : w2idx,\n",
        "            'idx2w' : idx2w,\n",
        "                }\n",
        "    with open(save_path+'metadata_1.pkl', 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "        \n",
        "    return eos_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aLnb8PzgXZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_EOS_line(idxs, eos_idx=8002):\n",
        "    new_idxs = np.copy(idxs)\n",
        "    for i,line in enumerate(idxs):\n",
        "        zreo_pos_arr = np.where(line==0)[0]\n",
        "        if len(zreo_pos_arr)>0:\n",
        "            new_idxs[i,25-len(zreo_pos_arr)]=eos_idx\n",
        "    return new_idxs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvtOk9YygZeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def idxs_add_EOS(f_idx_q, f_idx_a, m_idx_q, m_idx_a, eos_idx=8002, save_path='datasets/'):\n",
        "    \n",
        "    f_idx_q_1 = add_EOS_line(f_idx_q, eos_idx=eos_idx)\n",
        "    f_idx_a_1 = add_EOS_line(f_idx_a, eos_idx=eos_idx)\n",
        "    m_idx_q_1 = add_EOS_line(m_idx_q, eos_idx=eos_idx)\n",
        "    m_idx_a_1 = add_EOS_line(m_idx_a, eos_idx=eos_idx)\n",
        "    np.save(save_path+'f_idx_q_1.npy', f_idx_q_1)\n",
        "    np.save(save_path+'f_idx_a_1.npy', f_idx_a_1)\n",
        "    np.save(save_path+'m_idx_q_1.npy', m_idx_q_1)\n",
        "    np.save(save_path+'m_idx_a_1.npy', m_idx_a_1)\n",
        "    \n",
        "idxs_add_EOS(f_idx_q, f_idx_a, m_idx_q, m_idx_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9R6ow5kgbnN",
        "colab_type": "code",
        "outputId": "d99cd14f-ef85-4e84-e10e-57f8f9fc287b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def add_EOS(path='datasets/'):\n",
        "    metadata, f_idx_q, f_idx_a, m_idx_q, m_idx_a  = load_data()\n",
        "    eos_idx = add_EOS_metadata(metadata)\n",
        "    idxs_add_EOS(f_idx_q, f_idx_a, m_idx_q, m_idx_a, eos_idx=eos_idx)\n",
        "    \n",
        "    \n",
        "    f_idx_q_1 = np.load('datasets/f_idx_q_1.npy')\n",
        "    print(f_idx_q_1 [0])\n",
        "    print(f_idx_q [0])\n",
        "    print(f_idx_q_1 [350])\n",
        "    print(f_idx_q [350])\n",
        "    \n",
        "add_EOS()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  41    3  133  666  327   34    1   46   44  108   34    2 8999    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0]\n",
            "[ 41   3 133 666 327  34   1  46  44 108  34   2   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0]\n",
            "[   3   14   18   20    4  674  284    3  180    3   33  133    2   18\n",
            "  666 2753    6  212 1750   31 1386   35   27  309    5]\n",
            "[   3   14   18   20    4  674  284    3  180    3   33  133    2   18\n",
            "  666 2753    6  212 1750   31 1386   35   27  309    5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h1XPoun4Rlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DBsq0734Sq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = np.load(\"./datasets/f_idx_a_1.npy\")\n",
        "ques = np.load(\"./datasets/f_idx_q_1.npy\")\n",
        "\n",
        "with open('./datasets/metadata_1.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkP2P20b4Vd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FemaleDataset(data.Dataset): \n",
        "    def __init__(self,ques,ans):\n",
        "        self.ques = ques\n",
        "        self.ans = ans\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ques_tensor = torch.from_numpy(self.ques[index]).long()\n",
        "        ans_tensor = torch.from_numpy(self.ans[index]).long()\n",
        "        \n",
        "        return ques_tensor , ans_tensor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return 33589      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO7xcWBY4ciM",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8JIfvIS4Xjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "female_dataset = FemaleDataset(ques,ans)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=female_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFTwt1Q-4jUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_voc_size=9000,\n",
        "                 trg_voc_size=9000,\n",
        "                 src_embedding_size=256,\n",
        "                 trg_embedding_size=256,\n",
        "                 enc_hidden_size=200,\n",
        "                 dec_hidden_size=200):\n",
        "        \n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.trg_embedding_size = trg_embedding_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "        \n",
        "        self.src_embedder = nn.Embedding(src_voc_size , src_embedding_size)\n",
        "        self.encoder = nn.LSTM(src_embedding_size ,enc_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        \n",
        "        self.trg_embedder = nn.Embedding(trg_voc_size , trg_embedding_size)\n",
        "        self.decoder = nn.LSTM(trg_embedding_size ,dec_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        self.cls = nn.Linear(dec_hidden_size , trg_voc_size)\n",
        "    \n",
        "    def forward(self,source,target,feed_previous=False):\n",
        "        batch_size = source.size()[0]\n",
        "        src_em = self.src_embedder(source)\n",
        "        trg_em = self.trg_embedder(target)\n",
        "        \n",
        "        _ , enc_state = self.encoder(src_em)\n",
        "        \n",
        "        GO = Variable(torch.zeros(batch_size,1,self.trg_embedding_size)).cuda()\n",
        "        \n",
        "        if feed_previous: #test phase\n",
        "            logits_ = []\n",
        "            inputs = GO\n",
        "            h = enc_state\n",
        "            for i in range(25):\n",
        "                inputs = torch.cat([GO,trg_em[:,:-1,:]],1) # I modify this part\n",
        "                output , h = self.decoder(inputs,h)\n",
        "                logits = self.cls(output.reshape(-1, self.dec_hidden_size))\n",
        "#                logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "                logits_.append(logits)\n",
        "                \n",
        "                predicted = logits.max(1)[1]\n",
        "                inputs = self.trg_embedder(predicted)\n",
        "                    \n",
        "            return torch.cat(logits_,0)      \n",
        "            \n",
        "        else: #train phase\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "            outputs , _ = self.decoder(dec_in,enc_state)\n",
        "            outputs = outputs.contiguous().view(-1,self.dec_hidden_size)\n",
        "            logits = self.cls(outputs)\n",
        "        \n",
        "            return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFg9w5K64laP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0UsNiy4nX7",
        "colab_type": "code",
        "outputId": "286ea542-74b0-462f-d2ff-72b681551912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (src_embedder): Embedding(9000, 256)\n",
              "  (encoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (trg_embedder): Embedding(9000, 256)\n",
              "  (decoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (cls): Linear(in_features=200, out_features=9000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61w936zd4o7I",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yoXKR-z4qsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_op = optim.Adam(model.parameters() ,lr=3e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOdYck3t4ssh",
        "colab_type": "code",
        "outputId": "6e479e32-726f-4650-fc5d-16f487dd049b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "epochs = 200 # 200\n",
        "loss_hist = []\n",
        "loss_ = 3\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_mean_loss = []\n",
        "\n",
        "    for i , (q,a) in enumerate(train_loader):\n",
        "        q = Variable(q).cuda()\n",
        "        a = Variable(a).cuda()\n",
        "   \n",
        "        logits = model(q,a,feed_previous=False)\n",
        "        _,predict = logits.max(1)\n",
        "        \n",
        "        loss = F.cross_entropy(logits ,a.view(-1))\n",
        "        train_op.zero_grad()\n",
        "        loss.backward()\n",
        "        train_op.step()\n",
        "#        print('loss', loss.data.item())\n",
        "        epoch_mean_loss.append(loss.data.item())\n",
        "    \n",
        "    loss_ = np.mean(epoch_mean_loss)\n",
        "    loss_hist.append(loss_)\n",
        "    if epoch % 10 == 0  or epoch == epochs-1:\n",
        "        print(\"epoch:%s , loss:%s\" % (epoch , loss_ ))\n",
        "    if epoch % 50 == 0 or epoch == epochs-1:\n",
        "        torch.save(model.state_dict() , 'pth/model_female_epo%s.pth'%epoch) #save model\n",
        "        \n",
        "np.save('loss_female_epo%s.npy'%epochs,loss_hist)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 , loss:1.800540109021323\n",
            "epoch:10 , loss:1.6161178812526522\n",
            "epoch:20 , loss:1.4960530594417027\n",
            "epoch:30 , loss:1.3951059810320536\n",
            "epoch:40 , loss:1.3070394763492403\n",
            "epoch:50 , loss:1.2290448901766822\n",
            "epoch:60 , loss:1.1596916106769017\n",
            "epoch:70 , loss:1.0992823177292232\n",
            "epoch:80 , loss:1.045082654782704\n",
            "epoch:90 , loss:0.9966305358636947\n",
            "epoch:100 , loss:0.9525364519300915\n",
            "epoch:110 , loss:0.9123411789110729\n",
            "epoch:120 , loss:0.8773582619712467\n",
            "epoch:130 , loss:0.8435626072543008\n",
            "epoch:140 , loss:0.814842730533509\n",
            "epoch:150 , loss:0.7861349976630438\n",
            "epoch:160 , loss:0.759550437217667\n",
            "epoch:170 , loss:0.7357116180374509\n",
            "epoch:180 , loss:0.7132326432920637\n",
            "epoch:190 , loss:0.6924374946526118\n",
            "epoch:199 , loss:0.6764095634789694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8aZL97I4vO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,predict = logits.max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZe8pDwW4z5w",
        "colab_type": "text"
      },
      "source": [
        "### Print Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBiIRZF-4xnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self,idx2word,word2idx):\n",
        "        self.idx2word = idx2word\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = 25\n",
        "        self.eos_idx = 8002\n",
        "        self.EN_WHITELIST  = '0123456789abcdefghijklmnopqrstuvwxyz '             \n",
        "            \n",
        "    '''\n",
        "    idx -> word with EOS\n",
        "    '''        \n",
        "    def decode_line(self,sentence_idx,remove_pad=True,remove_eos=True):  #sentence_idx: 1d_matrix     \n",
        "        sentence = []\n",
        "        for w in sentence_idx:\n",
        "            if remove_eos and w==self.eos_idx:\n",
        "                continue\n",
        "            if remove_pad and w==0 : \n",
        "                continue\n",
        "            sentence.append(self.idx2word[w])\n",
        "            #if w==self.eos_idx:\n",
        "            #    break\n",
        "        sentence = ' '.join(sentence)\n",
        "        return sentence\n",
        "    \n",
        "    def decode(self,sentence_idxs,remove_pad=True,remove_eos=True): #sentence_idxs: 2d_matrix \n",
        "        sentences = []\n",
        "        for s in sentence_idxs: \n",
        "            sentences.append(self.decode_line(s,\n",
        "                                              remove_pad=remove_pad,\n",
        "                                              remove_eos=remove_eos))\n",
        "        return sentences\n",
        "            \n",
        "    '''\n",
        "    word -> idx with EOS\n",
        "    '''\n",
        "    def encode_line(self,sentence):  #sentence: 1d_matrix\n",
        "        sentence = sentence.lower()\n",
        "        s_list = ''.join([ ch for ch in sentence if ch in self.EN_WHITELIST ]).split()\n",
        "        sentence_idx = []\n",
        "        for w in s_list:\n",
        "            sentence_idx.append(self.word2idx[w])\n",
        "        n = len(sentence_idx)\n",
        "        if  n > self.max_len:\n",
        "            sentence_idx = sentence_idx[:self.max_len] \n",
        "        elif n < self.max_len:\n",
        "            sentence_idx = sentence_idx + [self.eos_idx] + [0]*(self.max_len-n-1)  \n",
        "        return sentence_idx\n",
        "    \n",
        "    def encode(self,sentences): #sentences: 2d_matrix   \n",
        "        sentence_idxs = []\n",
        "        for s in sentences: \n",
        "            sentence_idxs.append(self.encode_line(s))\n",
        "        return np.array(sentence_idxs)\n",
        "    \n",
        "    def print_QA(self, ques , pred_ans, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[2])\n",
        "            print('pred A :'+sents[1]) \n",
        "            \n",
        "    def print_QA_1(self, ques , pred_ans_train, pred_ans_test, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans_train[i], pred_ans_test[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[3])\n",
        "            print('train A:'+sents[1])    \n",
        "            print('test A :'+sents[2]) \n",
        "            \n",
        "    def print_QA_2(self, ques , ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i], ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfw-the044Ls",
        "colab_type": "code",
        "outputId": "36709686-858b-4cef-acb8-ac724d29f46b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict.unsqueeze(1).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([525, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxgxpfjIXmGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict.unsqueeze(1).cpu().view(-1,21)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkIJpu_jWvBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict.cpu().view(-1,n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BesQbFtvSzkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict.squeeze(1).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJTx7aQN453_",
        "colab_type": "code",
        "outputId": "b657c827-b8cd-4263-8645-ab8a2841f274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n = 525/25\n",
        "int(n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VcDxJ_X47mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab(metadata['idx2w'] , metadata['w2idx'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0FPxBw_4_rO",
        "colab_type": "text"
      },
      "source": [
        "### Try train corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlkUrbTD49ut",
        "colab_type": "code",
        "outputId": "69269142-d03c-44ff-e4e8-0d6fea2e74b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pred_ans = predict.cpu().view(-1,int(n)).data.numpy().T #predicted answer in train phase\n",
        "strd_ans = a.cpu().view(-1,int(n)).data.numpy().T #standard answer\n",
        "ques     = q.cpu().view(-1,int(n)).data.numpy().T #quenstions\n",
        "vocab.print_QA(ques , pred_ans, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :then from sad in split one want could a its\n",
            "A      :what out fix really your dont good data for\n",
            "pred A :what with fix what a guess unk data <EOS>\n",
            "\n",
            "Q      :whyd lincoln isnt the <EOS> <EOS> paula immediately possibly unk the\n",
            "A      :did there us what inspiration feel time i when <EOS>\n",
            "pred A :is there it what inspiration know time i when <EOS>\n",
            "\n",
            "Q      :you high it cards i what is be to ultimate\n",
            "A      :i <EOS> up we <EOS> so i with might the\n",
            "pred A :you <EOS> up we <EOS> so i for cant give\n",
            "\n",
            "Q      :say so he you this cant do this <EOS> help absurd\n",
            "A      :say <EOS> all throw well know you be message\n",
            "pred A :say <EOS> have throw well know you be unk\n",
            "\n",
            "Q      :that you uh had think morning stay you the us circus\n",
            "A      :<EOS> well need him <EOS> but until able unk\n",
            "pred A :<EOS> well got him i but at able get\n",
            "\n",
            "Q      :thing called <EOS> to you <EOS> here really daily not maybe i\n",
            "A      :it right out you you to well <EOS>\n",
            "pred A :it been out i you to well <EOS>\n",
            "\n",
            "Q      :about the sit are all were want news at never am\n",
            "A      :is someone <EOS> do what have have thats\n",
            "pred A :is to <EOS> were what think be thats\n",
            "\n",
            "Q      :tony casino on on in night you <EOS> melanie all <EOS> shot\n",
            "A      :but to i look do to them what\n",
            "pred A :but to i love do to some what\n",
            "\n",
            "Q      :unk a now that top the chasing asleep daniels im from\n",
            "A      :i boy sit dont so you go by were\n",
            "pred A :i boy do dont at you go for theyre\n",
            "\n",
            "Q      :<EOS> unk i wall now window your <EOS> would just gosh a\n",
            "A      :know wow up <EOS> silly want <EOS> tomorrow here theyll\n",
            "pred A :was wow up <EOS> silly mean <EOS> tomorrow talkin theyll\n",
            "\n",
            "Q      :<EOS> cant all well unk pain which you trying youll cannon\n",
            "A      :where <EOS> on <EOS> <EOS> the morning for unk\n",
            "pred A :i <EOS> after <EOS> <EOS> the morning for court\n",
            "\n",
            "Q      :see by there at yeah away one get to break into\n",
            "A      :i the dont blue would i <EOS>\n",
            "pred A :i the dont first <EOS> we <EOS>\n",
            "\n",
            "Q      :what its himself is work well <EOS> for me put a the\n",
            "A      :am i wall be <EOS> that mean\n",
            "pred A :did i house be <EOS> you think\n",
            "\n",
            "Q      :are okay he someone <EOS> i tomorrow the together lot theres energy\n",
            "A      :<EOS> cant with silly be <EOS> maybe\n",
            "pred A :<EOS> cant and safe be <EOS> maybe\n",
            "\n",
            "Q      :they its deserved coming wouldnt <EOS> im city all of no <EOS>\n",
            "A      :go us youll no all itll\n",
            "pred A :take us ill no okay itll\n",
            "\n",
            "Q      :doing alright to to expect here gonna desk the hearts way\n",
            "A      :out <EOS> be what thanks right be\n",
            "pred A :in and be what thanks right be\n",
            "\n",
            "Q      :<EOS> <EOS> no have challenge too it get please evidence <EOS> of\n",
            "A      :there we brilliant is <EOS> <EOS> at\n",
            "pred A :with we unk is <EOS> <EOS> careful\n",
            "\n",
            "Q      :unk somebody you much is a <EOS> to knowing the\n",
            "A      :how were think it the what\n",
            "pred A :i havent get that the what\n",
            "\n",
            "Q      :were <EOS> i <EOS> <EOS> soda what determine without world\n",
            "A      :can told of <EOS> to end are\n",
            "pred A :can unk of <EOS> to unk are\n",
            "\n",
            "Q      :the saw you you do what a is\n",
            "A      :i you me yeah have of you\n",
            "pred A :i they it yeah have of you\n",
            "\n",
            "Q      :unk thats it wanna want you it key fantastic\n",
            "A      :go could thats as i a the shooting\n",
            "pred A :get could thats if i a the talking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkHruBBOSVm9",
        "colab_type": "code",
        "outputId": "cebd5151-e2ea-4acc-ecc0-40d097c5df9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pred_ans = predict.unsqueeze(1).cpu().view(-1,int(n)).data.numpy().T #predicted answer in train phase\n",
        "strd_ans = a.cpu().unsqueeze(1).view(-1,int(n)).data.numpy().T #standard answer\n",
        "ques     = q.cpu().unsqueeze(1).view(-1,int(n)).data.numpy().T #quenstions\n",
        "vocab.print_QA(ques , pred_ans, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :then from sad in split one want could a its\n",
            "A      :what out fix really your dont good data for\n",
            "pred A :what with fix what a guess unk data <EOS>\n",
            "\n",
            "Q      :whyd lincoln isnt the <EOS> <EOS> paula immediately possibly unk the\n",
            "A      :did there us what inspiration feel time i when <EOS>\n",
            "pred A :is there it what inspiration know time i when <EOS>\n",
            "\n",
            "Q      :you high it cards i what is be to ultimate\n",
            "A      :i <EOS> up we <EOS> so i with might the\n",
            "pred A :you <EOS> up we <EOS> so i for cant give\n",
            "\n",
            "Q      :say so he you this cant do this <EOS> help absurd\n",
            "A      :say <EOS> all throw well know you be message\n",
            "pred A :say <EOS> have throw well know you be unk\n",
            "\n",
            "Q      :that you uh had think morning stay you the us circus\n",
            "A      :<EOS> well need him <EOS> but until able unk\n",
            "pred A :<EOS> well got him i but at able get\n",
            "\n",
            "Q      :thing called <EOS> to you <EOS> here really daily not maybe i\n",
            "A      :it right out you you to well <EOS>\n",
            "pred A :it been out i you to well <EOS>\n",
            "\n",
            "Q      :about the sit are all were want news at never am\n",
            "A      :is someone <EOS> do what have have thats\n",
            "pred A :is to <EOS> were what think be thats\n",
            "\n",
            "Q      :tony casino on on in night you <EOS> melanie all <EOS> shot\n",
            "A      :but to i look do to them what\n",
            "pred A :but to i love do to some what\n",
            "\n",
            "Q      :unk a now that top the chasing asleep daniels im from\n",
            "A      :i boy sit dont so you go by were\n",
            "pred A :i boy do dont at you go for theyre\n",
            "\n",
            "Q      :<EOS> unk i wall now window your <EOS> would just gosh a\n",
            "A      :know wow up <EOS> silly want <EOS> tomorrow here theyll\n",
            "pred A :was wow up <EOS> silly mean <EOS> tomorrow talkin theyll\n",
            "\n",
            "Q      :<EOS> cant all well unk pain which you trying youll cannon\n",
            "A      :where <EOS> on <EOS> <EOS> the morning for unk\n",
            "pred A :i <EOS> after <EOS> <EOS> the morning for court\n",
            "\n",
            "Q      :see by there at yeah away one get to break into\n",
            "A      :i the dont blue would i <EOS>\n",
            "pred A :i the dont first <EOS> we <EOS>\n",
            "\n",
            "Q      :what its himself is work well <EOS> for me put a the\n",
            "A      :am i wall be <EOS> that mean\n",
            "pred A :did i house be <EOS> you think\n",
            "\n",
            "Q      :are okay he someone <EOS> i tomorrow the together lot theres energy\n",
            "A      :<EOS> cant with silly be <EOS> maybe\n",
            "pred A :<EOS> cant and safe be <EOS> maybe\n",
            "\n",
            "Q      :they its deserved coming wouldnt <EOS> im city all of no <EOS>\n",
            "A      :go us youll no all itll\n",
            "pred A :take us ill no okay itll\n",
            "\n",
            "Q      :doing alright to to expect here gonna desk the hearts way\n",
            "A      :out <EOS> be what thanks right be\n",
            "pred A :in and be what thanks right be\n",
            "\n",
            "Q      :<EOS> <EOS> no have challenge too it get please evidence <EOS> of\n",
            "A      :there we brilliant is <EOS> <EOS> at\n",
            "pred A :with we unk is <EOS> <EOS> careful\n",
            "\n",
            "Q      :unk somebody you much is a <EOS> to knowing the\n",
            "A      :how were think it the what\n",
            "pred A :i havent get that the what\n",
            "\n",
            "Q      :were <EOS> i <EOS> <EOS> soda what determine without world\n",
            "A      :can told of <EOS> to end are\n",
            "pred A :can unk of <EOS> to unk are\n",
            "\n",
            "Q      :the saw you you do what a is\n",
            "A      :i you me yeah have of you\n",
            "pred A :i they it yeah have of you\n",
            "\n",
            "Q      :unk thats it wanna want you it key fantastic\n",
            "A      :go could thats as i a the shooting\n",
            "pred A :get could thats if i a the talking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psTz87qs5GoN",
        "colab_type": "text"
      },
      "source": [
        "### Try test corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46IB8CXaqvqt",
        "colab_type": "code",
        "outputId": "beaf9342-5e25-4438-d67c-5843d59cb698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "q.dim()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubtmCj9Nqx1Z",
        "colab_type": "code",
        "outputId": "4644bd38-ff39-469e-c679-eff0de8dfb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a.dim()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQuVsuixq2QM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a.unsqueeze(-3).dim()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfo4x2DIrPbr",
        "colab_type": "code",
        "outputId": "2576d02f-3409-4b7e-9bab-240304c91251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "q.unsqueeze(-3).dim()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_KCHxl-iBt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "o = model(q,a,feed_previous=True) #logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLpw98drNe2L",
        "colab_type": "code",
        "outputId": "aa8f4c5d-59d6-4e94-ba05-ec0c8d3df5eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()\n",
        "o = model(q,a,feed_previous=True) #logits\n",
        "_,predict_test = o.max(1)\n",
        "#vocab.decode(predict_test.cpu().view(-1,10).data.numpy().T,remove_eos=False,remove_pad=False)\n",
        "pred_ans_test = predict_test.cpu().view(-1,int(n)).data.numpy().T #predicted answer in test phase\n",
        "vocab.print_QA_1(ques , pred_ans, pred_ans_test, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :then from sad in split one want could a its\n",
            "A      :what out fix really your dont good data for\n",
            "train A:what with fix what a guess unk data <EOS>\n",
            "test A :what to fix what your guess dozen data for <EOS> <EOS> anything something mean rope field <EOS> <EOS> do anything something forcing job game <EOS> <EOS> <EOS> anything stuff forcing purpose field <EOS> <EOS> <EOS> anything stuff remembering thing field <EOS> <EOS> do anything stuff forcing thing field <EOS> <EOS> do anything stuff forcing thing field <EOS> <EOS> do anything unk forcing thing field <EOS> <EOS> do anything unk remembering thing field <EOS> <EOS> do anything unk tearing thing field <EOS> <EOS> do anything unk tearing thing field <EOS> <EOS> do anything unk tearing thing field <EOS> <EOS> do anything unk forcing thing field <EOS> <EOS> do anything unk forcing thing field <EOS> <EOS> do anything unk forcing thing field <EOS> <EOS> wear anything unk forcing thing field <EOS> tearing <EOS> wear anything unk forcing thing field <EOS> tearing <EOS> wear anything unk forcing thing field <EOS> tearing <EOS> wear anything unk forcing thing field <EOS> <EOS> wear anything unk forcing thing field <EOS> tearing <EOS> wear anything unk forcing thing field <EOS> proper <EOS> wear anything unk forcing thing field <EOS> tearing <EOS> wear anything unk forcing thing field <EOS> proper <EOS> wear anything unk forcing thing field <EOS> cheer <EOS> wear anything unk forcing thing field <EOS>\n",
            "\n",
            "Q      :whyd lincoln isnt the <EOS> <EOS> paula immediately possibly unk the\n",
            "A      :did there us what inspiration feel time i when <EOS>\n",
            "train A:is there it what inspiration know time i when <EOS>\n",
            "test A :did there it what inspiration know time i when <EOS> happened <EOS> this means darling think number <EOS> <EOS> <EOS> this thelma game plan number <EOS> <EOS> <EOS> this thelma examination mind breakdown <EOS> <EOS> <EOS> this thelma examination impress breakdown <EOS> <EOS> <EOS> this thelma examination michael breakdown <EOS> <EOS> <EOS> this thelma examination michael breakdown <EOS> <EOS> <EOS> this thelma examination impress breakdown <EOS> <EOS> about <EOS> this thelma examination impress breakdown <EOS> <EOS> about <EOS> this thelma examination thelma breakdown <EOS> <EOS> about <EOS> this thelma examination thelma breakdown <EOS> <EOS> <EOS> this thelma examination thelma breakdown <EOS> <EOS> <EOS> this thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> this thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> this thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS>\n",
            "\n",
            "Q      :you high it cards i what is be to ultimate\n",
            "A      :i <EOS> up we <EOS> so i with might the\n",
            "train A:you <EOS> up we <EOS> so i for cant give\n",
            "test A :you <EOS> up he <EOS> so i for dont unk you <EOS> here youre <EOS> <EOS> <EOS> mean <EOS> <EOS> here means <EOS> <EOS> <EOS> frighten <EOS> <EOS> here anything <EOS> cheap <EOS> visitors <EOS> <EOS> here anything <EOS> cheap <EOS> caused <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> <EOS> here anything <EOS> cheap <EOS> mean <EOS> <EOS> here anything <EOS> cheap <EOS> mean <EOS> sell <EOS> here anything <EOS> cheap <EOS> mean <EOS> lecter <EOS> here anything <EOS> anything <EOS> mean <EOS> impress <EOS> here anything <EOS> anything <EOS> mean <EOS> lecter <EOS> here anything <EOS> anything <EOS> mean <EOS> impress <EOS> here anything <EOS> anything <EOS> mean <EOS> lecter <EOS> here anything <EOS> anything <EOS> mean <EOS> impress <EOS> here anything <EOS> anything <EOS> mean <EOS> lecter <EOS> here anything <EOS> anything <EOS> mean <EOS> impress <EOS> here anything <EOS> anything <EOS> mean <EOS> lecter <EOS> here anything <EOS> anything <EOS> mean <EOS> impress <EOS> here anything <EOS> anything <EOS> mean <EOS> studio <EOS> here anything <EOS> anything <EOS> mean <EOS>\n",
            "\n",
            "Q      :say so he you this cant do this <EOS> help absurd\n",
            "A      :say <EOS> all throw well know you be message\n",
            "train A:say <EOS> have throw well know you be unk\n",
            "test A :do <EOS> do throw well know you be message frighten <EOS> did <EOS> mean coffee do customers frighten <EOS> did <EOS> caused notes do customers frighten <EOS> owe <EOS> caused notes explain childhood frighten <EOS> owe <EOS> visitors others explain childhood frighten <EOS> owe <EOS> visitors others do childhood frighten <EOS> owe <EOS> visitors others do childhood <EOS> <EOS> owe <EOS> visitors others do childhood <EOS> <EOS> owe forcing visitors others do childhood <EOS> <EOS> owe forcing visitors others do childhood frighten <EOS> owe forcing visitors others do childhood bump <EOS> owe forcing visitors others do childhood bump <EOS> owe forcing visitors others do childhood tearing <EOS> owe forcing visitors others do childhood bump <EOS> owe forcing visitors others do childhood impress <EOS> owe forcing visitors others do childhood impress <EOS> owe forcing visitors others do childhood impress <EOS> owe forcing visitors others do childhood jack <EOS> owe forcing visitors others do childhood tearing <EOS> owe forcing visitors others do childhood <EOS> <EOS> owe forcing visitors others do childhood impress <EOS> owe forcing visitors others do childhood <EOS> <EOS> owe forcing visitors others do childhood tearing <EOS> owe forcing visitors others do childhood monsieur <EOS> owe forcing visitors others do childhood\n",
            "\n",
            "Q      :that you uh had think morning stay you the us circus\n",
            "A      :<EOS> well need him <EOS> but until able unk\n",
            "train A:<EOS> well got him i but at able get\n",
            "test A :<EOS> well dont him <EOS> but at able finally <EOS> <EOS> anything <EOS> <EOS> in <EOS> <EOS> <EOS> <EOS> <EOS> it in cheap <EOS> <EOS> <EOS> <EOS> it followed <EOS> drain <EOS> <EOS> <EOS> that followed <EOS> is <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will forcing <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will ted <EOS> <EOS> jack followed <EOS> will ted <EOS> <EOS> jack followed <EOS> will owe <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will owe <EOS> <EOS> jack followed <EOS> will owe <EOS> <EOS> jack followed <EOS> will owe <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will owe <EOS> <EOS> jack followed <EOS> will ted <EOS> <EOS> jack followed <EOS> will\n",
            "\n",
            "Q      :thing called <EOS> to you <EOS> here really daily not maybe i\n",
            "A      :it right out you you to well <EOS>\n",
            "train A:it been out i you to well <EOS>\n",
            "test A :it up out i you to well <EOS> anything <EOS> happen it you <EOS> <EOS> about <EOS> <EOS> it you anything <EOS> about <EOS> <EOS> it jack <EOS> about <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> effect jacqueline jack <EOS> anything <EOS> effect jacqueline jack <EOS> anything <EOS> effect jacqueline jack <EOS> anything <EOS> ever jacqueline jack <EOS>\n",
            "\n",
            "Q      :about the sit are all were want news at never am\n",
            "A      :is someone <EOS> do what have have thats\n",
            "train A:is to <EOS> were what think be thats\n",
            "test A :is the <EOS> dont what love get thats <EOS> <EOS> you <EOS> take <EOS> about <EOS> <EOS> you letting keep <EOS> <EOS> <EOS> you tearing keep <EOS> about <EOS> <EOS> you tearing keep <EOS> about <EOS> <EOS> you tearing keep <EOS> about <EOS> <EOS> you tearing keep <EOS> <EOS> is <EOS> you tearing keep <EOS> <EOS> is <EOS> you tearing keep <EOS> anything is <EOS> you tearing keep <EOS> anything is <EOS> you tearing keep <EOS> anything is <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS>\n",
            "\n",
            "Q      :tony casino on on in night you <EOS> melanie all <EOS> shot\n",
            "A      :but to i look do to them what\n",
            "train A:but to i love do to some what\n",
            "test A :but to i understand do to some what anything loved <EOS> <EOS> coffee right anything anything loved <EOS> about this <EOS> anything anything loved <EOS> about this <EOS> anything anything loved <EOS> about this <EOS> anything anything loved <EOS> about this <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything\n",
            "\n",
            "Q      :unk a now that top the chasing asleep daniels im from\n",
            "A      :i boy sit dont so you go by were\n",
            "train A:i boy do dont at you go for theyre\n",
            "test A :i boy do dont like you go for were anything get frighten <EOS> you get <EOS> you my get frighten now you get <EOS> happened my get frighten now you get <EOS> happened my get forcing now you get <EOS> <EOS> anything get forcing now you get <EOS> <EOS> anything get forcing now you get <EOS> caused anything get forcing <EOS> you get <EOS> caused anything get frighten <EOS> you get <EOS> caused anything get frighten <EOS> you get <EOS> caused anything get frighten <EOS> you get <EOS> caused anything get frighten <EOS> you get <EOS> caused thelma get frighten <EOS> you get <EOS> caused thelma get frighten <EOS> you get <EOS> caused thelma get frighten <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused\n",
            "\n",
            "Q      :<EOS> unk i wall now window your <EOS> would just gosh a\n",
            "A      :know wow up <EOS> silly want <EOS> tomorrow here theyll\n",
            "train A:was wow up <EOS> silly mean <EOS> tomorrow talkin theyll\n",
            "test A :suppose wow up <EOS> silly want <EOS> tomorrow waiting theyll called in <EOS> unk think to <EOS> eating passed about in frighten jeff think to <EOS> unhappy passed anything off frighten awful <EOS> to <EOS> unhappy passed about off frighten awful <EOS> to <EOS> unhappy passed about off frighten awful <EOS> to <EOS> unhappy passed about off frighten awful <EOS> to <EOS> unhappy been about off value awful <EOS> to <EOS> unhappy thelma about off <EOS> awful <EOS> to <EOS> unhappy thelma about off <EOS> awful <EOS> to <EOS> unhappy thelma about off do awful <EOS> to <EOS> unhappy thelma about off charge awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten forcing <EOS> to <EOS> unhappy progress about off frighten forcing <EOS> to <EOS> unhappy\n",
            "\n",
            "Q      :<EOS> cant all well unk pain which you trying youll cannon\n",
            "A      :where <EOS> on <EOS> <EOS> the morning for unk\n",
            "train A:i <EOS> after <EOS> <EOS> the morning for court\n",
            "test A :i <EOS> on <EOS> <EOS> the morning for be <EOS> <EOS> on <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> detective on <EOS> about <EOS> <EOS> <EOS> <EOS> anything on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> anything <EOS> <EOS> on <EOS> about <EOS> <EOS> anything <EOS> <EOS> on <EOS> about <EOS> <EOS> anything <EOS> <EOS> on <EOS> about <EOS> <EOS> crap <EOS> <EOS> on <EOS> about <EOS> <EOS> crap <EOS> <EOS> on <EOS> about <EOS> <EOS> lately <EOS> <EOS> on <EOS> <EOS> <EOS> <EOS> lately <EOS> <EOS> on <EOS> <EOS> <EOS> <EOS> lately <EOS> <EOS> on <EOS> <EOS> <EOS> <EOS> lately <EOS> <EOS> on <EOS> <EOS> <EOS> <EOS> lately <EOS> <EOS> on <EOS> about <EOS> <EOS> lately <EOS> <EOS> on <EOS> about <EOS> <EOS> lately <EOS> <EOS> on <EOS> about <EOS> <EOS> lately <EOS> <EOS> on <EOS> about <EOS> <EOS> lately <EOS> <EOS> on <EOS> about <EOS> <EOS> anything\n",
            "\n",
            "Q      :see by there at yeah away one get to break into\n",
            "A      :i the dont blue would i <EOS>\n",
            "train A:i the dont first <EOS> we <EOS>\n",
            "test A :i the dont blue <EOS> we <EOS> <EOS> this meeting <EOS> <EOS> <EOS> <EOS> this drivers <EOS> <EOS> <EOS> <EOS> this drivers <EOS> <EOS> <EOS> <EOS> this childhood <EOS> <EOS> <EOS> <EOS> this childhood <EOS> <EOS> <EOS> is this childhood <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :what its himself is work well <EOS> for me put a the\n",
            "A      :am i wall be <EOS> that mean\n",
            "train A:did i house be <EOS> you think\n",
            "test A :was i road be <EOS> you think will door cheap i <EOS> will door player i <EOS> will door player <EOS> <EOS> will door towels <EOS> <EOS> will door towels <EOS> <EOS> will door towels <EOS> <EOS> will door player <EOS> <EOS> will door crying <EOS> <EOS> will door crack <EOS> <EOS> will door crack <EOS> <EOS> will door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS>\n",
            "\n",
            "Q      :are okay he someone <EOS> i tomorrow the together lot theres energy\n",
            "A      :<EOS> cant with silly be <EOS> maybe\n",
            "train A:<EOS> cant and safe be <EOS> maybe\n",
            "test A :<EOS> cant and silly be <EOS> maybe <EOS> mean <EOS> like <EOS> <EOS> <EOS> frighten <EOS> completely <EOS> <EOS> <EOS> frighten <EOS> diane <EOS> <EOS> <EOS> frighten <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> caused <EOS> diane <EOS> <EOS> <EOS> caused <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS>\n",
            "\n",
            "Q      :they its deserved coming wouldnt <EOS> im city all of no <EOS>\n",
            "A      :go us youll no all itll\n",
            "train A:take us ill no okay itll\n",
            "test A :take us ill no okay itll possibly unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS>\n",
            "\n",
            "Q      :doing alright to to expect here gonna desk the hearts way\n",
            "A      :out <EOS> be what thanks right be\n",
            "train A:in and be what thanks right be\n",
            "test A :out <EOS> be what thanks right be <EOS> <EOS> like right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test\n",
            "\n",
            "Q      :<EOS> <EOS> no have challenge too it get please evidence <EOS> of\n",
            "A      :there we brilliant is <EOS> <EOS> at\n",
            "train A:with we unk is <EOS> <EOS> careful\n",
            "test A :with we afraid is <EOS> <EOS> a <EOS> willing to <EOS> done <EOS> willing about <EOS> <EOS> done <EOS> willing about <EOS> <EOS> <EOS> <EOS> willing about <EOS> <EOS> <EOS> <EOS> willing about <EOS> <EOS> <EOS> <EOS> willing about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :unk somebody you much is a <EOS> to knowing the\n",
            "A      :how were think it the what\n",
            "train A:i havent get that the what\n",
            "test A :<EOS> could get it the what <EOS> <EOS> unfair anything trouble <EOS> happened criminals anything another <EOS> happened criminals anything <EOS> <EOS> <EOS> criminals anything <EOS> <EOS> <EOS> criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS>\n",
            "\n",
            "Q      :were <EOS> i <EOS> <EOS> soda what determine without world\n",
            "A      :can told of <EOS> to end are\n",
            "train A:can unk of <EOS> to unk are\n",
            "test A :can unk of <EOS> to unk are <EOS> wearing <EOS> to game <EOS> wearing <EOS> to game <EOS> thelma <EOS> to game about <EOS> thelma <EOS> to game about <EOS> mulder <EOS> to game about does mulder <EOS> to game about <EOS> mulder <EOS> to game about <EOS> mulder <EOS> to game about <EOS> mulder <EOS> to childhood about <EOS> mulder <EOS> to childhood about <EOS> mulder <EOS> to childhood about <EOS> mulder <EOS> to childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about\n",
            "\n",
            "Q      :the saw you you do what a is\n",
            "A      :i you me yeah have of you\n",
            "train A:i they it yeah have of you\n",
            "test A :we you me yeah have of you i you <EOS> impress or you i you <EOS> <EOS> <EOS> anything i you <EOS> <EOS> <EOS> anything i you <EOS> impress <EOS> you <EOS> you <EOS> impress <EOS> you <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything\n",
            "\n",
            "Q      :unk thats it wanna want you it key fantastic\n",
            "A      :go could thats as i a the shooting\n",
            "train A:get could thats if i a the talking\n",
            "test A :go could thats as i a the shooting <EOS> special and <EOS> anything woodsboro eating <EOS> all and anything anything woodsboro wearing <EOS> all and anything woodsboro <EOS> <EOS> all and <EOS> anything woodsboro <EOS> <EOS> special and <EOS> anything woodsboro <EOS> <EOS> is and <EOS> anything woodsboro <EOS> <EOS> is and <EOS> anything woodsboro <EOS> <EOS> is and <EOS> anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJQw23Tg5LPz",
        "colab_type": "code",
        "outputId": "4a54044e-c651-4934-d196-12ef1f9dab71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()\n",
        "o = model(q,a,feed_previous=True) #logits\n",
        "_,predict_test = o.max(1)\n",
        "#vocab.decode(predict_test.cpu().view(-1,10).data.numpy().T,remove_eos=False,remove_pad=False)\n",
        "pred_ans_test = predict_test.cpu().view(-1,int(n)).data.numpy().T #predicted answer in test phase\n",
        "vocab.print_QA_1(ques , pred_ans, pred_ans_test, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :then from sad in split one want could a its\n",
            "A      :what out fix really your dont good data for\n",
            "train A:what with fix what a guess unk data <EOS>\n",
            "test A :what to fix what your guess dozen data for <EOS> <EOS> anything something mean rope field <EOS> <EOS> do anything something forcing job game <EOS> <EOS> <EOS> anything stuff forcing purpose field <EOS> <EOS> <EOS> anything stuff remembering thing field <EOS> <EOS> do anything stuff forcing thing field <EOS> <EOS> do anything stuff forcing thing field <EOS> <EOS> do anything unk forcing thing field <EOS> <EOS> do anything unk remembering thing field <EOS> <EOS> do anything unk tearing thing field <EOS> <EOS> do anything unk tearing thing field <EOS> <EOS> do anything unk tearing thing field <EOS> <EOS> do anything unk forcing thing field <EOS> <EOS> do anything unk forcing thing field <EOS> <EOS> do anything unk forcing thing field <EOS> <EOS> wear anything unk forcing thing field <EOS> tearing <EOS> wear anything unk forcing thing field <EOS> tearing <EOS> wear anything unk forcing thing field <EOS> tearing <EOS> wear anything unk forcing thing field <EOS> <EOS> wear anything unk forcing thing field <EOS> tearing <EOS> wear anything unk forcing thing field <EOS> proper <EOS> wear anything unk forcing thing field <EOS> tearing <EOS> wear anything unk forcing thing field <EOS> proper <EOS> wear anything unk forcing thing field <EOS> cheer <EOS> wear anything unk forcing thing field <EOS>\n",
            "\n",
            "Q      :whyd lincoln isnt the <EOS> <EOS> paula immediately possibly unk the\n",
            "A      :did there us what inspiration feel time i when <EOS>\n",
            "train A:is there it what inspiration know time i when <EOS>\n",
            "test A :did there it what inspiration know time i when <EOS> happened <EOS> this means darling think number <EOS> <EOS> <EOS> this thelma game plan number <EOS> <EOS> <EOS> this thelma examination mind breakdown <EOS> <EOS> <EOS> this thelma examination impress breakdown <EOS> <EOS> <EOS> this thelma examination michael breakdown <EOS> <EOS> <EOS> this thelma examination michael breakdown <EOS> <EOS> <EOS> this thelma examination impress breakdown <EOS> <EOS> about <EOS> this thelma examination impress breakdown <EOS> <EOS> about <EOS> this thelma examination thelma breakdown <EOS> <EOS> about <EOS> this thelma examination thelma breakdown <EOS> <EOS> <EOS> this thelma examination thelma breakdown <EOS> <EOS> <EOS> this thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> this thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> this thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS> tearing <EOS> us thelma examination thelma breakdown <EOS> <EOS>\n",
            "\n",
            "Q      :you high it cards i what is be to ultimate\n",
            "A      :i <EOS> up we <EOS> so i with might the\n",
            "train A:you <EOS> up we <EOS> so i for cant give\n",
            "test A :you <EOS> up he <EOS> so i for dont unk you <EOS> here youre <EOS> <EOS> <EOS> mean <EOS> <EOS> here means <EOS> <EOS> <EOS> frighten <EOS> <EOS> here anything <EOS> cheap <EOS> visitors <EOS> <EOS> here anything <EOS> cheap <EOS> caused <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> you <EOS> here anything <EOS> cheap <EOS> mean <EOS> <EOS> here anything <EOS> cheap <EOS> mean <EOS> <EOS> here anything <EOS> cheap <EOS> mean <EOS> sell <EOS> here anything <EOS> cheap <EOS> mean <EOS> lecter <EOS> here anything <EOS> anything <EOS> mean <EOS> impress <EOS> here anything <EOS> anything <EOS> mean <EOS> lecter <EOS> here anything <EOS> anything <EOS> mean <EOS> impress <EOS> here anything <EOS> anything <EOS> mean <EOS> lecter <EOS> here anything <EOS> anything <EOS> mean <EOS> impress <EOS> here anything <EOS> anything <EOS> mean <EOS> lecter <EOS> here anything <EOS> anything <EOS> mean <EOS> impress <EOS> here anything <EOS> anything <EOS> mean <EOS> lecter <EOS> here anything <EOS> anything <EOS> mean <EOS> impress <EOS> here anything <EOS> anything <EOS> mean <EOS> studio <EOS> here anything <EOS> anything <EOS> mean <EOS>\n",
            "\n",
            "Q      :say so he you this cant do this <EOS> help absurd\n",
            "A      :say <EOS> all throw well know you be message\n",
            "train A:say <EOS> have throw well know you be unk\n",
            "test A :do <EOS> do throw well know you be message frighten <EOS> did <EOS> mean coffee do customers frighten <EOS> did <EOS> caused notes do customers frighten <EOS> owe <EOS> caused notes explain childhood frighten <EOS> owe <EOS> visitors others explain childhood frighten <EOS> owe <EOS> visitors others do childhood frighten <EOS> owe <EOS> visitors others do childhood <EOS> <EOS> owe <EOS> visitors others do childhood <EOS> <EOS> owe forcing visitors others do childhood <EOS> <EOS> owe forcing visitors others do childhood frighten <EOS> owe forcing visitors others do childhood bump <EOS> owe forcing visitors others do childhood bump <EOS> owe forcing visitors others do childhood tearing <EOS> owe forcing visitors others do childhood bump <EOS> owe forcing visitors others do childhood impress <EOS> owe forcing visitors others do childhood impress <EOS> owe forcing visitors others do childhood impress <EOS> owe forcing visitors others do childhood jack <EOS> owe forcing visitors others do childhood tearing <EOS> owe forcing visitors others do childhood <EOS> <EOS> owe forcing visitors others do childhood impress <EOS> owe forcing visitors others do childhood <EOS> <EOS> owe forcing visitors others do childhood tearing <EOS> owe forcing visitors others do childhood monsieur <EOS> owe forcing visitors others do childhood\n",
            "\n",
            "Q      :that you uh had think morning stay you the us circus\n",
            "A      :<EOS> well need him <EOS> but until able unk\n",
            "train A:<EOS> well got him i but at able get\n",
            "test A :<EOS> well dont him <EOS> but at able finally <EOS> <EOS> anything <EOS> <EOS> in <EOS> <EOS> <EOS> <EOS> <EOS> it in cheap <EOS> <EOS> <EOS> <EOS> it followed <EOS> drain <EOS> <EOS> <EOS> that followed <EOS> is <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will forcing <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will ted <EOS> <EOS> jack followed <EOS> will ted <EOS> <EOS> jack followed <EOS> will owe <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will owe <EOS> <EOS> jack followed <EOS> will owe <EOS> <EOS> jack followed <EOS> will owe <EOS> <EOS> jack followed <EOS> will <EOS> <EOS> <EOS> jack followed <EOS> will owe <EOS> <EOS> jack followed <EOS> will ted <EOS> <EOS> jack followed <EOS> will\n",
            "\n",
            "Q      :thing called <EOS> to you <EOS> here really daily not maybe i\n",
            "A      :it right out you you to well <EOS>\n",
            "train A:it been out i you to well <EOS>\n",
            "test A :it up out i you to well <EOS> anything <EOS> happen it you <EOS> <EOS> about <EOS> <EOS> it you anything <EOS> about <EOS> <EOS> it jack <EOS> about <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> <EOS> jacqueline jack <EOS> anything <EOS> effect jacqueline jack <EOS> anything <EOS> effect jacqueline jack <EOS> anything <EOS> effect jacqueline jack <EOS> anything <EOS> ever jacqueline jack <EOS>\n",
            "\n",
            "Q      :about the sit are all were want news at never am\n",
            "A      :is someone <EOS> do what have have thats\n",
            "train A:is to <EOS> were what think be thats\n",
            "test A :is the <EOS> dont what love get thats <EOS> <EOS> you <EOS> take <EOS> about <EOS> <EOS> you letting keep <EOS> <EOS> <EOS> you tearing keep <EOS> about <EOS> <EOS> you tearing keep <EOS> about <EOS> <EOS> you tearing keep <EOS> about <EOS> <EOS> you tearing keep <EOS> <EOS> is <EOS> you tearing keep <EOS> <EOS> is <EOS> you tearing keep <EOS> anything is <EOS> you tearing keep <EOS> anything is <EOS> you tearing keep <EOS> anything is <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> anything <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS> <EOS> <EOS> you tearing keep <EOS>\n",
            "\n",
            "Q      :tony casino on on in night you <EOS> melanie all <EOS> shot\n",
            "A      :but to i look do to them what\n",
            "train A:but to i love do to some what\n",
            "test A :but to i understand do to some what anything loved <EOS> <EOS> coffee right anything anything loved <EOS> about this <EOS> anything anything loved <EOS> about this <EOS> anything anything loved <EOS> about this <EOS> anything anything loved <EOS> about this <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything anything loved <EOS> about to <EOS> anything\n",
            "\n",
            "Q      :unk a now that top the chasing asleep daniels im from\n",
            "A      :i boy sit dont so you go by were\n",
            "train A:i boy do dont at you go for theyre\n",
            "test A :i boy do dont like you go for were anything get frighten <EOS> you get <EOS> you my get frighten now you get <EOS> happened my get frighten now you get <EOS> happened my get forcing now you get <EOS> <EOS> anything get forcing now you get <EOS> <EOS> anything get forcing now you get <EOS> caused anything get forcing <EOS> you get <EOS> caused anything get frighten <EOS> you get <EOS> caused anything get frighten <EOS> you get <EOS> caused anything get frighten <EOS> you get <EOS> caused anything get frighten <EOS> you get <EOS> caused thelma get frighten <EOS> you get <EOS> caused thelma get frighten <EOS> you get <EOS> caused thelma get frighten <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused thelma get forcing <EOS> you get <EOS> caused\n",
            "\n",
            "Q      :<EOS> unk i wall now window your <EOS> would just gosh a\n",
            "A      :know wow up <EOS> silly want <EOS> tomorrow here theyll\n",
            "train A:was wow up <EOS> silly mean <EOS> tomorrow talkin theyll\n",
            "test A :suppose wow up <EOS> silly want <EOS> tomorrow waiting theyll called in <EOS> unk think to <EOS> eating passed about in frighten jeff think to <EOS> unhappy passed anything off frighten awful <EOS> to <EOS> unhappy passed about off frighten awful <EOS> to <EOS> unhappy passed about off frighten awful <EOS> to <EOS> unhappy passed about off frighten awful <EOS> to <EOS> unhappy been about off value awful <EOS> to <EOS> unhappy thelma about off <EOS> awful <EOS> to <EOS> unhappy thelma about off <EOS> awful <EOS> to <EOS> unhappy thelma about off do awful <EOS> to <EOS> unhappy thelma about off charge awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten awful <EOS> to <EOS> unhappy progress about off frighten forcing <EOS> to <EOS> unhappy progress about off frighten forcing <EOS> to <EOS> unhappy\n",
            "\n",
            "Q      :<EOS> cant all well unk pain which you trying youll cannon\n",
            "A      :where <EOS> on <EOS> <EOS> the morning for unk\n",
            "train A:i <EOS> after <EOS> <EOS> the morning for court\n",
            "test A :i <EOS> on <EOS> <EOS> the morning for be <EOS> <EOS> on <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> detective on <EOS> about <EOS> <EOS> <EOS> <EOS> anything on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> <EOS> <EOS> <EOS> on <EOS> about <EOS> <EOS> anything <EOS> <EOS> on <EOS> about <EOS> <EOS> anything <EOS> <EOS> on <EOS> about <EOS> <EOS> anything <EOS> <EOS> on <EOS> about <EOS> <EOS> crap <EOS> <EOS> on <EOS> about <EOS> <EOS> crap <EOS> <EOS> on <EOS> about <EOS> <EOS> lately <EOS> <EOS> on <EOS> <EOS> <EOS> <EOS> lately <EOS> <EOS> on <EOS> <EOS> <EOS> <EOS> lately <EOS> <EOS> on <EOS> <EOS> <EOS> <EOS> lately <EOS> <EOS> on <EOS> <EOS> <EOS> <EOS> lately <EOS> <EOS> on <EOS> about <EOS> <EOS> lately <EOS> <EOS> on <EOS> about <EOS> <EOS> lately <EOS> <EOS> on <EOS> about <EOS> <EOS> lately <EOS> <EOS> on <EOS> about <EOS> <EOS> lately <EOS> <EOS> on <EOS> about <EOS> <EOS> anything\n",
            "\n",
            "Q      :see by there at yeah away one get to break into\n",
            "A      :i the dont blue would i <EOS>\n",
            "train A:i the dont first <EOS> we <EOS>\n",
            "test A :i the dont blue <EOS> we <EOS> <EOS> this meeting <EOS> <EOS> <EOS> <EOS> this drivers <EOS> <EOS> <EOS> <EOS> this drivers <EOS> <EOS> <EOS> <EOS> this childhood <EOS> <EOS> <EOS> <EOS> this childhood <EOS> <EOS> <EOS> is this childhood <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS> is this teenager <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :what its himself is work well <EOS> for me put a the\n",
            "A      :am i wall be <EOS> that mean\n",
            "train A:did i house be <EOS> you think\n",
            "test A :was i road be <EOS> you think will door cheap i <EOS> will door player i <EOS> will door player <EOS> <EOS> will door towels <EOS> <EOS> will door towels <EOS> <EOS> will door towels <EOS> <EOS> will door player <EOS> <EOS> will door crying <EOS> <EOS> will door crack <EOS> <EOS> will door crack <EOS> <EOS> will door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS> am door crack <EOS> <EOS>\n",
            "\n",
            "Q      :are okay he someone <EOS> i tomorrow the together lot theres energy\n",
            "A      :<EOS> cant with silly be <EOS> maybe\n",
            "train A:<EOS> cant and safe be <EOS> maybe\n",
            "test A :<EOS> cant and silly be <EOS> maybe <EOS> mean <EOS> like <EOS> <EOS> <EOS> frighten <EOS> completely <EOS> <EOS> <EOS> frighten <EOS> diane <EOS> <EOS> <EOS> frighten <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> caused <EOS> diane <EOS> <EOS> <EOS> caused <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS> <EOS> visitors <EOS> diane <EOS> <EOS>\n",
            "\n",
            "Q      :they its deserved coming wouldnt <EOS> im city all of no <EOS>\n",
            "A      :go us youll no all itll\n",
            "train A:take us ill no okay itll\n",
            "test A :take us ill no okay itll possibly unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS> explain unk <EOS> <EOS>\n",
            "\n",
            "Q      :doing alright to to expect here gonna desk the hearts way\n",
            "A      :out <EOS> be what thanks right be\n",
            "train A:in and be what thanks right be\n",
            "test A :out <EOS> be what thanks right be <EOS> <EOS> like right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test <EOS> <EOS> hurt anything right test\n",
            "\n",
            "Q      :<EOS> <EOS> no have challenge too it get please evidence <EOS> of\n",
            "A      :there we brilliant is <EOS> <EOS> at\n",
            "train A:with we unk is <EOS> <EOS> careful\n",
            "test A :with we afraid is <EOS> <EOS> a <EOS> willing to <EOS> done <EOS> willing about <EOS> <EOS> done <EOS> willing about <EOS> <EOS> <EOS> <EOS> willing about <EOS> <EOS> <EOS> <EOS> willing about <EOS> <EOS> <EOS> <EOS> willing about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS> <EOS> dust about <EOS> <EOS> <EOS>\n",
            "\n",
            "Q      :unk somebody you much is a <EOS> to knowing the\n",
            "A      :how were think it the what\n",
            "train A:i havent get that the what\n",
            "test A :<EOS> could get it the what <EOS> <EOS> unfair anything trouble <EOS> happened criminals anything another <EOS> happened criminals anything <EOS> <EOS> <EOS> criminals anything <EOS> <EOS> <EOS> criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS> <EOS> caused criminals anything <EOS>\n",
            "\n",
            "Q      :were <EOS> i <EOS> <EOS> soda what determine without world\n",
            "A      :can told of <EOS> to end are\n",
            "train A:can unk of <EOS> to unk are\n",
            "test A :can unk of <EOS> to unk are <EOS> wearing <EOS> to game <EOS> wearing <EOS> to game <EOS> thelma <EOS> to game about <EOS> thelma <EOS> to game about <EOS> mulder <EOS> to game about does mulder <EOS> to game about <EOS> mulder <EOS> to game about <EOS> mulder <EOS> to game about <EOS> mulder <EOS> to childhood about <EOS> mulder <EOS> to childhood about <EOS> mulder <EOS> to childhood about <EOS> mulder <EOS> to childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about <EOS> mulder <EOS> wrong childhood about\n",
            "\n",
            "Q      :the saw you you do what a is\n",
            "A      :i you me yeah have of you\n",
            "train A:i they it yeah have of you\n",
            "test A :we you me yeah have of you i you <EOS> impress or you i you <EOS> <EOS> <EOS> anything i you <EOS> <EOS> <EOS> anything i you <EOS> impress <EOS> you <EOS> you <EOS> impress <EOS> you <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything <EOS> you <EOS> impress <EOS> anything\n",
            "\n",
            "Q      :unk thats it wanna want you it key fantastic\n",
            "A      :go could thats as i a the shooting\n",
            "train A:get could thats if i a the talking\n",
            "test A :go could thats as i a the shooting <EOS> special and <EOS> anything woodsboro eating <EOS> all and anything anything woodsboro wearing <EOS> all and anything woodsboro <EOS> <EOS> all and <EOS> anything woodsboro <EOS> <EOS> special and <EOS> anything woodsboro <EOS> <EOS> is and <EOS> anything woodsboro <EOS> <EOS> is and <EOS> anything woodsboro <EOS> <EOS> is and <EOS> anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS> <EOS> is and anything anything woodsboro <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZK_ZYC55NRd",
        "colab_type": "text"
      },
      "source": [
        "### Try Chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxzM3DlE5EhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = []\n",
        "lines.append( 'you can do it'  )\n",
        "lines.append( 'how are you'    )\n",
        "lines.append( 'fuck you'  )\n",
        "lines.append( 'jesus christ you scared the shit out of me'  )\n",
        "lines.append( 'youre terrible'  )\n",
        "lines.append( 'is something wrong' )\n",
        "lines.append( 'nobodys gonna get inside' )\n",
        "lines.append( 'im sorry'  )\n",
        "lines.append( 'shut up'  )\n",
        "N = len(lines)\n",
        "lines = vocab.encode(lines)\n",
        "q_o = Variable(torch.from_numpy(lines).long()).cuda()\n",
        "#vocab.decode(vocab.encode(lines))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBazLuWI5QHW",
        "colab_type": "code",
        "outputId": "54b6495f-d4ad-4d37-86ee-abec180a3bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "model.eval()\n",
        "o = model(q_o,a[:N],feed_previous=True)\n",
        "_,predict_o = o.max(1)\n",
        "#vocab.decode(predict_o.cpu().view(-1,3).data.numpy().T)\n",
        "pred_ans_o = predict_o.cpu().view(-1,N).data.numpy().T #predicted answer \n",
        "vocab.print_QA_2(lines, pred_ans_o)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q      :you can do it\n",
            "A      :i doesnt can <EOS> right out i of <EOS> is <EOS> anything around you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> does <EOS> anything off you <EOS> <EOS> does <EOS> anything off you <EOS> is does <EOS> anything off you <EOS> is <EOS> <EOS> anything off you <EOS> is <EOS> <EOS> anything off you <EOS> is <EOS> <EOS> anything off you <EOS> is <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS> <EOS> <EOS> <EOS> anything off you <EOS>\n",
            "\n",
            "Q      :how are you\n",
            "A      :do great we i funny and yourself <EOS> something i thelma on <EOS> about anything i thelma on <EOS> about anything i thelma on <EOS> about anything it thelma on <EOS> about anything it thelma on <EOS> about anything <EOS> thelma on <EOS> about anything <EOS> thelma on <EOS> about anything <EOS> thelma on <EOS> about anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> about anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> anything <EOS> thelma on <EOS> tearing anything <EOS> thelma on <EOS> tearing anything <EOS> thelma on <EOS> tearing anything <EOS> thelma on <EOS>\n",
            "\n",
            "Q      :fuck you\n",
            "A      :you its go dont i the what and you something <EOS> <EOS> youre this and mine <EOS> about means this and you my <EOS> <EOS> anything this and you nick <EOS> <EOS> anything this and you anything <EOS> caused anything this and you anything <EOS> caused anything this and you anything <EOS> caused anything this and you anything <EOS> caused anything this and you anything <EOS> caused anything this and you anything <EOS> caused anything this and you thelma <EOS> caused anything this and you thelma <EOS> caused anything this and you thelma <EOS> caused anything this and you thelma <EOS> caused anything this and you thelma <EOS> caused anything this and you thelma <EOS> caused anything this and you thelma <EOS> caused anything this and you thelma <EOS> caused anything this and you thelma <EOS> caused anything this and jack thelma <EOS> caused anything this and jack thelma <EOS> caused anything this and jack thelma <EOS> caused anything this and jack thelma <EOS> caused anything this and jack thelma <EOS> caused anything this and\n",
            "\n",
            "Q      :jesus christ you scared the shit out of me\n",
            "A      :do cant thank back unk have road be well do thelma <EOS> eating did door something frighten passed <EOS> learned are door stuff <EOS> passed <EOS> thelma owe door stuff <EOS> passed <EOS> thelma owe door stuff <EOS> been <EOS> mulder owe door stuff <EOS> been <EOS> mulder owe door stuff <EOS> thelma <EOS> mulder owe door stuff <EOS> thelma <EOS> mulder owe door unk <EOS> thelma <EOS> mulder owe door unk <EOS> thelma <EOS> mulder owe door unk <EOS> progress <EOS> mulder owe door unk frighten progress <EOS> mulder owe door unk frighten progress <EOS> mulder owe door unk frighten progress <EOS> mulder owe door unk frighten progress <EOS> mulder owe door unk <EOS> progress <EOS> mulder owe door unk <EOS> progress <EOS> mulder owe door unk <EOS> progress <EOS> mulder owe door unk <EOS> progress <EOS> mulder owe door unk <EOS> progress <EOS> mulder owe door unk <EOS> progress <EOS> mulder owe door unk <EOS> progress <EOS> mulder owe door unk <EOS> progress <EOS> mulder owe door unk <EOS> progress <EOS> mulder owe door unk\n",
            "\n",
            "Q      :youre terrible\n",
            "A      :<EOS> how love there <EOS> have and what mad inspiration <EOS> <EOS> mean <EOS> you <EOS> <EOS> completely daughter about <EOS> frighten <EOS> you <EOS> <EOS> completely property <EOS> <EOS> frighten <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> frighten <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> frighten <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> forcing <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> caused <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination <EOS> <EOS> visitors <EOS> you <EOS> <EOS> diane examination\n",
            "\n",
            "Q      :is something wrong\n",
            "A      :i dont be <EOS> know out the know <EOS> <EOS> <EOS> possibly <EOS> unk <EOS> unk mean <EOS> <EOS> <EOS> explain <EOS> special <EOS> unk frighten <EOS> <EOS> <EOS> <EOS> <EOS> all <EOS> unk frighten <EOS> <EOS> <EOS> <EOS> <EOS> all <EOS> unk frighten <EOS> <EOS> is <EOS> <EOS> special <EOS> unk frighten <EOS> <EOS> is explain <EOS> is <EOS> unk frighten <EOS> <EOS> is explain <EOS> is <EOS> unk frighten <EOS> <EOS> is explain <EOS> is <EOS> unk forcing <EOS> <EOS> is explain <EOS> is <EOS> unk forcing <EOS> <EOS> is explain <EOS> is <EOS> unk forcing <EOS> <EOS> is explain <EOS> is <EOS> unk forcing <EOS> <EOS> is explain <EOS> is <EOS> unk forcing <EOS> <EOS> is explain <EOS> is <EOS> unk forcing <EOS> <EOS> is explain <EOS> is <EOS> unk forcing <EOS> <EOS> is explain <EOS> is <EOS> unk forcing <EOS> <EOS> is explain <EOS> is <EOS> unk tearing <EOS> <EOS> is explain <EOS> is <EOS> unk tearing <EOS> <EOS> is explain <EOS> is <EOS> unk tearing <EOS> <EOS> is explain <EOS> is <EOS> unk tearing <EOS> <EOS> is explain <EOS> is <EOS> unk tearing <EOS> <EOS> is explain <EOS> is <EOS> unk tearing <EOS> <EOS> is explain <EOS> is <EOS> unk tearing <EOS> <EOS> is explain <EOS> is <EOS> unk tearing <EOS> <EOS> is explain <EOS> is <EOS> unk tearing <EOS> <EOS>\n",
            "\n",
            "Q      :nobodys gonna get inside\n",
            "A      :was we back get on <EOS> wow know like will anything <EOS> get <EOS> <EOS> plan be will anything <EOS> <EOS> <EOS> <EOS> frighten hurt will anything <EOS> <EOS> <EOS> <EOS> frighten hurt will about <EOS> <EOS> <EOS> <EOS> frighten hurt will about <EOS> do <EOS> <EOS> frighten hurt will about <EOS> do <EOS> <EOS> frighten hurt will about <EOS> do <EOS> <EOS> frighten hurt will about <EOS> do <EOS> <EOS> frighten hurt will about <EOS> do <EOS> <EOS> frighten hurt will about <EOS> do <EOS> <EOS> frighten hurt am about <EOS> do <EOS> <EOS> frighten hurt am about <EOS> do <EOS> <EOS> frighten hurt am about <EOS> do <EOS> <EOS> value hurt am about <EOS> do <EOS> <EOS> <EOS> hurt am about <EOS> wear <EOS> <EOS> charge hurt am about <EOS> wear <EOS> <EOS> <EOS> hurt am about <EOS> wear <EOS> <EOS> charge hurt am about <EOS> wear <EOS> <EOS> charge hurt am about <EOS> wear <EOS> <EOS> charge hurt am about <EOS> wear <EOS> <EOS> charge hurt am about <EOS> wear <EOS> <EOS> charge hurt am about <EOS> wear <EOS> <EOS> impress hurt am about <EOS> wear <EOS> <EOS> impress hurt am about <EOS> wear <EOS> <EOS> impress hurt\n",
            "\n",
            "Q      :im sorry\n",
            "A      :its <EOS> you <EOS> some out me so <EOS> anything <EOS> this loved it willing <EOS> anything <EOS> this loved willing <EOS> anything <EOS> this loved willing <EOS> <EOS> <EOS> this loved willing <EOS> <EOS> <EOS> this loved willing <EOS> <EOS> <EOS> this loved willing <EOS> <EOS> <EOS> this loved dust <EOS> <EOS> <EOS> this loved dust <EOS> <EOS> <EOS> this loved dust <EOS> <EOS> <EOS> this loved dust <EOS> <EOS> <EOS> this loved dust <EOS> <EOS> <EOS> this loved dust <EOS> <EOS> <EOS> this loved dust <EOS> <EOS> <EOS> this loved dust <EOS> <EOS> <EOS> us loved dust <EOS> <EOS> <EOS> us loved dust <EOS> <EOS> <EOS> us loved dust <EOS> <EOS> <EOS> us loved dust <EOS> <EOS> <EOS> us loved dust <EOS> <EOS> <EOS> us loved dust <EOS> <EOS> <EOS> us loved dust <EOS> <EOS> <EOS> us loved dust <EOS> <EOS> <EOS> us loved dust <EOS> <EOS> <EOS> us loved dust\n",
            "\n",
            "Q      :shut up\n",
            "A      :its is up oh get <EOS> do <EOS> <EOS> here get do criminals about <EOS> here get <EOS> criminals about <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get anything criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get <EOS> criminals anything <EOS> here get let criminals\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDVOetNF5Wfr",
        "colab_type": "text"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBushY7V5SQx",
        "colab_type": "code",
        "outputId": "ce5b8bbc-3318-4bf9-b91d-c0d8f44d33d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "epoch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dRJY-kH5ZL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#np.save('loss_female_prob_feed_epo141.npy',epoch_mean_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU5J7MaF5bEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict() , 'model_female_prob_feed_epo141.pth') #save model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSvqzYpD5etY",
        "colab_type": "text"
      },
      "source": [
        "### Back up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ8HttgO5efj",
        "colab_type": "code",
        "outputId": "e0202fb0-0001-417f-f012-0239d23978bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  11,  564, 8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  52,  112,   39,   45,   12,   43,  795,   52,  112,   39,   45,   12,\n",
              "           19, 8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  17,  154,   13, 1031,   36,   17,  268,   20, 4522,    3,  258,   32,\n",
              "           28,  174, 8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  69,   66,   15,  255,   70, 8999,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [ 710,   86, 1038,    2,    6,  340,  441, 8999,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   4,  312,  826,   89,    3,   73,   34,   40,    4,  166,  490,   92,\n",
              "           35,    3,  494,    7,    5,    4,  286,   64,    3,   56,  167, 8999,\n",
              "            0],\n",
              "        [ 592,    2,   39,   50,    5,   78,   72, 8999,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   3,   14,   18,   46,   44,  724, 1602, 8999,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  36,    3,   14,  232, 8999,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  72,   21,    4, 2017,  508,    4,  384,    3,   48, 8999,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  19,   15,   25,  995, 8999,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   3,  158,   22,    2,    1,   50,  119,  116,   20,  166, 8999,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [ 162,   17,   65,    5,  224,   34,    2, 8999,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  27,   27,  271,    5,  336,  320,  900, 8999,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [ 173,    4,  863, 8999,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [ 471,   32, 3844,   38,    2,   40,    4,   79,   26,   83,  552,  349,\n",
              "          150,  275,   50, 8999,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  72, 8999,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   3,    3,  109,  794,   16,    9,   24, 5515,    2,   68,   32,  178,\n",
              "           18,   17,   43, 8999,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [   4,    1,   93,  200,  147,  636,   21,   29,  308, 8999,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0],\n",
              "        [  41,   27,   53,  346,   74,   27,   27,  611,   14,   22,  119, 1604,\n",
              "            6, 2317, 1630,    6, 6448,  440,   21,  123,   61, 8999,    0,    0,\n",
              "            0],\n",
              "        [   8,   38,   24,  256, 8999,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiU501ST5h-9",
        "colab_type": "code",
        "outputId": "e065f4aa-d80f-478f-be35-8d14624ed097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "x = Variable(torch.rand(3,25)*200).long()\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 68,  45, 184,  74, 127, 123,  83, 113, 132,  63, 161,  62, 122,  65,\n",
              "          38,  12, 137, 188, 154,  56,  89,  62, 128,   2, 148],\n",
              "        [ 13,  80, 156, 142,  86,  71, 111, 174,  79,  45, 103, 100, 119,  90,\n",
              "         116,  83,  44, 116,  86, 117,   3,   1,  45,  64, 148],\n",
              "        [146,  59,  55,  32,  63, 105, 186, 129,  72,  48,  42,  83,  21,  23,\n",
              "          57,  91, 191,  33,  85,  60, 137,  85, 158, 112, 182]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glS4EvYE5jzP",
        "colab_type": "code",
        "outputId": "7bb798fe-9f61-4390-bf79-32c72544dbe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "_,p = model(x,q,feed_previous=True).max(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-f3cb48f03413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_previous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-8c35984161e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target, feed_previous)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_previous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msrc_em\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtrg_em\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUcolkLl5lOg",
        "colab_type": "code",
        "outputId": "8f57bef3-28a0-40ca-9713-29e06d4d841e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "vocab.decode(p.view(3,25).data.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-978c7dfa8ff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8tmmD-i5p_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaKI0pcu5sEp",
        "colab_type": "text"
      },
      "source": [
        "### Data Handling\n",
        "load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7tDvLai5uLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = np.load(\"./datasets/m_idx_a_1.npy\")\n",
        "ques = np.load(\"./datasets/m_idx_q_1.npy\")\n",
        "\n",
        "with open('./datasets/metadata_1.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDAALWWq5xhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaleDataset(data.Dataset): \n",
        "    def __init__(self,ques,ans):\n",
        "        self.ques = ques\n",
        "        self.ans = ans\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ques_tensor = torch.from_numpy(self.ques[index]).long()\n",
        "        ans_tensor = torch.from_numpy(self.ans[index]).long()\n",
        "        \n",
        "        return ques_tensor , ans_tensor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return 78119"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owpKeooS5ze7",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFZAXFti5yNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "male_dataset = MaleDataset(ques,ans)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=male_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr40H1Rf52cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def __init__(self,\n",
        "                 src_voc_size=9000,\n",
        "                 trg_voc_size=9000,\n",
        "                 src_embedding_size=256,\n",
        "                 trg_embedding_size=256,\n",
        "                 enc_hidden_size=200,\n",
        "                 dec_hidden_size=200):\n",
        "        \n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.trg_embedding_size = trg_embedding_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "        \n",
        "        self.src_embedder = nn.Embedding(src_voc_size , src_embedding_size)\n",
        "        self.encoder = nn.LSTM(src_embedding_size ,enc_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        \n",
        "        self.trg_embedder = nn.Embedding(trg_voc_size , trg_embedding_size)\n",
        "        self.decoder = nn.LSTM(trg_embedding_size ,dec_hidden_size,3, batch_first=True,dropout=0.5)\n",
        "        self.cls = nn.Linear(dec_hidden_size , trg_voc_size)\n",
        "    \n",
        "    def forward(self,source,target,feed_previous=False):\n",
        "        batch_size = source.size()[0]\n",
        "        src_em = self.src_embedder(source)\n",
        "        trg_em = self.trg_embedder(target)\n",
        "        \n",
        "        _ , enc_state = self.encoder(src_em)\n",
        "        \n",
        "        GO = Variable(torch.zeros(batch_size,1,self.trg_embedding_size)).cuda()\n",
        "        \n",
        "        if feed_previous: #test phase\n",
        "            logits_ = []\n",
        "            inputs = GO\n",
        "            h = enc_state\n",
        "            for i in range(25):\n",
        "                output , h = self.decoder(inputs,h)\n",
        "                logits = self.cls(output.view(-1, self.dec_hidden_size))  # (1, vocab_size)\n",
        "                logits_.append(logits)\n",
        "                \n",
        "                predicted = logits.max(1)[1]\n",
        "                inputs = self.trg_embedder(predicted)\n",
        "                    \n",
        "            return torch.cat(logits_,0)\n",
        "            \n",
        "        else: #train phase\n",
        "            dec_in = torch.cat([GO,trg_em[:,:-1,:]],1)\n",
        "            outputs , _ = self.decoder(dec_in,enc_state)\n",
        "            outputs = outputs.contiguous().view(-1,self.dec_hidden_size)\n",
        "            logits = self.cls(outputs)\n",
        "        \n",
        "            return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPPJi5TqO1oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxgSZcB9O5TN",
        "colab_type": "code",
        "outputId": "f0ebb4e2-d71c-4197-8fda-897b3487367f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (src_embedder): Embedding(9000, 256)\n",
              "  (encoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (trg_embedder): Embedding(9000, 256)\n",
              "  (decoder): LSTM(256, 200, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (cls): Linear(in_features=200, out_features=9000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhfN2mBwQMec",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvsdjak3PRjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_op = optim.Adam(model.parameters() ,lr=3e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx5c2kz4QPI4",
        "colab_type": "code",
        "outputId": "6b19f0fa-0ea9-4486-e35e-71b7bbcd0a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "epochs = 5\n",
        "loss_hist = []\n",
        "loss_ = 3\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_mean_loss = []\n",
        "\n",
        "    for i , (q,a) in enumerate(train_loader):\n",
        "        q = Variable(q).cuda()\n",
        "        a = Variable(a).cuda()\n",
        "   \n",
        "        logits = model(q,a,feed_previous=False)\n",
        "        _,predict = logits.max(1)\n",
        "        \n",
        "        loss = F.cross_entropy(logits ,a.view(-1))\n",
        "        train_op.zero_grad()\n",
        "        loss.backward()\n",
        "        train_op.step()\n",
        "        \n",
        "        epoch_mean_loss.append(loss.data.item())\n",
        "    \n",
        "    loss_ = np.mean(epoch_mean_loss)\n",
        "    loss_hist.append(loss_)\n",
        "    if epoch % 10 == 0  or epoch == epochs-1:\n",
        "        print(\"epoch:%s , loss:%s\" % (epoch , loss_ ))\n",
        "    if epoch % 50 == 0 or epoch == epochs-1:\n",
        "        torch.save(model.state_dict() , 'pth/model_male_epo%s.pth'%epoch) #save model\n",
        "        \n",
        "np.save('loss_male_epo%s.npy'%epochs,loss_hist)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0 , loss:2.1695852559380215\n",
            "epoch:4 , loss:1.7801758587409198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jir6ULXWQWrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,predict = logits.max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diAyTKIpQrTh",
        "colab_type": "text"
      },
      "source": [
        "### Print Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhO1QLxnQper",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self,idx2word,word2idx):\n",
        "        self.idx2word = idx2word\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = 25\n",
        "        self.eos_idx = 8002\n",
        "        self.EN_WHITELIST  = '0123456789abcdefghijklmnopqrstuvwxyz '             \n",
        "            \n",
        "    '''\n",
        "    idx -> word with EOS\n",
        "    '''        \n",
        "    def decode_line(self,sentence_idx,remove_pad=True,remove_eos=True):  #sentence_idx: 1d_matrix     \n",
        "        sentence = []\n",
        "        for w in sentence_idx:\n",
        "            if remove_eos and w==self.eos_idx:\n",
        "                continue\n",
        "            if remove_pad and w==0 : \n",
        "                continue\n",
        "            sentence.append(self.idx2word[w])\n",
        "            #if w==self.eos_idx:\n",
        "            #    break\n",
        "        sentence = ' '.join(sentence)\n",
        "        return sentence\n",
        "    \n",
        "    def decode(self,sentence_idxs,remove_pad=True,remove_eos=True): #sentence_idxs: 2d_matrix \n",
        "        sentences = []\n",
        "        for s in sentence_idxs: \n",
        "            sentences.append(self.decode_line(s,\n",
        "                                              remove_pad=remove_pad,\n",
        "                                              remove_eos=remove_eos))\n",
        "        return sentences\n",
        "            \n",
        "    '''\n",
        "    word -> idx with EOS\n",
        "    '''\n",
        "    def encode_line(self,sentence):  #sentence: 1d_matrix\n",
        "        sentence = sentence.lower()\n",
        "        s_list = ''.join([ ch for ch in sentence if ch in self.EN_WHITELIST ]).split()\n",
        "        sentence_idx = []\n",
        "        for w in s_list:\n",
        "            sentence_idx.append(self.word2idx[w])\n",
        "        n = len(sentence_idx)\n",
        "        if  n > self.max_len:\n",
        "            sentence_idx = sentence_idx[:self.max_len] \n",
        "        elif n < self.max_len:\n",
        "            sentence_idx = sentence_idx + [self.eos_idx] + [0]*(self.max_len-n-1)  \n",
        "        return sentence_idx\n",
        "    \n",
        "    def encode(self,sentences): #sentences: 2d_matrix   \n",
        "        sentence_idxs = []\n",
        "        for s in sentences: \n",
        "            sentence_idxs.append(self.encode_line(s))\n",
        "        return np.array(sentence_idxs)\n",
        "    \n",
        "    def print_QA(self, ques , pred_ans, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[2])\n",
        "            print('pred A :'+sents[1]) \n",
        "            \n",
        "    def print_QA_1(self, ques , pred_ans_train, pred_ans_test, strd_ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i],  pred_ans_train[i], pred_ans_test[i] , strd_ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[3])\n",
        "            print('train A:'+sents[1])    \n",
        "            print('test A :'+sents[2]) \n",
        "            \n",
        "    def print_QA_2(self, ques , ans):\n",
        "        n = len(ques)\n",
        "        for i in range(n):\n",
        "            idxs = [ ques[i], ans[i]]\n",
        "            sents = vocab.decode(idxs)\n",
        "            print('\\nQ      :'+sents[0])  \n",
        "            print('A      :'+sents[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmkfh_1aQt6y",
        "colab_type": "code",
        "outputId": "7371915d-171b-44e0-fcc4-d6479d54578f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([175])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V17ZAyNDQvWp",
        "colab_type": "code",
        "outputId": "01655c0d-8639-47ba-aa32-e9312e92d044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n = 175/25\n",
        "n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGCixp6zQxsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocab(metadata['idx2w'] , metadata['w2idx'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "livc4AL_QzPA",
        "colab_type": "text"
      },
      "source": [
        "### Try train corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IflRyfITQ5Ap",
        "colab_type": "code",
        "outputId": "2969987a-8723-407e-85c6-180775c58118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "pred_ans = predict.cpu().view(-1,n).data.numpy().T #predicted answer in train phase\n",
        "strd_ans = a.cpu().view(-1,n).data.numpy().T #standard answer\n",
        "ques     = q.cpu().view(-1,n).data.numpy().T #quenstions\n",
        "vocab.print_QA(ques , pred_ans, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-851016d63976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m#predicted answer in train phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstrd_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m#standard answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mques\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m#quenstions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_QA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mques\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpred_ans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrd_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: view(): argument 'size' must be tuple of ints, but found element of type float at pos 2"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwJMC8onRGRZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Try test corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ9J1h3yQ9Nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "o = model(q,a,feed_previous=True) #logits\n",
        "_,predict_test = o.max(1)\n",
        "#vocab.decode(predict_test.cpu().view(-1,10).data.numpy().T,remove_eos=False,remove_pad=False)\n",
        "pred_ans_test = predict_test.cpu().view(-1,n).data.numpy().T #predicted answer in test phase\n",
        "vocab.print_QA_1(ques , pred_ans, pred_ans_test, strd_ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvgEIEicRKtn",
        "colab_type": "text"
      },
      "source": [
        "### Try chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vlXBrx3RIPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "lines = []\n",
        "lines.append( 'you can do it'  )\n",
        "lines.append( 'how are you'    )\n",
        "lines.append( 'fuck you'  )\n",
        "lines.append( 'jesus christ you scared the shit out of me'  )\n",
        "lines.append( 'youre terrible'  )\n",
        "lines.append( 'is something wrong' )\n",
        "lines.append( 'nobodys gonna get inside' )\n",
        "lines.append( 'im sorry'  )\n",
        "lines.append( 'shut up'  )\n",
        "N = len(lines)\n",
        "lines = vocab.encode(lines)\n",
        "q_o = Variable(torch.from_numpy(lines).long()).cuda()\n",
        "#vocab.decode(vocab.encode(lines))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSkj5gmhRMj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "o = model(q_o,a[:N],feed_previous=True)\n",
        "_,predict_o = o.max(1)\n",
        "#vocab.decode(predict_o.cpu().view(-1,3).data.numpy().T)\n",
        "pred_ans_o = predict_o.cpu().view(-1,N).data.numpy().T #predicted answer \n",
        "vocab.print_QA_2(lines, pred_ans_o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZK7FozRQFQ",
        "colab_type": "text"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNT_sm-lROqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3H_Lw2lRRyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#np.save('loss_female_prob_feed_epo141.npy',epoch_mean_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHuRKc-vRUUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#torch.save(model.state_dict() , 'model_female_prob_feed_epo141.pth') #save model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOH2N5bVRVta",
        "colab_type": "text"
      },
      "source": [
        "### Back up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVm8jt3PRXzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJb7Zr3-RZdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Variable(torch.rand(3,25)*200).long()\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-ALOhNPRbZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,p = model(x,q,feed_previous=True).max(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfPhKo2QRdBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.decode(p.view(3,25).data.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}