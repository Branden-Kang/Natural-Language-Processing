{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjxL6KZcJeEcDb++0l4lK7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://towardsdatascience.com/summarize-a-text-with-python-b3b260c60e72)"
      ],
      "metadata": {
        "id": "KjW4hS5g2jWT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izVy4jmi1DOP",
        "outputId": "8f779084-b75f-4efe-9cce-1f06b489c0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk import tokenize, word_tokenize\n",
        "\n",
        "# with open(\"stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "#      text = \" \".join(f.readlines())\n",
        "# STOP_WORDS = set(text.split())\n",
        "\n",
        "# def summarize(text, no_sentences=3):\n",
        "#     word_weights={}\n",
        "#     for word in word_tokenize(text):\n",
        "#         word = word.lower()\n",
        "#         if len(word) > 1 and word not in STOP_WORDS:\n",
        "#             if word in word_weights.keys():            \n",
        "#                 word_weights[word] += 1\n",
        "#             else:\n",
        "#                 word_weights[word] = 1\n",
        "\n",
        "#     sentence_weights={}\n",
        "#     for sent in tokenize.sent_tokenize(text):\n",
        "#         for word in word_tokenize(sent) :  \n",
        "#             word = word.lower()\n",
        "#             if word in word_weights.keys():            \n",
        "#                 if sent in sentence_weights.keys():\n",
        "#                     sentence_weights[sent] += word_weights[word]\n",
        "#                 else:\n",
        "#                     sentence_weights[sent] = word_weights[word]\n",
        "#     highest_weights = sorted(sentence_weights.values())[-no_sentences:]\n",
        "\n",
        "#     summary=\"\"\n",
        "#     for sentence,strength in sentence_weights.items():  \n",
        "#         if strength in highest_weights:\n",
        "#             summary += sentence + \" \"\n",
        "#     summary = summary.replace('_', ' ').strip()\n",
        "#     return summary"
      ],
      "metadata": {
        "id": "bKt-H2ih205A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"longtext.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "#      text = \" \".join(f.readlines())\n",
        "\n",
        "# sentences = []\n",
        "# for sent in tokenize.sent_tokenize():\n",
        "#     sentences.append(sent)\n",
        "# chunks = [sentences[x:x+50] for x in range(0, len(sentences), 50)]\n",
        "\n",
        "# summary = []\n",
        "# for c in chunks:\n",
        "#     summary.append(unidecode.unidecode(summarize(' '.join(c))))\n",
        "# summary = \" \".join(summary)"
      ],
      "metadata": {
        "id": "p4OScRVi3Bre"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}