{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised text classification model in Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxGO9nVow8U1OlqWBgHLLw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQtJLbi74Wpo",
        "colab_type": "text"
      },
      "source": [
        "[Reference: Part 1](https://towardsdatascience.com/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96) <br>\n",
        "[Reference: Part 2](https://towardsdatascience.com/introduction-to-nlp-part-2-difference-between-lemmatisation-and-stemming-3789be1c55bc) <br>\n",
        "[Reference: Part 3](https://towardsdatascience.com/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc)<br>\n",
        "[Reference: Part 4](https://towardsdatascience.com/introduction-to-nlp-part-4-supervised-text-classification-model-in-python-96e9709b4267#%20https://stackoverflow.com/questions/46109166/converting-categorizedplaintextcorpusreader-into-dataframe)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YkB2drl7uf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import movie_reviews, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IorE7gN74VJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "25275db1-a647-4d79-8a84-958715dc4181"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "nltk.download('wordnet')\n",
        "nltk.download('movie_reviews')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkJaExnt7mKv",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Preprocessing text in Python\n",
        "- Tokenise\n",
        "- Normalise\n",
        "- Remove stopwords\n",
        "- Count vectorise\n",
        "- Transform to tf-idf representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FK4B4P38LBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "part1 = \"\"\"We are gathered here today on this joyous occasion to celebrate the special love that Monica and Chandler share. It is a love based on giving and receiving as well as having and sharing. And the love that they give and have is shared and received. And\n",
        "through this having and giving and sharing and receiving, we too can share and love and have... and receive.\"\"\"\n",
        "part2 = \"\"\"When I think of the love these two givers and receivers share I cannot help but envy the lifetime ahead of having and loving and giving and receiving.\"\"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3HkDnR38QM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataframe\n",
        "X_train = pd.DataFrame([part1, part2], columns=['speech'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3kw42-s8ZAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenise words while ignoring punctuation\n",
        "    tokeniser = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokeniser.tokenize(text)\n",
        "    \n",
        "    # Lowercase and lemmatise \n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
        "    \n",
        "    # Remove stopwords\n",
        "    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
        "    return keywords"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcCkwSJ2_fSL",
        "colab_type": "text"
      },
      "source": [
        "STEP 1: TOKENISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va95CcW-9lvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff979638-9078-4f69-d42f-6639a88e054b"
      },
      "source": [
        "# Import module\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Create an instance of RegexpTokenizer for alphanumeric tokens\n",
        "tokeniser = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Tokenise 'part1' string\n",
        "tokens = tokeniser.tokenize(part1)\n",
        "print(tokens)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We', 'are', 'gathered', 'here', 'today', 'on', 'this', 'joyous', 'occasion', 'to', 'celebrate', 'the', 'special', 'love', 'that', 'Monica', 'and', 'Chandler', 'share', 'It', 'is', 'a', 'love', 'based', 'on', 'giving', 'and', 'receiving', 'as', 'well', 'as', 'having', 'and', 'sharing', 'And', 'the', 'love', 'that', 'they', 'give', 'and', 'have', 'is', 'shared', 'and', 'received', 'And', 'through', 'this', 'having', 'and', 'giving', 'and', 'sharing', 'and', 'receiving', 'we', 'too', 'can', 'share', 'and', 'love', 'and', 'have', 'and', 'receive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB3JK92__c0b",
        "colab_type": "text"
      },
      "source": [
        "STEP 2. NORMALISE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "667PULeg9xsZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807e411d-0ef1-4313-f741-5380a3c4d660"
      },
      "source": [
        "# Import module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create an instance of WordNetLemmatizer\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "\n",
        "# Lowercase and lemmatise tokens\n",
        "lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
        "print(lemmas)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['we', 'be', 'gather', 'here', 'today', 'on', 'this', 'joyous', 'occasion', 'to', 'celebrate', 'the', 'special', 'love', 'that', 'monica', 'and', 'chandler', 'share', 'it', 'be', 'a', 'love', 'base', 'on', 'give', 'and', 'receive', 'as', 'well', 'as', 'have', 'and', 'share', 'and', 'the', 'love', 'that', 'they', 'give', 'and', 'have', 'be', 'share', 'and', 'receive', 'and', 'through', 'this', 'have', 'and', 'give', 'and', 'share', 'and', 'receive', 'we', 'too', 'can', 'share', 'and', 'love', 'and', 'have', 'and', 'receive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI_CdyaJ_Zx7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c88c142-e9ae-4698-ec5b-e252bb34737c"
      },
      "source": [
        "# Check how many words we have\n",
        "len(lemmas)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7TOH8Px_jgg",
        "colab_type": "text"
      },
      "source": [
        "STEP 3. REMOVE STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzRdqxeD_nKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d965a1-4d7d-429a-dc57-5dd13e35991d"
      },
      "source": [
        "# Import module\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Check out how many stopwords there are \n",
        "print(len(stopwords.words('english')))\n",
        "\n",
        "# See first 5 stopwords\n",
        "stopwords.words('english')[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dowrOnZA_qXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ed8b3b0d-fdbf-49ab-bc07-627f57a96e46"
      },
      "source": [
        "keywords = [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
        "print(keywords)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gather', 'today', 'joyous', 'occasion', 'celebrate', 'special', 'love', 'monica', 'chandler', 'share', 'love', 'base', 'give', 'receive', 'well', 'share', 'love', 'give', 'share', 'receive', 'give', 'share', 'receive', 'share', 'love', 'receive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNPQyF-r_v1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0cf0083-b605-490b-e522-39e2517e9eb5"
      },
      "source": [
        "# Check how many words we have\n",
        "len(keywords)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu9AevNCA0m5",
        "colab_type": "text"
      },
      "source": [
        "STEP 4. COUNT VECTORISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gseYGWOlA1Um",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8bc25b47-14d2-4ed2-df68-ff7d771f86e4"
      },
      "source": [
        "{word: keywords.count(word) for word in set(keywords)}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'base': 1,\n",
              " 'celebrate': 1,\n",
              " 'chandler': 1,\n",
              " 'gather': 1,\n",
              " 'give': 3,\n",
              " 'joyous': 1,\n",
              " 'love': 4,\n",
              " 'monica': 1,\n",
              " 'occasion': 1,\n",
              " 'receive': 4,\n",
              " 'share': 5,\n",
              " 'special': 1,\n",
              " 'today': 1,\n",
              " 'well': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO7Hr0QlA94S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import module\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Create an instance of CountfVectorizer\n",
        "vectoriser = CountVectorizer(analyzer=preprocess_text)\n",
        "# Fit to the data and transform to feature matrix\n",
        "X_train = vectoriser.fit_transform(X_train['speech'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxg_QIqmBN4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "6e59a858-d563-4b8d-9c83-da5b9462edf5"
      },
      "source": [
        "# Convert sparse matrix to dataframe\n",
        "X_train = pd.DataFrame.sparse.from_spmatrix(X_train)# Save mapping on which index refers to which terms\n",
        "col_map = {v:k for k, v in vectoriser.vocabulary_.items()}# Rename each column using the mapping\n",
        "for col in X_train.columns:\n",
        "    X_train.rename(columns={col: col_map[col]}, inplace=True)\n",
        "X_train"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ahead</th>\n",
              "      <th>base</th>\n",
              "      <th>cannot</th>\n",
              "      <th>celebrate</th>\n",
              "      <th>chandler</th>\n",
              "      <th>envy</th>\n",
              "      <th>gather</th>\n",
              "      <th>give</th>\n",
              "      <th>givers</th>\n",
              "      <th>help</th>\n",
              "      <th>joyous</th>\n",
              "      <th>lifetime</th>\n",
              "      <th>love</th>\n",
              "      <th>monica</th>\n",
              "      <th>occasion</th>\n",
              "      <th>receive</th>\n",
              "      <th>receivers</th>\n",
              "      <th>share</th>\n",
              "      <th>special</th>\n",
              "      <th>think</th>\n",
              "      <th>today</th>\n",
              "      <th>two</th>\n",
              "      <th>well</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ahead  base  cannot  celebrate  chandler  ...  special  think  today  two  well\n",
              "0      0     1       0          1         1  ...        1      0      1    0     1\n",
              "1      1     0       1          0         0  ...        0      1      0    1     0\n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqvNxh3iBRzp",
        "colab_type": "text"
      },
      "source": [
        "STEP 5. TRANSFORM TO TF-IDF REPRESENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1PQgIFcBUsc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24cd3ca-9e32-4915-c79b-f480262b573d"
      },
      "source": [
        "# Import module\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Create an instance of TfidfTransformer\n",
        "transformer = TfidfTransformer()\n",
        "\n",
        "# Fit to the data and transform to tf-idf\n",
        "X_train = pd.DataFrame(transformer.fit_transform(X_train).toarray(), columns=X_train.columns)\n",
        "X_train"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ahead</th>\n",
              "      <th>base</th>\n",
              "      <th>cannot</th>\n",
              "      <th>celebrate</th>\n",
              "      <th>chandler</th>\n",
              "      <th>envy</th>\n",
              "      <th>gather</th>\n",
              "      <th>give</th>\n",
              "      <th>givers</th>\n",
              "      <th>help</th>\n",
              "      <th>joyous</th>\n",
              "      <th>lifetime</th>\n",
              "      <th>love</th>\n",
              "      <th>monica</th>\n",
              "      <th>occasion</th>\n",
              "      <th>receive</th>\n",
              "      <th>receivers</th>\n",
              "      <th>share</th>\n",
              "      <th>special</th>\n",
              "      <th>think</th>\n",
              "      <th>today</th>\n",
              "      <th>two</th>\n",
              "      <th>well</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.323963</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.431951</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.431951</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.539939</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200894</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.401788</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200894</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.200894</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ahead      base   cannot  celebrate  ...    think     today      two      well\n",
              "0  0.00000  0.151773  0.00000   0.151773  ...  0.00000  0.151773  0.00000  0.151773\n",
              "1  0.28235  0.000000  0.28235   0.000000  ...  0.28235  0.000000  0.28235  0.000000\n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8B0qPQhBtWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataframe\n",
        "X_train = pd.DataFrame([part1, part2], columns=['speech'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSwOkZKx9Uuc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7778d579-945d-4908-d41b-ed0fe25e3bf9"
      },
      "source": [
        "# Create an instance of TfidfVectorizer\n",
        "vectoriser = TfidfVectorizer(analyzer=preprocess_text)\n",
        "\n",
        "# Fit to the data and transform to feature matrix\n",
        "X_train = vectoriser.fit_transform(X_train['speech'])\n",
        "\n",
        "# Convert sparse matrix to dataframe\n",
        "X_train = pd.DataFrame.sparse.from_spmatrix(X_train)\n",
        "\n",
        "# Save mapping on which index refers to which words\n",
        "col_map = {v:k for k, v in vectoriser.vocabulary_.items()}\n",
        "\n",
        "# Rename each column using the mapping\n",
        "for col in X_train.columns:\n",
        "    X_train.rename(columns={col: col_map[col]}, inplace=True)\n",
        "X_train"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ahead</th>\n",
              "      <th>base</th>\n",
              "      <th>cannot</th>\n",
              "      <th>celebrate</th>\n",
              "      <th>chandler</th>\n",
              "      <th>envy</th>\n",
              "      <th>gather</th>\n",
              "      <th>give</th>\n",
              "      <th>givers</th>\n",
              "      <th>help</th>\n",
              "      <th>joyous</th>\n",
              "      <th>lifetime</th>\n",
              "      <th>love</th>\n",
              "      <th>monica</th>\n",
              "      <th>occasion</th>\n",
              "      <th>receive</th>\n",
              "      <th>receivers</th>\n",
              "      <th>share</th>\n",
              "      <th>special</th>\n",
              "      <th>think</th>\n",
              "      <th>today</th>\n",
              "      <th>two</th>\n",
              "      <th>well</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.323963</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.431951</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.431951</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.539939</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200894</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.401788</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200894</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.200894</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.28235</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ahead      base   cannot  celebrate  ...    think     today      two      well\n",
              "0  0.00000  0.151773  0.00000   0.151773  ...  0.00000  0.151773  0.00000  0.151773\n",
              "1  0.28235  0.000000  0.28235   0.000000  ...  0.28235  0.000000  0.28235  0.000000\n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxwvmK0iBhY0",
        "colab_type": "text"
      },
      "source": [
        "More efficient way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex9m7aMRBmgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataframe\n",
        "X_train = pd.DataFrame([part1, part2], columns=['speech'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIIZcnrIBg1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create an instance of TfidfVectorizer\n",
        "vectoriser = TfidfVectorizer(analyzer=preprocess_text)\n",
        "\n",
        "# Fit to the data and transform to tf-idf\n",
        "X_train = vectoriser.fit_transform(X_train['speech'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VvKfY94FGc0",
        "colab_type": "text"
      },
      "source": [
        "# Part2: Difference between lemmatisation and stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsU8MvG4FK_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import packages\n",
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Instantiate stemmers and lemmatiser\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "\n",
        "# Create function that normalises text using all three techniques\n",
        "def normalise_text(words):\n",
        "    \"\"\"Stem and lemmatise each word in a list. Return output in a dataframe.\"\"\"\n",
        "    normalised_text = pd.DataFrame(index=words, columns=['Porter', 'Lancaster', 'Lemmatiser'])\n",
        "    for word in words:\n",
        "        normalised_text.loc[word,'Porter'] = porter.stem(word)\n",
        "        normalised_text.loc[word,'Lancaster'] = lancaster.stem(word)\n",
        "        normalised_text.loc[word,'Lemmatiser'] = lemmatiser.lemmatize(word, pos='v')\n",
        "    return normalised_text"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay25nfFwFOxy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "853a6c17-f6e8-4853-90e7-5b7d2c9c4980"
      },
      "source": [
        "normalise_text([\"stemming\", \"lemmatisation\"])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Porter</th>\n",
              "      <th>Lancaster</th>\n",
              "      <th>Lemmatiser</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>stemming</th>\n",
              "      <td>stem</td>\n",
              "      <td>stem</td>\n",
              "      <td>stem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lemmatisation</th>\n",
              "      <td>lemmatis</td>\n",
              "      <td>lem</td>\n",
              "      <td>lemmatisation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Porter Lancaster     Lemmatiser\n",
              "stemming           stem      stem           stem\n",
              "lemmatisation  lemmatis       lem  lemmatisation"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ianOtqXRFi4x",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: TF-IDF explained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQv7q80SFtBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1 = 'I thought, I thought of thinking of thanking you for the gift'\n",
        "d2 = 'She was thinking of going to go and get you a GIFT!'"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhH9LW1ZFuX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataframe\n",
        "X_train = pd.DataFrame({'text': [d1, d2]})"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwedr8sEFyL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenise words while ignoring punctuation\n",
        "    tokeniser = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokeniser.tokenize(text)\n",
        "    \n",
        "    # Lowercase and lemmatise \n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
        "    \n",
        "    # Remove stopwords\n",
        "    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
        "    return keywords"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWS_1NWjF0Lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1 = ['think', 'think', 'think', 'thank,' 'gift']\n",
        "d2 = ['think', 'go', 'go', 'get', 'gift']"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm4Vu2EHGBBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "ca633614-b6a2-4931-f385-17638a9f3bcd"
      },
      "source": [
        "# Create an instance of TfidfVectorizer\n",
        "vectoriser = TfidfVectorizer(analyzer=preprocess_text)\n",
        "\n",
        "# Fit to the data and transform to feature matrix\n",
        "X_train = vectoriser.fit_transform(X_train['text'])\n",
        "\n",
        "# Convert sparse matrix to dataframe\n",
        "X_train = pd.DataFrame.sparse.from_spmatrix(X_train)\n",
        "\n",
        "# Save mapping on which index refers to which words\n",
        "col_map = {v:k for k, v in vectoriser.vocabulary_.items()}\n",
        "\n",
        "# Rename each column using the mapping\n",
        "for col in X_train.columns:\n",
        "    X_train.rename(columns={col: col_map[col]}, inplace=True)\n",
        "X_train"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>get</th>\n",
              "      <th>gift</th>\n",
              "      <th>go</th>\n",
              "      <th>thank</th>\n",
              "      <th>think</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.288972</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.40614</td>\n",
              "      <td>0.866917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.407824</td>\n",
              "      <td>0.290170</td>\n",
              "      <td>0.815648</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.290170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        get      gift        go    thank     think\n",
              "0  0.000000  0.288972  0.000000  0.40614  0.866917\n",
              "1  0.407824  0.290170  0.815648  0.00000  0.290170"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOSllEMwGJO1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "98375aa6-05e8-4afb-a0fc-0d50c1247bd5"
      },
      "source": [
        "d3 = \"He thinks he will go!\"\n",
        "d4 = \"They don’t know what to buy!\"\n",
        "\n",
        "# Create dataframe\n",
        "X_test = pd.DataFrame({'text': [d3, d4]})\n",
        "\n",
        "# Transform to feature matrix\n",
        "X_test = vectoriser.transform(X_test['text'])\n",
        "\n",
        "# Convert sparse matrix to dataframe\n",
        "X_test = pd.DataFrame.sparse.from_spmatrix(X_test)\n",
        "# Add column names to make it more readible\n",
        "for col in X_test.columns:\n",
        "    X_test.rename(columns={col: col_map[col]}, inplace=True)\n",
        "X_test"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>get</th>\n",
              "      <th>gift</th>\n",
              "      <th>go</th>\n",
              "      <th>thank</th>\n",
              "      <th>think</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.814802</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.579739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   get  gift        go  thank     think\n",
              "0  0.0   0.0  0.814802    0.0  0.579739\n",
              "1  0.0   0.0  0.000000    0.0  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjAkMB2hGXDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d3 = ['think', 'go'] # vectoritiser is familiar with these terms\n",
        "d4 = ['know', 'buy'] # vectoritiser is not familiar with these terms"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRdn9AqpGiAL",
        "colab_type": "text"
      },
      "source": [
        "# Part4: Supervised text classification model in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwRn2smm6CUm",
        "colab_type": "text"
      },
      "source": [
        "## 1. Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH7qO3MW4d1v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df287e5-84a0-4e1c-dc01-9b99d185070b"
      },
      "source": [
        "reviews = []\n",
        "for fileid in movie_reviews.fileids():\n",
        "    tag, filename = fileid.split('/')\n",
        "    reviews.append((tag, movie_reviews.raw(fileid)))\n",
        "sample = pd.DataFrame(reviews, columns=['target', 'document'])\n",
        "print(f'Dimensions: {sample.shape}')\n",
        "sample.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions: (2000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neg</td>\n",
              "      <td>plot : two teen couples go to a church party ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neg</td>\n",
              "      <td>the happy bastard's quick movie review \\ndamn ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neg</td>\n",
              "      <td>it is movies like these that make a jaded movi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>neg</td>\n",
              "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neg</td>\n",
              "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  target                                           document\n",
              "0    neg  plot : two teen couples go to a church party ,...\n",
              "1    neg  the happy bastard's quick movie review \\ndamn ...\n",
              "2    neg  it is movies like these that make a jaded movi...\n",
              "3    neg   \" quest for camelot \" is warner bros . ' firs...\n",
              "4    neg  synopsis : a mentally unstable man undergoing ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBWqOxuw5abB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0c98a4f9-1684-41f4-9edd-cdff68d2c64e"
      },
      "source": [
        "sample['target'].value_counts()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pos    1000\n",
              "neg    1000\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHBk9SgU5dyk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6a59fed0-0eed-410a-ee66-be3b94f2e438"
      },
      "source": [
        "sample['target'] = np.where(sample['target']=='pos', 1, 0)\n",
        "sample['target'].value_counts()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1000\n",
              "0    1000\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF8O0vE659W5",
        "colab_type": "text"
      },
      "source": [
        "## 2. Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10p-YnHR5iey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "403b8cad-ed77-4d6a-e0b3-a843bb8237e1"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(sample['document'], sample['target'], test_size=0.3, random_state=123)\n",
        "\n",
        "print(f'Train dimensions: {X_train.shape, y_train.shape}')\n",
        "print(f'Test dimensions: {X_test.shape, y_test.shape}')\n",
        "\n",
        "# Check out target distribution\n",
        "print(y_train.value_counts())\n",
        "print(y_test.value_counts())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dimensions: ((1400,), (1400,))\n",
            "Test dimensions: ((600,), (600,))\n",
            "1    700\n",
            "0    700\n",
            "Name: target, dtype: int64\n",
            "1    300\n",
            "0    300\n",
            "Name: target, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEgEQaOO52JX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8127876d-4750-4788-b755-7360721135f9"
      },
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenise words while ignoring punctuation\n",
        "    tokeniser = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokeniser.tokenize(text)\n",
        "    \n",
        "    # Lowercase and lemmatise \n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
        "    \n",
        "    # Remove stopwords\n",
        "    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
        "    return keywords\n",
        "    \n",
        "# Create an instance of TfidfVectorizer\n",
        "vectoriser = TfidfVectorizer(analyzer=preprocess_text)\n",
        "\n",
        "# Fit to the data and transform to feature matrix\n",
        "X_train_tfidf = vectoriser.fit_transform(X_train)\n",
        "X_train_tfidf.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1400, 27676)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE7CyCnB6Q0q",
        "colab_type": "text"
      },
      "source": [
        "## 3. Let's do modelling\n",
        "\n",
        "Let’s build a baseline model using Stochastic Gradient Descent Classifier. I have chosen this classifier because it is fast and works well with sparse matrix. Using 5-fold cross validation, let’s fit the model to the data and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXfkvVVW6MrX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9eb716d1-fc99-4be1-8acf-b95e08a9842a"
      },
      "source": [
        "sgd_clf = SGDClassifier(random_state=123)\n",
        "sgf_clf_scores = cross_val_score(sgd_clf, X_train_tfidf, y_train, cv=5)\n",
        "\n",
        "print(sgf_clf_scores)\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (sgf_clf_scores.mean(), sgf_clf_scores.std() * 2))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.82857143 0.85       0.84285714 0.81785714 0.81428571]\n",
            "Accuracy: 0.83 (+/- 0.03)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je4OOqvw6dFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "beaab03c-219f-4ba4-e879-c91ecaffc598"
      },
      "source": [
        "sgf_clf_pred = cross_val_predict(sgd_clf, X_train_tfidf, y_train, cv=5)\n",
        "print(confusion_matrix(y_train, sgf_clf_pred))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[580 120]\n",
            " [117 583]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LReADT2U6nNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "75e69c44-3e4c-4f2b-b507-696730e1c4e9"
      },
      "source": [
        "grid = {'fit_intercept': [True,False],\n",
        "        'early_stopping': [True, False],\n",
        "        'loss' : ['hinge', 'log', 'squared_hinge'],\n",
        "        'penalty' : ['l2', 'l1', 'none']}\n",
        "search = GridSearchCV(estimator=sgd_clf, param_grid=grid, cv=5)\n",
        "search.fit(X_train_tfidf, y_train)\n",
        "search.best_params_"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'early_stopping': False,\n",
              " 'fit_intercept': False,\n",
              " 'loss': 'log',\n",
              " 'penalty': 'l1'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frby5okl6qEm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cc9d867e-3fe8-46d3-a60b-2a11ddeb76ba"
      },
      "source": [
        "grid_sgd_clf_scores = cross_val_score(search.best_estimator_, X_train_tfidf, y_train, cv=5)\n",
        "print(grid_sgd_clf_scores)\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (grid_sgd_clf_scores.mean(), grid_sgd_clf_scores.std() * 2))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.85       0.85714286 0.83571429 0.84285714 0.82857143]\n",
            "Accuracy: 0.84 (+/- 0.02)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RzsJfSQ6wxb",
        "colab_type": "text"
      },
      "source": [
        "## 4. Finalize the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iwElIRS6u_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "d620e1c0-8fb0-48f9-f0cc-6de658f0bec5"
      },
      "source": [
        "pipe = Pipeline([('vectoriser', vectoriser),\n",
        "                 ('classifier', search.best_estimator_)])\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectoriser',\n",
              "                 TfidfVectorizer(analyzer=<function preprocess_text at 0x7f0377222d90>,\n",
              "                                 binary=False, decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents...\n",
              "                ('classifier',\n",
              "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
              "                               fit_intercept=False, l1_ratio=0.15,\n",
              "                               learning_rate='optimal', loss='log',\n",
              "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "                               penalty='l1', power_t=0.5, random_state=123,\n",
              "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
              "                               verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbGRiUYz61pQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a70ff4c6-fe5a-45b0-b13c-bd6538f61fd2"
      },
      "source": [
        "y_test_pred = pipe.predict(X_test)\n",
        "print(\"Accuracy: %0.2f\" % (accuracy_score(y_test, y_test_pred)))\n",
        "print(confusion_matrix(y_test, y_test_pred))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.85\n",
            "[[249  51]\n",
            " [ 37 263]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}