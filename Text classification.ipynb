{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assignment0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bjZFXTIo90Ho",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pwd\n",
        "os.chdir('gdrive/My Drive/research/Datasets/Amazon_UCSD')\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "StRFU3z5-zG0",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data import Field, LabelField\n",
        "from torchtext import data, datasets\n",
        "from torchtext.data import TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "def split_save_data(path_to_csv, path_to_split):\n",
        "\tcorpus = pd.read_csv(path_to_csv)\n",
        "\n",
        "\t# create small datasets for quick tweaking\n",
        "\ttrain_small = corpus[:16000]\n",
        "\tvalid_small = corpus[16000:20000]\n",
        "\ttest_small = corpus[20000:25000]\n",
        "\t# write to files\n",
        "\ttrain_small.to_csv(path_to_split + '/train_small.csv', index=False)\n",
        "\tvalid_small.to_csv(path_to_split + '/valid_small.csv', index=False)\n",
        "\ttest_small.to_csv(path_to_split + '/test_small.csv', index=False)\n",
        "\n",
        "class BatchWrapper:\n",
        "    def __init__(self, iterator, x_var, y_var):\n",
        "        self.iterator, self.x_var, self.y_var = iterator, x_var, y_var # we pass in the list of attributes for x \n",
        "        print (self.y_var)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in self.iterator:\n",
        "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
        "            y = getattr(batch, self.y_var)\n",
        "            y[y < 4] = 0\n",
        "            y[y >= 4] = 1\n",
        "            yield (x, y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.iterator)\n",
        "\n",
        "def read_test_valid(path_to_split, device = 'cpu'):\n",
        "\n",
        "\tBATCH_SIZE = 64\n",
        "\tMAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "\t# a very simple tokenizer\n",
        "\ttokenize = lambda x:x.split()\n",
        "\n",
        "\t# Fields from torchtext: specifying how to process each field in the CSV files\n",
        "\n",
        "\tTEXT = Field(sequential=True,\n",
        "             tokenize=tokenize,\n",
        "             lower=True,\n",
        "             include_lengths=True)\n",
        "\n",
        "\tLABEL = Field(sequential=False,\n",
        "\t             use_vocab=False,\n",
        "\t             dtype = torch.long)\n",
        "\n",
        "\tfields = [('reviewText', TEXT), ('overall', LABEL)]\n",
        "\n",
        "\t# load train, validation, and test data all in once\n",
        "\ttrain_data, valid_data, test_data = TabularDataset.splits(\n",
        "\t            path='',\n",
        "\t            train=path_to_split + '/train_small.csv',\n",
        "\t            validation= path_to_split + '/valid_small.csv',\n",
        "\t            test = path_to_split + '/test_small.csv',\n",
        "\t            format='csv',\n",
        "\t            skip_header=True,\n",
        "\t            fields=fields)\n",
        "\n",
        "\t# the vocab can only be built from the training portion\n",
        "\tTEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "\tLABEL.build_vocab(train_data)\n",
        "\tvocab = TEXT.vocab\n",
        "\n",
        "\ttrain_iter = BucketIterator(\n",
        "\t    train_data,\n",
        "\t    batch_size = BATCH_SIZE,\n",
        "\t    device = device,\n",
        "\t    sort_key=lambda x: len(x.reviewText),\n",
        "\t    sort_within_batch=False,\n",
        "\t    repeat=False\n",
        "\t)\n",
        "\n",
        "\tvalid_iter = BucketIterator(\n",
        "\t    valid_data,\n",
        "\t    batch_size = BATCH_SIZE * 4,\n",
        "\t    device = device,\n",
        "\t    repeat = False\n",
        "\t)\n",
        "\n",
        "\ttest_iter = BucketIterator(\n",
        "        test_data,\n",
        "        batch_size = BATCH_SIZE * 4,\n",
        "        device = device,\n",
        "        repeat = False\n",
        "      )\n",
        "      \n",
        "\ttrain_iter = BatchWrapper(train_iter, 'reviewText', 'overall')\n",
        "\tvalid_iter = BatchWrapper(valid_iter, 'reviewText', 'overall')\n",
        "\ttest_iter = BatchWrapper(test_iter, 'reviewText', 'overall')\n",
        "\n",
        "\treturn train_iter, valid_iter, test_iter, vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7xcuJyYK_GtX",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, D_in, D_hidden, D_out, device='cpu'):\n",
        "        super(MLP, self).__init__()\n",
        "        self.vocab_size = D_in\n",
        "        self.D_in = D_in\n",
        "        self.D_hidden = D_hidden\n",
        "        self.D_out = D_out\n",
        "        \n",
        "        self.l1 = nn.Linear(D_in, D_hidden)\n",
        "        self.l2 = nn.Linear(D_hidden, int(D_hidden/2))\n",
        "        self.l3 = nn.Linear(int(D_hidden / 2), D_out)\n",
        "        \n",
        "        # using nn.Dropout rather than F.dropout. See https://stackoverflow.com/questions/53419474/nn-dropout-vs-f-dropout-pytorch\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        self.device = device\n",
        "    \n",
        "    def indexTensor2sparseTensor(self, text, text_length):\n",
        "        \"\"\"\n",
        "            Transform the document in each column in the batch to a sparse vector. Then pack them back in a sparse Tensor\n",
        "            text: max_len x batch_size\n",
        "            text_length: 1 x batch_size\n",
        "            return: vocab_size x batch_size\n",
        "        \"\"\"\n",
        "        values = []\n",
        "        indices = []\n",
        "        for col in range(text_length.size()[0]):\n",
        "            for i in range(text_length[col]):\n",
        "                values.append(1)\n",
        "                indices.append((text[i, col], col))\n",
        "        \n",
        "        indices = torch.LongTensor(indices).t()\n",
        "        values = torch.FloatTensor(values)\n",
        "        shape = (self.vocab_size, text_length.size()[0])\n",
        "        \n",
        "        return torch.sparse.FloatTensor(indices, values, torch.Size(shape))#.to_dense()\n",
        "    \n",
        "    def forward(self, text, text_length):\n",
        "        x = self.indexTensor2sparseTensor(text, text_length)\n",
        "        x = x.to(self.device)\n",
        "        \n",
        "        h = F.relu(self.l1(x.t()))\n",
        "        h = self.dropout(h)\n",
        "        \n",
        "        h = F.relu(self.l2(h))\n",
        "#         h = self.dropout(h)\n",
        "        \n",
        "        out = self.l3(h)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dBauOM0U_vrx",
        "outputId": "d15c4c4a-e166-4a65-a941-ef91c72c8747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "path_to_split = './'\n",
        "model_path = './'\n",
        "\n",
        "train_iter, valid_iter, test_iter, vocab = read_test_valid(path_to_split, device = device)\n",
        "\n",
        "HIDDEN_SIZE = 128\n",
        "num_classes = 2\n",
        "model = MLP(len(vocab), HIDDEN_SIZE, num_classes, device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2, weight_decay = 1e-5)\n",
        "loss_func = nn.CrossEntropyLoss(reduction = 'mean')\n",
        "\n",
        "model.to(device)\n",
        "loss_func.to(device)\n",
        "\n",
        "save_path = './best_model.pt'\n",
        "\n",
        "def train_validate(num_epoches = 2, save_path=model_path):\n",
        "    \n",
        "    # I don't how many epoches are enough, so we will track the best performing model\n",
        "    best_acc = 0\n",
        "    best_epoch = -1\n",
        "    to_save = {}\n",
        "    \n",
        "    for e in range(num_epoches):\n",
        "        # because of dropout layer, we turn training on\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for x, y in train_iter:\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            text, text_lengths = x\n",
        "            out = model(text, text_lengths)\n",
        "            \n",
        "            loss = loss_func(out, y)\n",
        "            epoch_loss += loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print('epoch = {}, loss = {}'.format(e, epoch_loss / len(train_iter)))\n",
        "        correct, total = 0, 0\n",
        "        \n",
        "        \n",
        "        # enter evaluation mode, no need for grads to make it run faster\n",
        "        with torch.no_grad():\n",
        "            # because of dropout layer, we turn training off\n",
        "            model.eval()\n",
        "            for val_x, val_y in valid_iter:\n",
        "                text, text_lengths = val_x\n",
        "                # dim of out is batch_size x num_classes\n",
        "                out = model(text, text_lengths)\n",
        "                correct += torch.max(out, 1)[1].eq(val_y).sum()\n",
        "                total += out.size()[0]\n",
        "            cur_acc = float(correct) / total\n",
        "            print('Validation accuracy = {}'.format(cur_acc))\n",
        "            \n",
        "            if cur_acc > best_acc:\n",
        "                best_acc = cur_acc\n",
        "                best_epoch = e\n",
        "                \n",
        "                # record the model state_dict() for saving later\n",
        "                to_save = {\n",
        "                    'epoch': e,\n",
        "                    'model_state_dict': model.state_dict()\n",
        "                }\n",
        "                \n",
        "    # report and save the best model\n",
        "    print(f'best epoch = {best_epoch} with best validation accuracy = {best_acc}')\n",
        "    torch.save(to_save, save_path + '/best_model.pt')\n",
        "\n",
        "def load_model(save_path):\n",
        "\n",
        "    model = MLP(len(vocab), HIDDEN_SIZE, num_classes, device = device)\n",
        "\n",
        "    checkpoint = torch.load(save_path + '/best_model.pt')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "\n",
        "    # move the model to GPU if has one\n",
        "    model.to(device)\n",
        "\n",
        "    # need this for dropout\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_model(model):\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        # because of dropout layer, we turn training off\n",
        "        model.eval()\n",
        "        print(next(model.parameters()).device)\n",
        "        for test_x, test_y in test_iter:\n",
        "            text, text_lengths = test_x\n",
        "            # dim of out is batch_size x num_classes\n",
        "            out = model(text, text_lengths)\n",
        "            correct += torch.max(out, 1)[1].eq(test_y).sum()\n",
        "            total += out.size()[0]\n",
        "        cur_acc = float(correct) / total\n",
        "    print('Test accuracy = {}'.format(cur_acc))\n",
        "\n",
        "train_validate()\n",
        "model = load_model(model_path)\n",
        "test_model(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "overall\n",
            "overall\n",
            "overall\n",
            "epoch = 0, loss = 0.4739110767841339\n",
            "Validation accuracy = 0.80325\n",
            "epoch = 1, loss = 0.3649899363517761\n",
            "Validation accuracy = 0.82275\n",
            "best epoch = 1 with best validation accuracy = 0.82275\n",
            "cuda:0\n",
            "Test accuracy = 0.8278\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}