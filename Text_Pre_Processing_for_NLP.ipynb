{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfzI2BlSmnKMs49jeeBRLb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/@abdallahashraf90x/text-pre-processing-for-nlp-95cef3ad6bab)"
      ],
      "metadata": {
        "id": "BMailzLq4Ft-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preliminaries\n",
        "## a) Sentence segmentation"
      ],
      "metadata": {
        "id": "H24aNvYg4KkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R3lfqXl4SQ5",
        "outputId": "4d14d78b-1069-4a19-ff19-962488974a93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8_H16rYv3_tP"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "mytext = \"\"\" GPT-4 image analysis goes beyond describing the picture.\n",
        "In the same demonstration Vee watched, an OpenAI representative sketched an\n",
        "image of a simple website and fed the drawing to GPT-4. Next the model was\n",
        " asked to write the code required to produce such a website—and it did.\n",
        " “It looked basically like what the image is. It was very, very simple,\n",
        " but it worked pretty well,” says Jonathan May, a research associate professor\n",
        " at the University of Southern California. “So that was cool.” \"\"\"\n",
        "\n",
        "my_sentences = sent_tokenize(mytext)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Word tokenization"
      ],
      "metadata": {
        "id": "CbrB9aYA4NoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in my_sentences:\n",
        "  print(sentence)\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHTPu46J4MHl",
        "outputId": "f7a1e5f6-0248-4bda-837e-21fe8f0802a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GPT-4 image analysis goes beyond describing the picture.\n",
            "['GPT-4', 'image', 'analysis', 'goes', 'beyond', 'describing', 'the', 'picture', '.']\n",
            "In the same demonstration Vee watched, an OpenAI representative sketched an\n",
            "image of a simple website and fed the drawing to GPT-4.\n",
            "['In', 'the', 'same', 'demonstration', 'Vee', 'watched', ',', 'an', 'OpenAI', 'representative', 'sketched', 'an', 'image', 'of', 'a', 'simple', 'website', 'and', 'fed', 'the', 'drawing', 'to', 'GPT-4', '.']\n",
            "Next the model was\n",
            " asked to write the code required to produce such a website—and it did.\n",
            "['Next', 'the', 'model', 'was', 'asked', 'to', 'write', 'the', 'code', 'required', 'to', 'produce', 'such', 'a', 'website—and', 'it', 'did', '.']\n",
            "“It looked basically like what the image is.\n",
            "['“', 'It', 'looked', 'basically', 'like', 'what', 'the', 'image', 'is', '.']\n",
            "It was very, very simple,\n",
            " but it worked pretty well,” says Jonathan May, a research associate professor\n",
            " at the University of Southern California.\n",
            "['It', 'was', 'very', ',', 'very', 'simple', ',', 'but', 'it', 'worked', 'pretty', 'well', ',', '”', 'says', 'Jonathan', 'May', ',', 'a', 'research', 'associate', 'professor', 'at', 'the', 'University', 'of', 'Southern', 'California', '.']\n",
            "“So that was cool.”\n",
            "['“', 'So', 'that', 'was', 'cool', '.', '”']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Frequent Steps"
      ],
      "metadata": {
        "id": "M9F7lBKa4Z23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "def preprocess_corpus(texts):\n",
        "    mystopwords = set(stopwords.words(\"english\"))\n",
        "    def remove_stops_digits(tokens):\n",
        "        return [token.lower() for token in tokens if token not in mystopwords and\n",
        "        not token.isdigit() and token not in punctuation]\n",
        "    return [remove_stops_digits(word_tokenize(text)) for text in texts]"
      ],
      "metadata": {
        "id": "UmetOQoZ4UOP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming and lemmatization"
      ],
      "metadata": {
        "id": "K8HLPhBl4pmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "word1, word2 = \"cars\", \"revolution\"\n",
        "print(stemmer.stem(word1), stemmer.stem(word2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NltpGKz74d5o",
        "outputId": "b987b14c-d880-4920-a38e-27d3bcd3bac9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car revolut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) #a is for adjective"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CgOt9Az44jL",
        "outputId": "bc043cb1-1e63-4713-8a11-faaa74c2e40e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "token = sp(u'better')\n",
        "for word in token:\n",
        "  print(word.text, word.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRP2H6sd5IZ6",
        "outputId": "b3ec0058-38d9-4727-b371-43855a3af4de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "better well\n"
          ]
        }
      ]
    }
  ]
}