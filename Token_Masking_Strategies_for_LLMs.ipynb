{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCNWUSB3oVaQe6oC2W+Tro"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://pub.towardsai.net/token-masking-strategies-for-llms-d2e6c926b22d)"
      ],
      "metadata": {
        "id": "RqZgk_osFX95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6McmWBFFVF3",
        "outputId": "bb7cabe1-4b6b-4ab7-b87a-bcb3e49eed10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.8.1-py3-none-any.whl (970 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m970.4/970.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji (from stanza)\n",
            "  Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.2.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m784.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/124.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:12\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('en')\n",
        "\n",
        "# Text used in our examples\n",
        "text = \"Huntington's disease is a neurodegenerative autosomal disease\n",
        "results due to expansion of polymorphic CAG repeats in the huntingtin gene.\n",
        "Phosphorylation of the translation initiation factor 4E-BP results in the\n",
        "alteration of the translation control leading to unwanted protein synthesis\n",
        "and neuronal function. Consequences of mutant huntington (mhtt) gene\n",
        "transcription are not well known. Variability of age of onset is an\n",
        "important factor of Huntington's disease separating adult and juvenile types.\n",
        "The factors which are taken into account are-genetic modifiers, maternal\n",
        "protection i.e excessive paternal transmission, superior ageing genes\n",
        "and environmental threshold. A major focus has been given to the molecular\n",
        "pathogenesis which includes-motor disturbance, cognitive disturbance and\n",
        "neuropsychiatric disturbance. The diagnosis part has also been taken care of.\n",
        "This includes genetic testing and both primary and secondary symptoms.\n",
        "The present review also focuses on the genetics and pathology of Huntington's\n",
        "disease.\"\n",
        "\n",
        "\n",
        "# We will use a stanza model for getting each different sentence\n",
        "# as an element of the list\n",
        "nlp = stanza.Pipeline('en', use_gpu=False)\n",
        "doc = nlp(text)\n",
        "sentences = [sentence.text for sentence in doc.sentences]"
      ],
      "metadata": {
        "id": "eI621EaFFevk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "def load_dataset_mlm(sentences, tokenizer_class=AutoTokenizer,\n",
        "                     collator_class=DataCollatorForLanguageModeling,\n",
        "                     mlm=True, mlm_probability=0.20):\n",
        "    tokenizer = tokenizer_class.from_pretrained('google-bert/bert-base-uncased')\n",
        "    inputs = tokenizer(sentences, return_tensors='pt', padding=True,\n",
        "                       truncation=True)\n",
        "\n",
        "    # Random masking configuration\n",
        "    data_collator = collator_class(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "        mlm_probability=mlm_probability\n",
        "    )\n",
        "\n",
        "    \"\"\"The collator expects a tuple of tensors, so you have to split\n",
        "    the input tensors and then remove the first dimension and pass it\n",
        "    to a tuple. \"\"\"\n",
        "    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n",
        "    tuple_ids = list(tuple_ids)\n",
        "    for tensor in range(len(tuple_ids)):\n",
        "        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n",
        "    tuple_ids = tuple(tuple_ids)\n",
        "\n",
        "    # Get input_ids, attention_masks and labels for each sentence.\n",
        "    batch = data_collator(tuple_ids)\n",
        "    return batch['input_ids'], inputs['attention_mask'], batch['labels']\n",
        "\n",
        "\n",
        "input_ids, attention_mask, labels = load_dataset_mlm(sentences)"
      ],
      "metadata": {
        "id": "0GKFDBluFgeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "def load_dataset_mlm(sentences, tokenizer_class=BartTokenizer,\n",
        "                     collator_class=DataCollatorForLanguageModeling,\n",
        "                     mlm=True, mlm_probability=0.20):\n",
        "    tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n",
        "    inputs = tokenizer(sentences, return_tensors='pt', padding=True,\n",
        "                       truncation=True)\n",
        "\n",
        "    # Random masking configuration\n",
        "    data_collator = collator_class(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,  # True for Masked Language Modelling\n",
        "        mlm_probability=mlm_probability  # Chance for every token to get masked\n",
        "    )\n",
        "\n",
        "    \"\"\"The collator expects a tuple of tensors, so you have to split\n",
        "    the input tensors and then remove the first dimension and pass it\n",
        "    to a tuple. \"\"\"\n",
        "    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n",
        "    tuple_ids = list(tuple_ids)\n",
        "    for tensor in range(len(tuple_ids)):\n",
        "        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n",
        "    tuple_ids = tuple(tuple_ids)\n",
        "\n",
        "    # Get input_ids, attention_masks and labels for each sentence.\n",
        "    batch = data_collator(tuple_ids)\n",
        "    batch['labels'] = inputs['input_ids']\n",
        "    return batch['input_ids'], inputs['attention_mask'],  batch['labels']\n",
        "\n",
        "input_ids, attention_mask, labels = load_dataset_mlm(sentences)\n"
      ],
      "metadata": {
        "id": "aicpnQXQFjSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_deletion(sentences, tokenizer_class=BartTokenizer, collator_class=DataCollatorForLanguageModeling,\n",
        "                 mlm=True, mlm_probability=0.20):\n",
        "    tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n",
        "    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    data_collator = collator_class(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "        mlm_probability=mlm_probability\n",
        "    )\n",
        "\n",
        "    tuple_ids = torch.split(inputs['input_ids'], 1, dim=0)\n",
        "    tuple_ids = list(tuple_ids)\n",
        "    for tensor in range(len(tuple_ids)):\n",
        "        tuple_ids[tensor] = tuple_ids[tensor].squeeze(0)\n",
        "    tuple_ids = tuple(tuple_ids)\n",
        "\n",
        "    batch = data_collator(tuple_ids)\n",
        "\n",
        "    # We use the initial inputs as labels\n",
        "    batch['labels'] = batch['input_ids'].clone()\n",
        "\n",
        "    # We remove tokens with mask identifier and thus make token deletion\n",
        "    # Change the value to the mask identifier of the specific token model\n",
        "    # It is necessary to know the identifier of the mask token for\n",
        "    # that specific model\n",
        "    mask = batch['input_ids'] != 50264\n",
        "    initial_size = batch['input_ids'].size(1)\n",
        "    total_sentences = batch['input_ids'].size(0)\n",
        "\n",
        "    # When we remove the specific token, we must fill with the padding\n",
        "    # token otherwise the tensor size is not respected.\n",
        "    for i in range(total_sentences):\n",
        "        new_tensor = batch['input_ids'][i][mask[i]]\n",
        "        new_tensor = F.pad(new_tensor, (0, initial_size - new_tensor.size(0)), value=1)\n",
        "        batch['input_ids'][i] = new_tensor\n",
        "        attention_mask = batch['input_ids'][i] == 1\n",
        "        inputs['attention_mask'][i][attention_mask] = 0\n",
        "\n",
        "    return batch['input_ids'], inputs['attention_mask'], batch['labels']\n",
        "\n",
        "input_ids, attention_mask, labels = token_deletion(sentences)"
      ],
      "metadata": {
        "id": "5GFLOrTBFlfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import BartTokenizer\n",
        "\n",
        "def text_infilling(sentence, probability=0.2, poisson_lambda=3):\n",
        "    # We'll use a binary mask to determine which words to replace\n",
        "    mask = np.random.choice([0, 1], size=len(sentence), p=[1-probability, probability])\n",
        "\n",
        "    # Now we'll replace the chosen words with a mask token\n",
        "    # We'll also use a Poisson distribution to determine the length of the spans to mask\n",
        "    for i in range(len(mask)):\n",
        "        if mask[i] == 1:\n",
        "            span_length = np.random.poisson(poisson_lambda)\n",
        "            for j in range(span_length):\n",
        "                if i + j < len(sentence):\n",
        "                    sentence[i + j] = \"<mask>\"\n",
        "\n",
        "    infilled_sentence = []\n",
        "    for token in range(len(sentence)):\n",
        "        if sentence[token] == \"<mask>\":\n",
        "            if token < len(sentence)-1:\n",
        "                if sentence[token+1] == \"<mask>\":\n",
        "                    continue\n",
        "                else:\n",
        "                    infilled_sentence.append(sentence[token])\n",
        "            else:\n",
        "                infilled_sentence.append(sentence[token])\n",
        "        else:\n",
        "            infilled_sentence.append(sentence[token])\n",
        "    return \" \".join(infilled_sentence)\n",
        "\n",
        "def text_infilling_input(masked_sentences, sentences, tokenizer_class=BartTokenizer):\n",
        "    tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n",
        "    inputs = tokenizer(masked_sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "    labels = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "    return inputs['input_ids'], inputs['attention_mask'], labels['input_ids']\n",
        "\n",
        "input_ids, attention_mask, labels = text_infilling_input(masked_sentences, sentences)"
      ],
      "metadata": {
        "id": "ThQf3XK1FnLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It selects the first \"number_sentences\" within a given set of \"sentences\"\n",
        "# and returns those sentences in a random order.\n",
        "def sentence_permutation(sentences, number_sentences):\n",
        "    new_sentences = sentences[:number_sentences]\n",
        "    random.shuffle(new_sentences)\n",
        "    new_sentences = sentence_joiner(new_sentences)\n",
        "    return new_sentences\n",
        "\n",
        "def permuted_data_generation(sentences: list, total_sentences: int):\n",
        "    training_sentences = []\n",
        "    training_labels = []\n",
        "    sentences_copy = sentences.copy()\n",
        "    # We can apply sentence_permutation a number of times equal to the\n",
        "    # size of the list - 1 to get an example with each new sentence in\n",
        "    # the text, removing the oldest one.\n",
        "    for _ in range(len(sentences)-total_sentences+1):\n",
        "        new_sentences = sentence_permutation(sentences_copy, total_sentences)\n",
        "        joined_sentences = sentence_joiner(sentences_copy[:total_sentences])\n",
        "        sentences_copy = sentences_copy[1:]\n",
        "        training_sentences.append(new_sentences)\n",
        "        training_labels.append(joined_sentences)\n",
        "\n",
        "    return training_sentences, training_labels\n",
        "\n",
        "\n",
        "def permutation_training(sentences: list, sentences_labels: list,\n",
        "                         tokenizer_class=BartTokenizer,\n",
        "                         collator_class=DataCollatorForLanguageModeling,\n",
        "                        mlm=True, mlm_probability=0.0):\n",
        "    # We get input_ids and attention mask from the permuted sentences\n",
        "    input, attention_mask, _ = load_dataset_mlm(sentences, tokenizer_class, collator_class, mlm, mlm_probability)\n",
        "\n",
        "    # Labels from the original sentences\n",
        "    labels, _, _ = load_dataset_mlm(sentences_labels, tokenizer_class, collator_class, mlm, mlm_probability)\n",
        "\n",
        "    return input.squeeze(0), attention_mask.squeeze(0), labels.squeeze(0)\n",
        "\n",
        "input_ids, attention_mask, labels = permutation_training(training_sentences, training_labels_sentences)"
      ],
      "metadata": {
        "id": "5U2uIDGVFpDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_joiner(sentences: list):\n",
        "  return ' '.join(sentences)\n",
        "\n",
        "# With this function we gather as many sentences as we want to form the input data to the tokenizer.\n",
        "def rotated_data_generation(sentences: list, total_sentences: int):\n",
        "  training_sentences = []\n",
        "  sentences_copy = sentences.copy()\n",
        "  for _ in range(len(sentences)-total_sentences+1):\n",
        "    new_sentences = sentences_copy[:total_sentences]\n",
        "    new_sentences = sentence_joiner(new_sentences)\n",
        "    sentences_copy = sentences_copy[1:]\n",
        "    training_sentences.append(new_sentences)\n",
        "  return training_sentences\n",
        "\n",
        "# Apply this function over the rotated sentences from previous function\n",
        "def document_rotation_training(sentences, tokenizer_class=BartTokenizer):\n",
        "  tokenizer = tokenizer_class.from_pretrained('facebook/bart-base')\n",
        "  tokens = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "  tokens['input_ids'] = tokens['input_ids'].squeeze(0)\n",
        "  tokens['labels'] = tokens['input_ids'].clone()\n",
        "\n",
        "  iterations = tokens['input_ids'].size(0)\n",
        "  for i in range(iterations):\n",
        "    # Get the attention mask and convert to list\n",
        "    attention_mask = tokens['attention_mask'][i].tolist()\n",
        "    # Calculate the position where padding starts\n",
        "    if 0 in attention_mask:\n",
        "      padding_start_position = attention_mask.index(0)\n",
        "    else:\n",
        "      padding_start_position = False\n",
        "    # We take into account the position of the padding so as not to rotate it along with the rest of the document.\n",
        "    if padding_start_position:\n",
        "      random_token = torch.randint(1, padding_start_position-1, (1,))\n",
        "      tokens['input_ids'][i] = torch.cat((tokens['input_ids'][i][0].unsqueeze(0), #initial token\n",
        "                                      tokens['input_ids'][i][random_token.item():padding_start_position-1], #from random to padding\n",
        "                                      tokens['input_ids'][i][1:random_token.item()], #from 1 to random\n",
        "                                      tokens['input_ids'][i][padding_start_position-1:-1],\n",
        "                                      tokens['input_ids'][i][-1].unsqueeze(0)), 0)\n",
        "\n",
        "    # If there is no padding, we rotate the document without taking the padding into account.\n",
        "    else:\n",
        "      random_token = torch.randint(1, tokens['input_ids'].size(0)-1, (1,))\n",
        "      tokens['input_ids'][i] = torch.cat((tokens['input_ids'][i][0].unsqueeze(0), #initial token\n",
        "                                      tokens['input_ids'][i][random_token.item():-1], #from random to end\n",
        "                                      tokens['input_ids'][i][1:random_token.item()],\n",
        "                                      tokens['input_ids'][i][-1].unsqueeze(0)), 0)\n",
        "  return tokens['input_ids'], tokens['attention_mask'].squeeze(0), tokens['labels']\n",
        "\n",
        "data = rotated_data_generation(sentences, 3)\n",
        "input_ids, attention_mask, labels = document_rotation_training(data)"
      ],
      "metadata": {
        "id": "NPAZ8V3_Fq0D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}